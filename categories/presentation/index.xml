<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Presentation | Behavioral Data Science</title><link>https://www.behavioral-ds.science/categories/presentation/</link><atom:link href="https://www.behavioral-ds.science/categories/presentation/index.xml" rel="self" type="application/rss+xml"/><description>Presentation</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2025</copyright><lastBuildDate>Mon, 27 Sep 2021 00:00:00 +0000</lastBuildDate><image><url>https://www.behavioral-ds.science/img/logo.png</url><title>Presentation</title><link>https://www.behavioral-ds.science/categories/presentation/</link></image><item><title>Discovering coordinated disinformation via Hawkes processes</title><link>https://www.behavioral-ds.science/theme2_content/coordinated_disinfo_hawkes/</link><pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.science/theme2_content/coordinated_disinfo_hawkes/</guid><description>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/kzGkvZRjnoI?start=70" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;p>Recording of the presentation &amp;ldquo;Discovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes&amp;rdquo; (Tim Graham, &lt;strong>Marian-Andrei Rizoiu&lt;/strong>, Axel Bruns, Dan Angus), presented at the &lt;a href="https://www.ecrea2021.eu/">European Communication Conference (ECREA) 2021&lt;/a>, 8 Sep. 2021.&lt;/p>
&lt;h1 id="discovering-the-strategies-and-promotion-schedules-of-coordinated-disinformation-via-hawkes-intensity-processes">Discovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes&lt;/h1>
&lt;p>&lt;a href="https://www.qut.edu.au/about/our-people/academic-profiles/timothy.graham">Tim Graham&lt;/a>, Digital Media Research Centre, Queensland University of Technology&lt;br>
&lt;a href="https://www.behavioral-ds.science/authors/ma-rizoiu/">Marian-Andrei Rizoiu&lt;/a>, Data Science Institute, University of Technology Sydney&lt;br>
&lt;a href="https://www.qut.edu.au/about/our-people/academic-profiles/a.bruns">Axel Bruns&lt;/a>, Digital Media Research Centre, Queensland University of Technology&lt;br>
&lt;a href="https://www.qut.edu.au/about/our-people/academic-profiles/daniel.angus">Dan Angus&lt;/a>, Digital Media Research Centre, Queensland University of Technology&lt;/p>
&lt;p>‘Fake news’ and broader ‘information disorders’ [6] such as mis- and disinformation have emerged as global issues that threaten to undermine democracy and authentic political communication on social media [1]. Increasingly sophisticated coordination strategies have intensified the scale and scope of the impact that disinformation has on public opinion and democratic trust. Howard et al. [3] found that coordinated disinformation operations are now occurring in 48 countries, and in 2019 the European External Action Service detected and exposed over 1,000 cases of disinformation within the European Union [2]. Whilst disinformation has attracted much scholarly attention, most studies to date have focussed on the diffusion and impact of individual content (e.g. ‘fake news’ articles) and the activity of individual accounts (e.g. bots and trolls).&lt;/p>
&lt;p>An emerging problem is to understand message coordination strategies, where content authored and distributed by agents (e.g. Twitter trolls) is governed and scheduled by some unknown principal actor [4]. We know that coordinated promotion (e.g. sharing, liking, retweeting) of ‘fake news’ articles by trolls and social bots can greatly increase and amplify the negative effects of these attempts to sow discord and manipulate public conversations about election candidates and partisan issues such as immigration and climate change. Likewise, it is evident that disinformation campaigns unfold via ‘collaborative work’ that co-opts and cultivates organic systems in order to produce desired effects such as increased polarisation, distrust in news media and confusion of the audience [7]. This makes identifying ‘inauthentic’ versus ‘organic’ activity ever more difficult, as they are intricately enmeshed in real-world disinformation campaigns.&lt;/p>
&lt;p>In this paper, we tackle the problem of inferring the coordinated promotion schedules of ‘fake news’ articles using a novel approach known as Hawkes Intensity Processes (HIP; see [5]). We analyse the diffusion of articles from ten major sources of hyperpartisan information and ‘fake news’ within over 16.5 million tweets that linked to content from these sites during July to September 2019. Using HIP, we uncover not only coordination strategies but also the promotion schedules of ‘fake news’ content, where agents (in this case Twitter accounts) are being centrally managed by principals (e.g. state operatives, government officials, etc.) in order to strategically promote ‘fake news’ content and maximise its virality and longevity in the social memory. This paper provides preliminary results from this ongoing research, highlighting the current challenges as well as open problems and gaps for future work.&lt;/p>
&lt;h3 id="references">References&lt;/h3>
&lt;p>[1] Benkler, Y., Faris, R., &amp;amp; Roberts, H. (2018). Network propaganda: Manipulation, disinformation, and radicalization in American politics. Oxford University Press.&lt;br>
[2] European Commission. (2019). Action plan against disinformation: Report in progress. Retrieved 20 November 2019 from: &lt;a href="https://ec.europa.eu/commission/sites/beta-political/files/factsheet_disinfo_elex_140619_final.pdf">https://ec.europa.eu/commission/sites/beta-political/files/factsheet_disinfo_elex_140619_final.pdf&lt;/a>.&lt;br>
[3] Howard, P. N., &amp;amp; Kollanyi, B. (2016). Bots, #StrongerIn, and #Brexit: Computational Propaganda during the UK-EU Referendum. SSRN Electronic Journal. doi:10.2139/ssrn.2798311&lt;br>
[4] Keller, F. B., Schoch, D., Stier, S., &amp;amp; Yang, J. (2019). Political Astroturfing on Twitter: How to Coordinate a Disinformation Campaign. Political Communication, 1-25.&lt;br>
[5] Rizoiu, M. A., Xie, L., Sanner, S., Cebrian, M., Yu, H., &amp;amp; Van Hentenryck, P. (2017, April). Expecting to be hip: Hawkes intensity processes for social media popularity. In Proceedings of the 26th International Conference on World Wide Web (pp. 735-744). International World Wide Web Conferences Steering Committee.&lt;br>
[6] Wardle, C., &amp;amp; Derakhshan, H. (2017). Information disorder: Toward an interdisciplinary framework for research and policymaking. Council of Europe Report DGI (2017) 09.&lt;br>
[7] Wilson, T., Zhou, K., &amp;amp; Starbird, K. (2018). Assembling Strategic Narratives: Information Operations as Collaborative Work within an Online Community. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW), 183.&lt;/p>
&lt;h2 id="the-panel-coordinated-inauthentic-behaviour-in-social-media-new-methods-and-findings">The panel: &amp;ldquo;Coordinated Inauthentic Behaviour in Social Media: New Methods and Findings&amp;rdquo;&lt;/h2>
&lt;h3 id="panel-rationale">Panel Rationale&lt;/h3>
&lt;p>Social media platforms are increasingly forced to address what Facebook now describes as ‘coordinated inauthentic behaviour’ (Gleicher 2018): online influence operations that seek to trick platform algorithms into promoting and recommending ‘problematic information’ (Jack 2017), to mislead the human users of such platforms into accepting and sharing such content, and thereby also to affect broader issue frames and news agendas in mainstream media coverage. Concerns about such coordinated inauthentic behaviour extend earlier fears about the influence of malignant social bots, but also transcend them: drawing on social bots as well as human labour, coordinated inauthentic behaviour is likely to involve a combination of manual and automated activity. This additional human factor also complicates the detection of such coordinated activities, and their distinction from genuine, organic, authentic coordinated actions.&lt;/p>
&lt;p>This cross-national and interdisciplinary panel approaches the study of coordinated inauthentic behaviour from a number of directions. It outlines novel and innovative detection and analysis approaches for a number of leading social media platforms, and presents their results in the context of domestic and international political debates across several national contexts. Further, it also considers how mainstream journalism might report on and respond to such activities in order to protect news audiences from being affected by coordinated inauthentic behaviours.&lt;/p>
&lt;p>The first two papers in this panel focus especially on coordinated inauthentic link-sharing practices. Paper 1 introduces Hawkes Intensity Processes (HIP), a novel technique for inferring the coordinated content promotion schedules of automated social media accounts, and applies this to a major dataset of 16.5 million tweets containing links to ten major sites identified as sources of hyperpartisan content and ‘fake news’. In doing so, it uncovers new networks of inauthentic Twitter actors. Paper 2 investigates similar coordinated link-sharing activity on Facebook in Italy during the 2018 Italian and 2019 European elections. It uncovers evidence for the involvement of dozens of pages, groups, and public profiles in such media manipulation attempts. Paper 3 complements this work by focussing especially on the temporal posting patterns in such coordinated activity. It employs the recurrence plotting technique to identify traces of inauthentic actors’ use of automated scheduling tools in systematically posting content to a network of apparently unrelated pages, focussing here especially on a group of far-right pages on Facebook. Paper 4 examines ten coordinated disinformation campaigns across the globe (e.g., Hong Kong, Russia, USA, Spain and Germany) and identifies important traits that help distinguish between those participating in the disinformation campaign and the regular users they try to imitate. Paper 5, finally, shifts our attention to a core target of such coordinated inauthentic behaviour: the journalists and editors whose perception of current political moods such influence operations often aim to affect. Drawing on a series of in-depth interviews with Danish news workers and related stakeholders, it examines their understanding of and responses to coordinated mis- and disinformation campaigns.&lt;/p>
&lt;p>Collectively, these studies contribute substantially to advancing the methodological toolkit and extending the empirical evidence base for the study of coordinated inauthentic behaviour, while also not losing sight of the stakeholders that such work seeks to support. They offer an independent assessment of the nature and extent of the problem across several leading social media platforms, complementing the platform providers’ own investigations into such activities and identifying possible responses to such concerns for both social and mainstream media actors.&lt;/p>
&lt;h3 id="the-papers-presented-are-presenters-bolded">The papers presented are (presenters bolded):&lt;/h3>
&lt;ul>
&lt;li>Discovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes (Tim Graham, &lt;strong>Marian-Andrei Rizoiu&lt;/strong>, Axel Bruns, Dan Angus)&lt;/li>
&lt;li>It Takes a Village to Manipulate the Media: Coordinated Link Sharing Behaviour during 2018 and 2019 Italian Elections (&lt;strong>Fabio Giglietto&lt;/strong>, Nicola Righetti, Luca Rossi, Giada Marino)&lt;/li>
&lt;li>Recurrence Plotting for Detecting Duplicate Online Posting Activities (&lt;strong>Dan Angus&lt;/strong>, Tim Graham, Tobias Keller, Brenda Moon, Axel Bruns)&lt;/li>
&lt;li>Astroturfing in Hong Kong and Elsewhere: Patterns of Coordination in Hidden Twitter Campaigns (&lt;strong>Franziska B. Keller&lt;/strong>, Sebastian Stier, David Schoch, JungHwan Yang)&lt;/li>
&lt;/ul></description></item></channel></rss>