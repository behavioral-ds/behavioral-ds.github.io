<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>(edited by Marian-Andrei Rizoiu) | Behavioral Data Science</title><link>https://www.behavioral-ds.science/authors/edited-by-marian-andrei-rizoiu/</link><atom:link href="https://www.behavioral-ds.science/authors/edited-by-marian-andrei-rizoiu/index.xml" rel="self" type="application/rss+xml"/><description>(edited by Marian-Andrei Rizoiu)</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2023</copyright><lastBuildDate>Fri, 13 Oct 2023 00:00:00 +0000</lastBuildDate><image><url>https://www.behavioral-ds.science/img/logo.png</url><title>(edited by Marian-Andrei Rizoiu)</title><link>https://www.behavioral-ds.science/authors/edited-by-marian-andrei-rizoiu/</link></image><item><title>Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions</title><link>https://www.behavioral-ds.science/theme2_content/omm/</link><pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.science/theme2_content/omm/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In our digital age, the surge of online extremism brings forth urgent concerns such as hate speech normalization, user radicalization, and societal division. Counteracting these issues calls for exploring mitigation strategies, be it through negative interventions, an example of which is the EU’s Digital Services Act seeking to regulate deletion of harmful content, or positive interventions strategically introduced into the opinion landscape to boost certain opinions. In our latest work (to be presented at ICWSM 2024), we introduce the Opinion Market Model (OMM) as a testbed to simulate and measure the impact of positive interventions on online opinion dynamics. Additionally, we show the OMM’s capability to detect possible backfiring of debunking.&lt;/p>
&lt;p>In this blog post, we delve into OMM's structure and demonstrate its ability to uncover cooperation-competition relations across online opinions about (a) Australian bushfires and (b) popular VEVO artists, and show how media coverage modulates the spread of far-right opinion.&lt;/p>
&lt;p>&lt;strong>Paper citation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code>Calderon, Pio, Rohit Ram, and Marian-Andrei Rizoiu. &amp;quot;Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions.&amp;quot; In ICWSM 2024.
&lt;/code>&lt;/pre>
&lt;p>(&lt;em>see full paper here: &lt;a href="https://arxiv.org/pdf/2208.06620.pdf">&lt;a href="https://arxiv.org/pdf/2208.04097.pdf">https://arxiv.org/pdf/2208.04097.pdf&lt;/a>&lt;/a>&lt;/em>)&lt;/p>
&lt;h2 id="interventions-in-the-online-opinion-ecosystem">Interventions in the Online Opinion Ecosystem&lt;/h2>
&lt;p>Online social media platforms are breeding grounds for discourse, fostering both cooperation and competition among opinions vying for limited public attention [1]. Two distinct intervention strategies have emerged to counter extremist views in the online landscape. Negative interventions operate by diverting attention away from certain opinions, employing measures such as fact-check alerts [2], shadow-banning [3], and outright banning of extremist content [4]. While these methods have proven effective [5], they are predominantly within the control of social media platforms that use them sparingly [6]. Conversely, positive interventions [7], including debunking misinformation [8] and amplifying media coverage [9], combat extremism by boosting certain opinions, often under the purview of governmental and media entities [10].&lt;/p>
&lt;p>Evaluating the feasibility and effectiveness of positive interventions requires the ability (1) to disentangle inter-opinion interactions from the reaction of opinions to interventions and (2) to test the sensitivity of the opinion ecosystem to interventions. We propose the Opinion Market Model handle these two issues effectively.&lt;/p>
&lt;h2 id="our-solution-the-opinion-market-model">Our Solution: The Opinion Market Model&lt;/h2>
&lt;p>The Opinion Market Model (OMM) models online opinion dynamics resulting from inter-opinion interactions and positive interventions&amp;rsquo; influence. Drawing an analogy from economic markets, the model views opinions as analogous to economic goods. Just as goods can compete or reinforce each other's market share, online opinions can engage in similar dynamics—competing for limited online attention or complementing each other. This analogy allows for applying market share attraction models [11], aiding in understanding opinion interactions and their competition for users&amp;rsquo; attention in the online realm.&lt;/p>
&lt;img src="./omm_1.png" width="60%">
&lt;p style="text-align: center;">&lt;em>Figure 1. The OMM as a two-tier model of the online opinion ecosystem.&lt;/em>&lt;/p>
&lt;p>OMM models the online opinion ecosystem across two tiers: online attention volumes and opinion market shares. Illustrated in Figure 1 is a simplified scenario with two opinions (labeled 0 and 1) on a social media platform. Each opinion is assumed to have two stances denoting far-right support (+) and moderate views (-).&lt;/p>
&lt;p>In terms of system inputs (top row of Figure 1), the OMM makes a distinction between exogenous signals \(S(t)\) (gray) and interventions \(X(t)\) (yellow) to influence online opinions. Exogenous signals modulate the dynamics of the opinions’ sizes and represent events like natural disasters or speeches. Interventions (such as increased media coverage) are designed to add attention to the opinion ecosystem, increasing the market share of certain opinions while suppressing others.&lt;/p>
&lt;p>The first tier (middle row of Figure 1) uses a discrete-time Hawkes process [12] to estimate opinion attention volumes (i.e. the daily number of postings featuring opinions) driven by both exogenous signals and the self-exciting nature of online popularity.
The second tier (bottom row of Figure 1) uses a market share attraction model to capture both inter-opinion interactions, either cooperative or competitive and driven by limited online user attention, and their response to positive interventions.&lt;/p>
&lt;h3 id="datasets">Datasets&lt;/h3>
&lt;p>We utilize two real-world datasets to showcase the predictive and interpretative capabilities of the OMM. Below, we give a short description of each.&lt;/p>
&lt;p>&lt;strong>Bushfire Opinions dataset.&lt;/strong> The Bushfire Opinions dataset contains 90 days of Twitter and Facebook discussions about bushfires and climate change between 1 November 2019 to 29 January 2020. We filter and label relevant Facebook and Twitter postings using the textual topic and opinion classifiers developed in [13] into six opinions:&lt;/p>
&lt;ol start="0">
&lt;li>Greens policies are the cause of the Australian bushfires.&lt;/li>
&lt;li>Mainstream media cannot be trusted.&lt;/li>
&lt;li>Climate change crisis is not real / is a UN hoax.&lt;/li>
&lt;li>Australian bushfires and climate change are not related.&lt;/li>
&lt;li>Australian bushfires were caused by random arsonists.&lt;/li>
&lt;li>Bushfires are a normal summer occurrence in Australia.&lt;/li>
&lt;/ol>
&lt;p>To distinguish between moderate (-) and far-right (+) opinions, we utilize the far-right stance detector in [14], which employs a measure of textual homophily to gauge the similarity between Twitter users and established far-right figures. Our set of opinions is denoted as \({(i−, i+)|i ∈ {0, &amp;hellip; , 5}}\). In total, we analyze data from two platforms, encompassing 74,461 tweets and 7,974 Facebook posts, all labeled with 12 distinct stanced opinions.&lt;/p>
&lt;p>The exogenous signal \(S(t)\) is defined as the 5-day rolling average of the Google Trends query &amp;ldquo;bushfire+climate change&amp;rdquo; in Australia, functioning as a representation of general interest and offline occurrences [15]. For positive interventions \({X_k(t)}\), we incorporate reputable (R) and controversial (C) news coverage from Australian and international sources. Reputable sources are drawn from the Reputable News Index (RNIX) [16], while controversial international sources are accessed through NELA-GT-2019 [17].&lt;/p>
&lt;p>&lt;strong>VEVO 2017 Top 10 dataset.&lt;/strong> We evaluate the OMM on a second dataset obtained by aligning artist-level time series of YouTube views and Twitter post counts for the top 10 VEVO-affiliated artists over a span of 100 days from 2 January 2017, to 11 April 2017. The YouTube data from the VEVO Music Graph dataset [18] encompasses daily view counts for music videos by verified VEVO artists across six English-speaking countries. Twitter post counts are collected through the Twitter API, using the artist's name as the input query. Unlike the Bushfire Opinions dataset, here we utilize distinct exogenous signals for each artist, referred to as \(S_i(t)\), derived from Google Trends. Our focus for the VEVO 2017 Top 10 dataset is uncovering intrinsic interactions among artists; hence, we leave out interventions \({X_k(t)}\).&lt;/p>
&lt;h3 id="predicting-opinion-volumes-and-market-shares">Predicting Opinion Volumes and Market Shares&lt;/h3>
&lt;p>Since OMM is a two-tier model, we can evaluate its predictive capabilities on two levels: (1) predicting opinion volumes and (2) predicting opinion shares. In the first task, we forecast the total volume of opinionated posts across each considered online platform during the prediction period after training on the training period. In the second task, we predict the market shares for each opinion on each platform during the prediction period.&lt;/p>
&lt;p>&lt;img src="./omm_2.png" alt="">&lt;/p>
&lt;p style="text-align: center;">&lt;em>Figure 2. Fitting and predicting with OMM on the Bushfire Opinions dataset.&lt;/em>&lt;/p>
&lt;p>Fig. 2(a) illustrates the alignment between observed (blue line) and modeled (orange line) opinion volumes in the bushfire dataset. Notably, we fit well within both training and prediction periods. In Fig. 2(b), the distribution of observed (left column) and fitted/predicted (right column) opinion market shares for the bushfire dataset are showcased. Again, OMM adeptly captures trends in opinion shares across both platforms. We also compare (see full text) the OMM to a multivariate linear regression baseline and state-of-the-art models in product share modeling (the Correlated Cascades model [19] and the Competing Products model [20]), and show superior performance.&lt;/p>
&lt;h3 id="uncovering-latent-opinion-interactions">Uncovering Latent Opinion Interactions&lt;/h3>
&lt;p>To understand the interactions of opinions with one another, we borrow the economic concept of elasticity and derive opinion share model elasticities from the OMM. These quantify the instantaneous competition-cooperation interactions across opinions, capturing both the direct effect of an opinion on another and the indirect effect of an opinion across other opinions. Elasticities are signed metrics: if positive we observe a cooperative relation, if negative a competitive relation.&lt;/p>
&lt;p>&lt;img src="./omm_3.png" alt="">&lt;/p>
&lt;p style="text-align: center;">&lt;em>Figure 3. (a) Inter-opinion OMM elasticities in the Bushfire Opinions dataset. (b) Youtube elasticities in the VEVO Top 10 dataset.&lt;/em>&lt;/p>
&lt;p>&lt;strong>Bushfire Opinions.&lt;/strong> Figure 3(a) shows the inter-opinion elasticities for the Bushfire Opinions dataset derived from the fitted OMM model. Within Twitter we observe substantial self-reinforcement among opinions, a sign of the echo chamber effect [21]. In addition, we see significant cross-reinforcement between far-right sympathizers and opponents, suggesting interactions or debates between opposing viewpoints. For Facebook, OMM identifies limited interaction between opinions due to Facebook far-right groups acting as a filter bubble.&lt;/p>
&lt;p>The results suggest that to suppress far-right opinions effectively we should avoid direct confrontation, which might unintentionally attract more attention to them. Instead, a more successful approach is to promote counter-arguments related to the far-right opinions. For example, on Twitter, to diminish the impact of the opinion &amp;ldquo;Australian bushfires were caused by random arsonists&amp;rdquo; (4+), the Opinion Market Model (OMM) suggests boosting opinions like &amp;ldquo;Climate change is real&amp;rdquo; (2-) and &amp;ldquo;Greens are not the cause of the bushfires&amp;rdquo; (0-). However, promoting the opposing view, such as &amp;ldquo;Australian bushfires were not caused by random arsonists&amp;rdquo; (4-), could actually backfire. Interestingly, the opinion &amp;ldquo;Bushfires are a normal summer occurrence in Australia&amp;rdquo; (5+) takes a different approach—it reinforces most moderate opinions and discourages far-right ones. Notably, this opinion (5+) seems to trigger &amp;ldquo;Climate change is real&amp;rdquo; (2-), possibly due to the stark contrast between these viewpoints. This effect is consistent across different platforms.&lt;/p>
&lt;p>&lt;strong>VEVO Artists.&lt;/strong> We show the fitted Youtube elasticities for the VEVO artists in Figure 3(b). We make three observations. Firstly, strong self-reinforcement exists for most artists, reflecting their dedicated fanbases. Secondly, the OMM captures significant artist interactions that align with real-world events and relationships. For example, it detects how Calvin Harris inhibits both Taylor Swift and Katy Perry, possibly linked to their past conflicts, while Harris&amp;rsquo; collaboration with Ariana Grande reinforces their positive relationship. Lastly, the unique relationships between artists (Katy Perry, Taylor Swift, and Ariana Grande) within the same genre (mainstream pop) highlight the intricate nature of fanbase dynamics, showing that similar artists can have diverse pairwise interactions rather than solely cooperating or competing for attention.&lt;/p>
&lt;h3 id="omm-as-a-testbed-for-interventions">OMM as a Testbed for Interventions&lt;/h3>
&lt;p>Positive interventions can have delayed impacts on the opinion ecosystem due to the interconnectedness of opinions. When an intervention boosts a particular opinion, it also indirectly boosts opinions that cooperate with it while inhibiting competitive opinions.&lt;/p>
&lt;p>We can leverage the OMM in a &amp;ldquo;what-if&amp;rdquo; scenario to understand long-term effects of media coverage in the bushfire case study. By synthetically varying intervention sizes, the OMM can serve as a testbed for prioritizing interventions and for designing A/B tests to determine causal impacts. The setup involves changing intervention volumes after a certain time, and then measuring the resulting percentage change in far-right opinions.&lt;/p>
&lt;p>&lt;img src="./omm_4.png" alt="">&lt;/p>
&lt;p style="text-align: center;">&lt;em>Figure 4. OMM-simulated percent changes in the far-right (+) opinion market shares on Facebook (left) and Twitter (right) by modulating the volume of reputable (R) and controversial (C) news for each opinion in the Bushfire Opinions dataset.&lt;/em>&lt;/p>
&lt;p>Figure 4 illustrates the average percentage changes in the market share of far-right opinions when manipulating the interventions \({R_i(t), C_i(t)}\) separately. For Facebook, reputable news generally suppresses far-right opinions and most controversial news reinforces them, except for certain topics like &amp;ldquo;Greens policies are the cause of the Australian bushfires&amp;rdquo; (\(R_0\)) and &amp;ldquo;Australian bushfires were caused by arsonists.&amp;rdquo; (\(R_4\)) On Twitter, both reputable and controversial news suppress far-right opinions, except for reputable news related to certain topics.&lt;/p>
&lt;p>The insights are twofold: firstly, Facebook's effect is moderate compared to Twitter due to Facebook's filter bubble nature. Secondly, indiscriminately increasing reputable news on Twitter can be ineffective and even counterproductive as it might attract more attention to far-right narratives and users (see \(R_3\) and \(R_4\)) [22].&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] Wu, S.; Rizoiu, M.-A.; and Xie, L. 2019. Estimating Attention Flow in Online Video Networks. CSCW, 3: 1–25.&lt;br>
[2] Nekmat, E. 2020. Nudge effect of fact-check alerts: source influence and media skepticism on sharing of news misinformation in social media. Social Media+ Society, 6(1).&lt;br>
[3] Young, G. K. 2022. How much is too much: the difficulties of social media content moderation. Information &amp;amp; Communications Technology Law, 31(1): 1–16.&lt;br>
[4] Jackson, S. 2019. The double-edged sword of banning ex- tremists from social media.&lt;br>
[5] Clayton, K.; Blair, S.; Busam, J. A.; Forstner, S.; Glance, J.; Green, G.; Kawata, A.; Kovvuri, A.; Martin, J.; Morgan, E.; et al. 2020. Real solutions for fake news? Measuring the effectiveness of general warnings and fact-check tags in reducing belief in false stories on social media. Political Behavior, 42(4): 1073–1095.&lt;br>
[6] Porter, E.; and Wood, T. J. 2021. Fact checks actually work, even on Facebook. But not enough people see them. The Washington Post.&lt;br>
[7] GIFCT. 2021. Content-Sharing Algorithms, Processes, and Positive Interventions Working Group.&lt;br>
[8] Haciyakupoglu, G.; Hui, J. Y.; Suguna, V.; Leong, D.; and Rahman, M. F. B. A. 2018. Countering fake news: A survey of recent global initiatives.&lt;br>
[9] Horowitz, M.; Cushion, S.; Dragomir, M.; Gutierrez Manjo ́n, S.; and Pantti, M. 2022. A framework for assessing the role of public service media organizations in countering disinformation. Digital Journalism, 10(5).&lt;br>
[10] Radsch, C. 2016. Media Development and Countering Violent Extremism: An Uneasy Relationship, a Need for Dialogue. Center for International Media Assistance. (2016).&lt;br>
[11] Cooper, L. G. 1993. Chapter 6 Market-share models. In Marketing, volume 5, 259–314. Elsevier.&lt;br>
[12] Browning, R.; Sulem, D.; Mengersen, K.; Rivoirard, V.; and Rousseau, J. 2021. Simple discrete-time self-exciting models can describe complex dynamic processes: A case study of COVID-19. PLoS ONE, 16.&lt;br>
[13] Kong, Q.; Booth, E.; Bailo, F.; Johns, A.; and Rizoiu, M.-A. 2022. Slipping to the Extreme: A Mixed Method to Explain How Extreme Opinions Infiltrate Online Discussions. In AAAI ICWSM, volume 16, 524–535.&lt;br>
[14] Ram, R.; Thomas, E.; Kernot, D.; and Rizoiu, M.-A. 2022. Detecting Extreme Ideologies in Shifting Landscapes: an Automatic &amp;amp; Context-Agnostic Approach. arXiv:2208.04097.&lt;br>
[15] Sheshadri, K.; and Singh, M. P. 2019. The public and legislative impact of hyperconcentrated topic news. Science Advances, 5(8).&lt;br>
[16] Kong, Q.; Rizoiu, M.-A.; and Xie, L. 2020. Describing and predicting online items with reshare cascades via dual mixture self-exciting processes. In 29th ACM CIKM, 645–654.&lt;br>
[17] Gruppi, M.; Horne, B. D.; and Adalı, S. 2020. NELA-GT- 2019: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles. arXiv:2003.08444.&lt;br>
[18] Wu, S.; Rizoiu, M.-A.; and Xie, L. 2019. Estimating Attention Flow in Online Video Networks. CSCW, 3: 1–25.&lt;br>
[19] Zarezade, A.; Khodadadi, A.; Farajtabar, M.; Rabiee, H. R.; and Zha, H. 2017. Correlated cascades: Compete or cooperate. AAAI 2017, 238–244.&lt;br>
[20] Valera, I.; and Gomez-Rodriguez, M. 2015. Modeling Adoption and Usage of Competing Products. In ICDM.&lt;br>
[21] Cinelli, M.; De Francisci Morales, G.; Galeazzi, A.; Quattrociocchi, W.; and Starnini, M. 2021. The echo chamber effect on social media. PNAS, 118(9).&lt;br>
[22] Peucker, M.; Fisher, T. J.; and Davey, J. 2022. Mainstream media use in far-right online ecosystems. Technical report.&lt;/p></description></item><item><title>Detecting extreme ideologies in shifting landscapes</title><link>https://www.behavioral-ds.science/blogpost/ideology_detection/</link><pubDate>Thu, 02 Feb 2023 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.science/blogpost/ideology_detection/</guid><description>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/csWMgU7R52Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
&lt;/iframe>
&lt;p>Also check out &lt;a href="../authors/rohit-ram/">Rohit&lt;/a>’s and &lt;a href="../authors/ma-rizoiu/">Andrei&lt;/a>’s article that just appeared in The Conversation: &lt;a href="https://theconversation.com/can-ideology-detecting-algorithms-catch-online-extremism-before-it-takes-hold-200629">Can ideology-detecting algorithms catch online extremism before it takes hold?&lt;/a>&lt;/p>
&lt;p>In this &lt;a href="https://arxiv.org/pdf/2208.04097.pdf">our latest working paper&lt;/a>, we propose a completely automatic end-to-end ideology detection pipeline for the detection and psychosocial profiling of left-right political ideology, as well as far-right ideological users.
The pipeline fills a crucial gap by providing flexible methodology and tooling for understanding ideologies and building early warning systems for extreme ideology-motivated (and potentially violent) activity.&lt;/p>
&lt;p>&lt;strong>Paper citation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code>Ram, R. and Rizoiu, M.A., 2022. You are what you browse: A robust framework for uncovering political ideology.
arXiv preprint arXiv:2208.04097.&lt;/code>&lt;/pre>
&lt;p>(&lt;em>see full paper here: &lt;a href="https://arxiv.org/pdf/2208.04097.pdf">https://arxiv.org/pdf/2208.04097.pdf&lt;/a>&lt;/em>)&lt;/p>
&lt;div id="ideology-in-an-online-world" class="section level2">
&lt;h2>Ideology in an Online World&lt;/h2>
&lt;p>Ideology determines how we make sense of much of the world, our opinions, and our political actions.
It is not a new concept; throughout history, it served as the context for unrest.
However, ideological spread and radicalization have entered a new paradigm in our ever-connected world. The internet is a significant source of information and spreads opinions quickly through social platforms.
In particular, the anonymity and lack of accountability often associated with online communication set up a supportive environment for spreading far-right ideologies and radicalizing individuals into extremist groups.
Far-right extremism is a form of ideology that advocates for ultranationalism, racism, and opposition to immigration and multiculturalism.
These ideologies strongly correlate with violence and terrorism and threaten individual and collective security.&lt;/p>
&lt;p>The Australian Security Intelligence Organisation (ASIO) raised concerns about Australians being radicalized very young and the rise of extremist movements in Australia through online technologies &lt;span class="citation">(&lt;a href="#ref-asio" role="doc-biblioref">&lt;span>“Director-General’s Annual Threat Assessment”&lt;/span> 2021&lt;/a>)&lt;/span>.
ASIO claimed that during the COVID period, 30-40% of their caseload was devoted to far-right extremism, up from 10-15% in 2016 &lt;span class="citation">(&lt;a href="#ref-guardian1" role="doc-biblioref">Karp 2020&lt;/a>)&lt;/span>.&lt;/p>
&lt;p>Unfortunately, Ideologically Motivated Violent Extremism (IMVE) continues to be an issue in Australia.
On December 12th, 2022, two Queensland police officers were killed while performing routine duties &lt;span class="citation">(&lt;a href="#ref-guardian2" role="doc-biblioref">Gillespie and McGowan 2022&lt;/a>)&lt;/span>.
Later investigations would uncover that the three people, who killed the officers, were active online in producing deep-state and religious conspiratorial content.
Their content has since been removed from mainstream social platforms but continues to be shared on conspiratorial websites.
Such extreme-leaning content often serves as a lead indicator of violent extremism (as was the case in this incident and the Christchurch Mosque Shootings three years prior). However, the tools to identify and understand the psychosocial characteristics of these extreme individuals and communities are lacking.&lt;/p>
&lt;p>In this work, we build an end-to-end ideology detection pipeline and psychosocial profiles of ideological groups.
We find that right-leaning individuals tend to use moral-vice language more than left-leaning and that far-right individuals’ grievance language (violence, hate, paranoia, etc.) significantly differs from the moderates.&lt;/p>
&lt;/div>
&lt;div id="signals-of-ideology" class="section level2">
&lt;h2>Signals of Ideology&lt;/h2>
&lt;p>In online social settings, researchers face numerous barriers that prevent using traditional methods.
Directly asking users for their ideologies has dubious success, infringes on platform T&amp;amp;Cs, and does not scale to online populations.
Inferring users’ ideologies from their activity also does not scale as the data requires is prohibitively expensive and tedious to compile.&lt;/p>
&lt;p>Instead, to reduce expert labor to feasible levels, researchers infer ideologies from signals in user behavior – such as whether they use political hashtags, retweet politicians, or follow political parties.
We dub these signals &lt;em>ideological proxies&lt;/em>.&lt;/p>
&lt;p>Importantly, these &lt;em>ideological proxies&lt;/em> for online users can still require laborious labeling by context-specific experts.
For example, the hashtag &lt;em>#ScottyFromMarketing&lt;/em> requires an up-to-date expert in Australian politics to uncover that it expresses an anti-right-wing ideology.
For many researchers:
- access to contextual experts is difficult,
- labeling of signals is still laborious and expensive,
- and context switches require relabelling (exasperating the above problems).&lt;/p>
&lt;p>Unfortunately, such context switches are commonplace, as the context changes with time, country, or social platform.
Figure &lt;a href="#fig:teaser">1&lt;/a> showcases the problem: most commonly used &lt;em>ideological proxies&lt;/em> can only be transferred in narrow circumstances (represented by the green dotted regions).
For example, following political parties is country-dependent, politicians come and go with time, and hashtags are platform-dependent.
As such, we desire an &lt;em>ideological proxy&lt;/em> that is robust to changes in context, requires no expert labeling and is true to the gold standard.&lt;/p>
&lt;div class="figure">&lt;span style="display:block;" id="fig:teaser">&lt;/span>
&lt;img src="teaser_v2.svg" alt="Schema showing that not all ideological proxies can context-switch." width="700px" />
&lt;p class="caption">
Figure 1: Schema showing that not all ideological proxies can context-switch.
&lt;/p>
&lt;/div>
&lt;!-- ![Not all ideological proxies can context-switch](teaser.png) -->
&lt;p>Furthermore, the &lt;em>ideological proxies&lt;/em> are often sparse among users; however, we would ideally like to detect influence amongst the entire population of users (as taking only active users could bias our inferences).
We further desire a method for inferring the ideology of (potentially inactive) users without direct &lt;em>ideological proxy&lt;/em> information.&lt;/p>
&lt;/div>
&lt;div id="our-solution-you-are-what-you-browse" class="section level2">
&lt;h2>Our Solution: You are what you browse&lt;/h2>
&lt;p>Our solution is a large-scale end-to-end ideology detection pipeline that can be used to profile entire populations of users.
The solution has two main components; the media proxy and the inference architecture.
The media proxy allows for labeling a subset of users, and the inference architecture allows for propagating these labels to the remaining users via socially-informed homophilic lenses.&lt;/p>
&lt;div id="the-media-proxy" class="section level3">
&lt;h3>The Media Proxy&lt;/h3>
&lt;p>For the first part of our work, we generate a proxy based on media-sharing behavior, which satisfies the desiderata.&lt;/p>
&lt;p>We generate the media proxy via two media slant datasets (although many are widely available).
The first is an extensive survey of media consumption behaviors conducted by Reuters &lt;span class="citation">(&lt;a href="#ref-newman2019reuters" role="doc-biblioref">Newman et al. 2019&lt;/a>)&lt;/span> in several countries in 2020 and 2021.
Participants reported the media publications they consume and their own political ideology.
We estimate the slant of a media source for each country and year as the average ideology of the participants who consume it.
The second dataset is the Allsides Media Bias Dataset &lt;span class="citation">(&lt;a href="#ref-sides2018media" role="doc-biblioref">Sides 2018&lt;/a>)&lt;/span>, which contains an expert-curated set of media publications.
The Allsides dataset contains mostly American-based media; conversely, Reuters covers the major media outlets in each country.
Given that each country and period will have a different conception of ideologies, we calibrate Reuter’s media slants to approximate the Allsides (minimizing the mean-squared error). Figure &lt;a href="#fig:slants">2&lt;/a> shows the slants for each media website within the Reuters dataset.&lt;/p>
&lt;div class="figure">&lt;span style="display:block;" id="fig:slants">&lt;/span>
&lt;img src="url_slants.svg" alt="Plot showing the slants for various media websites." width="700px" />
&lt;p class="caption">
Figure 2: Plot showing the slants for various media websites.
&lt;/p>
&lt;/div>
&lt;!-- ![The slants for various media websites](./url_slants.png) -->
&lt;!-- We finally define the media slant of a media website, as its average over the country, year, and sources, and -->
&lt;p>Finally, we quantify a user’s ideology as the average ideology of their shared media.&lt;/p>
&lt;p>The media proxy resolves the issue of context switching; since it is applicable across many contexts and can be used widely in a fully automated fashion.
This allows us to create an end-to-end ideology detection pipeline.&lt;/p>
&lt;p>We further define methods to classify far-right users from their media-sharing behaviors, which we fully describe in the paper.&lt;/p>
&lt;/div>
&lt;div id="the-inference-architecture" class="section level3">
&lt;h3>The Inference Architecture&lt;/h3>
&lt;p>In the second part of our work, we define an inference architecture that allows inferring the ideological labels of the remaining users – e.g., users who do not share any URLs.
Our inference architecture relies on the sociological principle of homophily, where we hypothesize that similar users will share a similar ideology.
We measure homophily through three distinct lenses;&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;em>Lexical&lt;/em>: Users with similar language will have similar ideology&lt;/li>
&lt;li>&lt;em>Hashtag&lt;/em>: Users who participate in similar topics of discussion share a similar ideology&lt;/li>
&lt;li>&lt;em>Resharing&lt;/em>: Users who consume similar content (signaled via resharing of other users) will share a similar ideology&lt;/li>
&lt;/ol>
&lt;p>Through these lenses, we utilize an AutoML model, FLAML &lt;span class="citation">(&lt;a href="#ref-wang2021flaml" role="doc-biblioref">Wang et al. 2021&lt;/a>)&lt;/span> (with the LightGBM architecture), trained on users identified via an ideological proxy to propagate the labels to the remaining users and generate a complete ideological profile for a dataset.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="the-data" class="section level2">
&lt;h2>The Data&lt;/h2>
&lt;p>We utilize several large-scale datasets from various platforms to showcase the relative ease of applying our end-to-end pipeline.
The datasets’ characteristics are described in Table &lt;a href="#tab:datasets">1&lt;/a>.
&lt;!-- For evaluation purposes, we use \#QandA based on the popular ABC panel show. -->&lt;/p>
&lt;table>
&lt;caption>&lt;span id="tab:datasets">Table 1: &lt;/span>The datasets used through-out the analysis, with the number of users, posts, and affliated country.&lt;/caption>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">Dataset&lt;/th>
&lt;th align="left">Users&lt;/th>
&lt;th align="left">Posts&lt;/th>
&lt;th align="left">Country&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">#Qanda&lt;/td>
&lt;td align="left">103,074&lt;/td>
&lt;td align="left">768,808&lt;/td>
&lt;td align="left">AUS&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">#Ausvotes&lt;/td>
&lt;td align="left">273,874&lt;/td>
&lt;td align="left">5,033,982&lt;/td>
&lt;td align="left">AUS&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">#SocialSense&lt;/td>
&lt;td align="left">49,442&lt;/td>
&lt;td align="left">358,292&lt;/td>
&lt;td align="left">AUS&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">Riot&lt;/td>
&lt;td align="left">574,281&lt;/td>
&lt;td align="left">1,067,794&lt;/td>
&lt;td align="left">US&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">Parler&lt;/td>
&lt;td align="left">120,048&lt;/td>
&lt;td align="left">603,820&lt;/td>
&lt;td align="left">US&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- ![The datasets we apply our pipeline on](datasets.png) -->
&lt;div id="psychosocial-profiles-of-the-ideological-groups" class="section level3">
&lt;h3>Psychosocial profiles of the Ideological Groups&lt;/h3>
&lt;p>Large-scale profiling of entire online populations gives us significant insights into the characteristics of online populations.
We apply our inferred ideological labels of online users in two critical ways:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>We derive a method for distinguishing the left and the right in terms of their moral language.&lt;/li>
&lt;li>We derive a method for distinguishing moderate and extreme ideologies in terms of their grievance language.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Distinguishing Left and Right&lt;/strong>:
We utilize the FrameAxis &lt;span class="citation">(&lt;a href="#ref-mokhberian2020moral" role="doc-biblioref">Mokhberian et al. 2020&lt;/a>)&lt;/span> methodology to metricize each user’s association with each of the five Moral Foundations &lt;span class="citation">(&lt;a href="#ref-graham2013moral" role="doc-biblioref">Graham et al. 2013&lt;/a>)&lt;/span> in terms of their vice and virtue axes.
Given the measures of the Moral Foundations in the user language, we can start to detect in what way the left and the right differ.
To do this, we find each ideological group’s mean vice and virtue scores and compare these to the neutral group.
Figure &lt;a href="#fig:mft">3&lt;/a> shows the outcome of this analysis on the SocialSense dataset.&lt;/p>
&lt;div class="figure">&lt;span style="display:block;" id="fig:mft">&lt;/span>
&lt;img src="mft_diff_plot.svg" alt="Plot of the moral foundations of ideological groups in the SocialSense dataset, showing that the left prefer virtue and the right prefer vice language." width="700px" />
&lt;p class="caption">
Figure 3: Plot of the moral foundations of ideological groups in the SocialSense dataset, showing that the left prefer virtue and the right prefer vice language.
&lt;/p>
&lt;/div>
&lt;!-- ![Plot showing that the left prefer virtue and the right prefer vice language](mft_diff_plot.png) -->
&lt;p>We see that the left prefers the language of virtue, while the right prefers the language of vice.
This trend is largely consistent across all datasets.&lt;/p>
&lt;p>&lt;strong>Distinguishing Moderates and Extremes&lt;/strong>: We similarly generate measures via the Grievance Dictionary &lt;span class="citation">(&lt;a href="#ref-van2021grievance" role="doc-biblioref">Van der Vegt et al. 2021&lt;/a>)&lt;/span>, a threat assessment tool designed to highlight potential threats through their language.
Similar to the previous plot, we investigate the distribution of grievance scores for the ideological groups.
However, here we measure the difference between distributions with the Signed KL-divergence (a measure of the difference in the location and shape of distributions).
Figure &lt;a href="#fig:grievance">4&lt;/a> shows the results for the Ausvotes dataset.&lt;/p>
&lt;div class="figure">&lt;span style="display:block;" id="fig:grievance">&lt;/span>
&lt;img src="grievance_diff_plot.svg" alt="Plot of the grievance categories of ideological groups in the #Ausvotes dataset, showing that the far-right is significantly different." width="700px" />
&lt;p class="caption">
Figure 4: Plot of the grievance categories of ideological groups in the #Ausvotes dataset, showing that the far-right is significantly different.
&lt;/p>
&lt;/div>
&lt;!-- ![Plot showing that the far-right use grievance language](grievance_diff_plot.png) -->
&lt;p>We observe that the far-right’s usage of grievance language is significantly different from the moderate ideological groups.
This adds evidence to the growing concern that members of the far-right may vent their frustration and participate in violent behavior.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="conclusion" class="section level2">
&lt;h2>Conclusion&lt;/h2>
&lt;p>In this work, we build a fully automatic end-to-end ideology detection pipeline for left-right and far-right detection.
Importantly, with the pipeline, we can show the differences between the left and right, and moderates and extremes in terms of psychosocial language, across a range of diverse datasets.&lt;/p>
&lt;div id="references" class="section level3 unnumbered">
&lt;h3>References&lt;/h3>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-asio" class="csl-entry">
&lt;span>“Director-General’s Annual Threat Assessment.”&lt;/span> 2021. &lt;em>ASIO&lt;/em>. &lt;a href="https://www.asio.gov.au/resources/speeches-and-statements/director-generals-annual-threat-assessment-2021">https://www.asio.gov.au/resources/speeches-and-statements/director-generals-annual-threat-assessment-2021&lt;/a>.
&lt;/div>
&lt;div id="ref-guardian2" class="csl-entry">
Gillespie, Eden, and Michael McGowan. 2022. &lt;span>“Queensland Shooting: Gareth and Stacey Train Published YouTube Video After Killing Police Officers.”&lt;/span> &lt;a href="https://www.theguardian.com/australia-news/2022/dec/16/queensland-shooting-gareth-and-stacey-train-youtube-video-published-after-killing-police" class="uri">https://www.theguardian.com/australia-news/2022/dec/16/queensland-shooting-gareth-and-stacey-train-youtube-video-published-after-killing-police&lt;/a>; The Guardian.
&lt;/div>
&lt;div id="ref-graham2013moral" class="csl-entry">
Graham, Jesse, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, and Peter H Ditto. 2013. &lt;span>“Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism.”&lt;/span> In &lt;em>Advances in Experimental Social Psychology&lt;/em>, 47:55–130. Elsevier.
&lt;/div>
&lt;div id="ref-guardian1" class="csl-entry">
Karp, Paul. 2020. &lt;span>“Asio Reveals up to 40% of Its Counter-Terrorism Cases Involve Far-Right Violent Extremism.”&lt;/span> &lt;a href="https://www.theguardian.com/australia-news/2020/sep/22/asio-reveals-up-to-40-of-its-counter-terrorism-cases-involve-far-right-violent-extremism" class="uri">https://www.theguardian.com/australia-news/2020/sep/22/asio-reveals-up-to-40-of-its-counter-terrorism-cases-involve-far-right-violent-extremism&lt;/a>; The Guardian.
&lt;/div>
&lt;div id="ref-mokhberian2020moral" class="csl-entry">
Mokhberian, Negar, Andrés Abeliuk, Patrick Cummings, and Kristina Lerman. 2020. &lt;span>“Moral Framing and Ideological Bias of News.”&lt;/span> In &lt;em>International Conference on Social Informatics&lt;/em>, 206–19. Springer.
&lt;/div>
&lt;div id="ref-newman2019reuters" class="csl-entry">
Newman, Nic, Richard Fletcher, Antonis Kalogeropoulos, DAL Levy, and Rasmus Kleis Nielsen. 2019. &lt;span>“Reuters Institute Digital News Report 2018. Reuters Institute for the Study of Journalism.”&lt;/span> Oxford.
&lt;/div>
&lt;div id="ref-sides2018media" class="csl-entry">
Sides, All. 2018. &lt;span>“Media Bias Ratings.”&lt;/span> &lt;em>Allsides. Com&lt;/em>. &lt;a href="https://www.allsides.com/media-bias/ratings">https://www.allsides.com/media-bias/ratings&lt;/a>.
&lt;/div>
&lt;div id="ref-van2021grievance" class="csl-entry">
Van der Vegt, Isabelle, Maximilian Mozes, Bennett Kleinberg, and Paul Gill. 2021. &lt;span>“The Grievance Dictionary: Understanding Threatening Language Use.”&lt;/span> &lt;em>Behavior Research Methods&lt;/em> 53 (5): 2105–19.
&lt;/div>
&lt;div id="ref-wang2021flaml" class="csl-entry">
Wang, Chi, Qingyun Wu, Markus Weimer, and Erkang Zhu. 2021. &lt;span>“FLAML: A Fast and Lightweight Automl Library.”&lt;/span> &lt;em>Proceedings of Machine Learning and Systems&lt;/em> 3: 434–47.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Detecting extreme ideologies in shifting landscapes</title><link>https://www.behavioral-ds.science/theme1_content/ideology_detection/</link><pubDate>Thu, 02 Feb 2023 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.science/theme1_content/ideology_detection/</guid><description>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/csWMgU7R52Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
&lt;/iframe>
&lt;p>Also check out &lt;a href="../authors/rohit-ram/">Rohit&lt;/a>’s and &lt;a href="../authors/ma-rizoiu/">Andrei&lt;/a>’s article that just appeared in The Conversation: &lt;a href="https://theconversation.com/can-ideology-detecting-algorithms-catch-online-extremism-before-it-takes-hold-200629">Can ideology-detecting algorithms catch online extremism before it takes hold?&lt;/a>&lt;/p>
&lt;p>In this &lt;a href="https://arxiv.org/pdf/2208.04097.pdf">our latest working paper&lt;/a>, we propose a completely automatic end-to-end ideology detection pipeline for the detection and psychosocial profiling of left-right political ideology, as well as far-right ideological users.
The pipeline fills a crucial gap by providing flexible methodology and tooling for understanding ideologies and building early warning systems for extreme ideology-motivated (and potentially violent) activity.&lt;/p>
&lt;p>&lt;strong>Paper citation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code>Ram, R. and Rizoiu, M.A., 2022. You are what you browse: A robust framework for uncovering political ideology.
arXiv preprint arXiv:2208.04097.&lt;/code>&lt;/pre>
&lt;p>(&lt;em>see full paper here: &lt;a href="https://arxiv.org/pdf/2208.04097.pdf">https://arxiv.org/pdf/2208.04097.pdf&lt;/a>&lt;/em>)&lt;/p>
&lt;div id="ideology-in-an-online-world" class="section level2">
&lt;h2>Ideology in an Online World&lt;/h2>
&lt;p>Ideology determines how we make sense of much of the world, our opinions, and our political actions.
It is not a new concept; throughout history, it served as the context for unrest.
However, ideological spread and radicalization have entered a new paradigm in our ever-connected world. The internet is a significant source of information and spreads opinions quickly through social platforms.
In particular, the anonymity and lack of accountability often associated with online communication set up a supportive environment for spreading far-right ideologies and radicalizing individuals into extremist groups.
Far-right extremism is a form of ideology that advocates for ultranationalism, racism, and opposition to immigration and multiculturalism.
These ideologies strongly correlate with violence and terrorism and threaten individual and collective security.&lt;/p>
&lt;p>The Australian Security Intelligence Organisation (ASIO) raised concerns about Australians being radicalized very young and the rise of extremist movements in Australia through online technologies &lt;span class="citation">(&lt;a href="#ref-asio" role="doc-biblioref">&lt;span>“Director-General’s Annual Threat Assessment”&lt;/span> 2021&lt;/a>)&lt;/span>.
ASIO claimed that during the COVID period, 30-40% of their caseload was devoted to far-right extremism, up from 10-15% in 2016 &lt;span class="citation">(&lt;a href="#ref-guardian1" role="doc-biblioref">Karp 2020&lt;/a>)&lt;/span>.&lt;/p>
&lt;p>Unfortunately, Ideologically Motivated Violent Extremism (IMVE) continues to be an issue in Australia.
On December 12th, 2022, two Queensland police officers were killed while performing routine duties &lt;span class="citation">(&lt;a href="#ref-guardian2" role="doc-biblioref">Gillespie and McGowan 2022&lt;/a>)&lt;/span>.
Later investigations would uncover that the three people, who killed the officers, were active online in producing deep-state and religious conspiratorial content.
Their content has since been removed from mainstream social platforms but continues to be shared on conspiratorial websites.
Such extreme-leaning content often serves as a lead indicator of violent extremism (as was the case in this incident and the Christchurch Mosque Shootings three years prior). However, the tools to identify and understand the psychosocial characteristics of these extreme individuals and communities are lacking.&lt;/p>
&lt;p>In this work, we build an end-to-end ideology detection pipeline and psychosocial profiles of ideological groups.
We find that right-leaning individuals tend to use moral-vice language more than left-leaning and that far-right individuals’ grievance language (violence, hate, paranoia, etc.) significantly differs from the moderates.&lt;/p>
&lt;/div>
&lt;div id="signals-of-ideology" class="section level2">
&lt;h2>Signals of Ideology&lt;/h2>
&lt;p>In online social settings, researchers face numerous barriers that prevent using traditional methods.
Directly asking users for their ideologies has dubious success, infringes on platform T&amp;amp;Cs, and does not scale to online populations.
Inferring users’ ideologies from their activity also does not scale as the data requires is prohibitively expensive and tedious to compile.&lt;/p>
&lt;p>Instead, to reduce expert labor to feasible levels, researchers infer ideologies from signals in user behavior – such as whether they use political hashtags, retweet politicians, or follow political parties.
We dub these signals &lt;em>ideological proxies&lt;/em>.&lt;/p>
&lt;p>Importantly, these &lt;em>ideological proxies&lt;/em> for online users can still require laborious labeling by context-specific experts.
For example, the hashtag &lt;em>#ScottyFromMarketing&lt;/em> requires an up-to-date expert in Australian politics to uncover that it expresses an anti-right-wing ideology.
For many researchers:
- access to contextual experts is difficult,
- labeling of signals is still laborious and expensive,
- and context switches require relabelling (exasperating the above problems).&lt;/p>
&lt;p>Unfortunately, such context switches are commonplace, as the context changes with time, country, or social platform.
Figure &lt;a href="#fig:teaser">1&lt;/a> showcases the problem: most commonly used &lt;em>ideological proxies&lt;/em> can only be transferred in narrow circumstances (represented by the green dotted regions).
For example, following political parties is country-dependent, politicians come and go with time, and hashtags are platform-dependent.
As such, we desire an &lt;em>ideological proxy&lt;/em> that is robust to changes in context, requires no expert labeling and is true to the gold standard.&lt;/p>
&lt;div class="figure">&lt;span style="display:block;" id="fig:teaser">&lt;/span>
&lt;img src="teaser_v2.svg" alt="Schema showing that not all ideological proxies can context-switch." width="700px" />
&lt;p class="caption">
Figure 1: Schema showing that not all ideological proxies can context-switch.
&lt;/p>
&lt;/div>
&lt;!-- ![Not all ideological proxies can context-switch](teaser.png) -->
&lt;p>Furthermore, the &lt;em>ideological proxies&lt;/em> are often sparse among users; however, we would ideally like to detect influence amongst the entire population of users (as taking only active users could bias our inferences).
We further desire a method for inferring the ideology of (potentially inactive) users without direct &lt;em>ideological proxy&lt;/em> information.&lt;/p>
&lt;/div>
&lt;div id="our-solution-you-are-what-you-browse" class="section level2">
&lt;h2>Our Solution: You are what you browse&lt;/h2>
&lt;p>Our solution is a large-scale end-to-end ideology detection pipeline that can be used to profile entire populations of users.
The solution has two main components; the media proxy and the inference architecture.
The media proxy allows for labeling a subset of users, and the inference architecture allows for propagating these labels to the remaining users via socially-informed homophilic lenses.&lt;/p>
&lt;div id="the-media-proxy" class="section level3">
&lt;h3>The Media Proxy&lt;/h3>
&lt;p>For the first part of our work, we generate a proxy based on media-sharing behavior, which satisfies the desiderata.&lt;/p>
&lt;p>We generate the media proxy via two media slant datasets (although many are widely available).
The first is an extensive survey of media consumption behaviors conducted by Reuters &lt;span class="citation">(&lt;a href="#ref-newman2019reuters" role="doc-biblioref">Newman et al. 2019&lt;/a>)&lt;/span> in several countries in 2020 and 2021.
Participants reported the media publications they consume and their own political ideology.
We estimate the slant of a media source for each country and year as the average ideology of the participants who consume it.
The second dataset is the Allsides Media Bias Dataset &lt;span class="citation">(&lt;a href="#ref-sides2018media" role="doc-biblioref">Sides 2018&lt;/a>)&lt;/span>, which contains an expert-curated set of media publications.
The Allsides dataset contains mostly American-based media; conversely, Reuters covers the major media outlets in each country.
Given that each country and period will have a different conception of ideologies, we calibrate Reuter’s media slants to approximate the Allsides (minimizing the mean-squared error). Figure &lt;a href="#fig:slants">2&lt;/a> shows the slants for each media website within the Reuters dataset.&lt;/p>
&lt;div class="figure">&lt;span style="display:block;" id="fig:slants">&lt;/span>
&lt;img src="url_slants.svg" alt="Plot showing the slants for various media websites." width="700px" />
&lt;p class="caption">
Figure 2: Plot showing the slants for various media websites.
&lt;/p>
&lt;/div>
&lt;!-- ![The slants for various media websites](./url_slants.png) -->
&lt;!-- We finally define the media slant of a media website, as its average over the country, year, and sources, and -->
&lt;p>Finally, we quantify a user’s ideology as the average ideology of their shared media.&lt;/p>
&lt;p>The media proxy resolves the issue of context switching; since it is applicable across many contexts and can be used widely in a fully automated fashion.
This allows us to create an end-to-end ideology detection pipeline.&lt;/p>
&lt;p>We further define methods to classify far-right users from their media-sharing behaviors, which we fully describe in the paper.&lt;/p>
&lt;/div>
&lt;div id="the-inference-architecture" class="section level3">
&lt;h3>The Inference Architecture&lt;/h3>
&lt;p>In the second part of our work, we define an inference architecture that allows inferring the ideological labels of the remaining users – e.g., users who do not share any URLs.
Our inference architecture relies on the sociological principle of homophily, where we hypothesize that similar users will share a similar ideology.
We measure homophily through three distinct lenses;&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;em>Lexical&lt;/em>: Users with similar language will have similar ideology&lt;/li>
&lt;li>&lt;em>Hashtag&lt;/em>: Users who participate in similar topics of discussion share a similar ideology&lt;/li>
&lt;li>&lt;em>Resharing&lt;/em>: Users who consume similar content (signaled via resharing of other users) will share a similar ideology&lt;/li>
&lt;/ol>
&lt;p>Through these lenses, we utilize an AutoML model, FLAML &lt;span class="citation">(&lt;a href="#ref-wang2021flaml" role="doc-biblioref">Wang et al. 2021&lt;/a>)&lt;/span> (with the LightGBM architecture), trained on users identified via an ideological proxy to propagate the labels to the remaining users and generate a complete ideological profile for a dataset.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="the-data" class="section level2">
&lt;h2>The Data&lt;/h2>
&lt;p>We utilize several large-scale datasets from various platforms to showcase the relative ease of applying our end-to-end pipeline.
The datasets’ characteristics are described in Table &lt;a href="#tab:datasets">1&lt;/a>.
&lt;!-- For evaluation purposes, we use \#QandA based on the popular ABC panel show. -->&lt;/p>
&lt;table>
&lt;caption>&lt;span id="tab:datasets">Table 1: &lt;/span>The datasets used through-out the analysis, with the number of users, posts, and affliated country.&lt;/caption>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">Dataset&lt;/th>
&lt;th align="left">Users&lt;/th>
&lt;th align="left">Posts&lt;/th>
&lt;th align="left">Country&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">#Qanda&lt;/td>
&lt;td align="left">103,074&lt;/td>
&lt;td align="left">768,808&lt;/td>
&lt;td align="left">AUS&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">#Ausvotes&lt;/td>
&lt;td align="left">273,874&lt;/td>
&lt;td align="left">5,033,982&lt;/td>
&lt;td align="left">AUS&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">#SocialSense&lt;/td>
&lt;td align="left">49,442&lt;/td>
&lt;td align="left">358,292&lt;/td>
&lt;td align="left">AUS&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">Riot&lt;/td>
&lt;td align="left">574,281&lt;/td>
&lt;td align="left">1,067,794&lt;/td>
&lt;td align="left">US&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">Parler&lt;/td>
&lt;td align="left">120,048&lt;/td>
&lt;td align="left">603,820&lt;/td>
&lt;td align="left">US&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- ![The datasets we apply our pipeline on](datasets.png) -->
&lt;div id="psychosocial-profiles-of-the-ideological-groups" class="section level3">
&lt;h3>Psychosocial profiles of the Ideological Groups&lt;/h3>
&lt;p>Large-scale profiling of entire online populations gives us significant insights into the characteristics of online populations.
We apply our inferred ideological labels of online users in two critical ways:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>We derive a method for distinguishing the left and the right in terms of their moral language.&lt;/li>
&lt;li>We derive a method for distinguishing moderate and extreme ideologies in terms of their grievance language.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Distinguishing Left and Right&lt;/strong>:
We utilize the FrameAxis &lt;span class="citation">(&lt;a href="#ref-mokhberian2020moral" role="doc-biblioref">Mokhberian et al. 2020&lt;/a>)&lt;/span> methodology to metricize each user’s association with each of the five Moral Foundations &lt;span class="citation">(&lt;a href="#ref-graham2013moral" role="doc-biblioref">Graham et al. 2013&lt;/a>)&lt;/span> in terms of their vice and virtue axes.
Given the measures of the Moral Foundations in the user language, we can start to detect in what way the left and the right differ.
To do this, we find each ideological group’s mean vice and virtue scores and compare these to the neutral group.
Figure &lt;a href="#fig:mft">3&lt;/a> shows the outcome of this analysis on the SocialSense dataset.&lt;/p>
&lt;div class="figure">&lt;span style="display:block;" id="fig:mft">&lt;/span>
&lt;img src="mft_diff_plot.svg" alt="Plot of the moral foundations of ideological groups in the SocialSense dataset, showing that the left prefer virtue and the right prefer vice language." width="700px" />
&lt;p class="caption">
Figure 3: Plot of the moral foundations of ideological groups in the SocialSense dataset, showing that the left prefer virtue and the right prefer vice language.
&lt;/p>
&lt;/div>
&lt;!-- ![Plot showing that the left prefer virtue and the right prefer vice language](mft_diff_plot.png) -->
&lt;p>We see that the left prefers the language of virtue, while the right prefers the language of vice.
This trend is largely consistent across all datasets.&lt;/p>
&lt;p>&lt;strong>Distinguishing Moderates and Extremes&lt;/strong>: We similarly generate measures via the Grievance Dictionary &lt;span class="citation">(&lt;a href="#ref-van2021grievance" role="doc-biblioref">Van der Vegt et al. 2021&lt;/a>)&lt;/span>, a threat assessment tool designed to highlight potential threats through their language.
Similar to the previous plot, we investigate the distribution of grievance scores for the ideological groups.
However, here we measure the difference between distributions with the Signed KL-divergence (a measure of the difference in the location and shape of distributions).
Figure &lt;a href="#fig:grievance">4&lt;/a> shows the results for the Ausvotes dataset.&lt;/p>
&lt;div class="figure">&lt;span style="display:block;" id="fig:grievance">&lt;/span>
&lt;img src="grievance_diff_plot.svg" alt="Plot of the grievance categories of ideological groups in the #Ausvotes dataset, showing that the far-right is significantly different." width="700px" />
&lt;p class="caption">
Figure 4: Plot of the grievance categories of ideological groups in the #Ausvotes dataset, showing that the far-right is significantly different.
&lt;/p>
&lt;/div>
&lt;!-- ![Plot showing that the far-right use grievance language](grievance_diff_plot.png) -->
&lt;p>We observe that the far-right’s usage of grievance language is significantly different from the moderate ideological groups.
This adds evidence to the growing concern that members of the far-right may vent their frustration and participate in violent behavior.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="conclusion" class="section level2">
&lt;h2>Conclusion&lt;/h2>
&lt;p>In this work, we build a fully automatic end-to-end ideology detection pipeline for left-right and far-right detection.
Importantly, with the pipeline, we can show the differences between the left and right, and moderates and extremes in terms of psychosocial language, across a range of diverse datasets.&lt;/p>
&lt;div id="references" class="section level3 unnumbered">
&lt;h3>References&lt;/h3>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-asio" class="csl-entry">
&lt;span>“Director-General’s Annual Threat Assessment.”&lt;/span> 2021. &lt;em>ASIO&lt;/em>. &lt;a href="https://www.asio.gov.au/resources/speeches-and-statements/director-generals-annual-threat-assessment-2021">https://www.asio.gov.au/resources/speeches-and-statements/director-generals-annual-threat-assessment-2021&lt;/a>.
&lt;/div>
&lt;div id="ref-guardian2" class="csl-entry">
Gillespie, Eden, and Michael McGowan. 2022. &lt;span>“Queensland Shooting: Gareth and Stacey Train Published YouTube Video After Killing Police Officers.”&lt;/span> &lt;a href="https://www.theguardian.com/australia-news/2022/dec/16/queensland-shooting-gareth-and-stacey-train-youtube-video-published-after-killing-police" class="uri">https://www.theguardian.com/australia-news/2022/dec/16/queensland-shooting-gareth-and-stacey-train-youtube-video-published-after-killing-police&lt;/a>; The Guardian.
&lt;/div>
&lt;div id="ref-graham2013moral" class="csl-entry">
Graham, Jesse, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, and Peter H Ditto. 2013. &lt;span>“Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism.”&lt;/span> In &lt;em>Advances in Experimental Social Psychology&lt;/em>, 47:55–130. Elsevier.
&lt;/div>
&lt;div id="ref-guardian1" class="csl-entry">
Karp, Paul. 2020. &lt;span>“Asio Reveals up to 40% of Its Counter-Terrorism Cases Involve Far-Right Violent Extremism.”&lt;/span> &lt;a href="https://www.theguardian.com/australia-news/2020/sep/22/asio-reveals-up-to-40-of-its-counter-terrorism-cases-involve-far-right-violent-extremism" class="uri">https://www.theguardian.com/australia-news/2020/sep/22/asio-reveals-up-to-40-of-its-counter-terrorism-cases-involve-far-right-violent-extremism&lt;/a>; The Guardian.
&lt;/div>
&lt;div id="ref-mokhberian2020moral" class="csl-entry">
Mokhberian, Negar, Andrés Abeliuk, Patrick Cummings, and Kristina Lerman. 2020. &lt;span>“Moral Framing and Ideological Bias of News.”&lt;/span> In &lt;em>International Conference on Social Informatics&lt;/em>, 206–19. Springer.
&lt;/div>
&lt;div id="ref-newman2019reuters" class="csl-entry">
Newman, Nic, Richard Fletcher, Antonis Kalogeropoulos, DAL Levy, and Rasmus Kleis Nielsen. 2019. &lt;span>“Reuters Institute Digital News Report 2018. Reuters Institute for the Study of Journalism.”&lt;/span> Oxford.
&lt;/div>
&lt;div id="ref-sides2018media" class="csl-entry">
Sides, All. 2018. &lt;span>“Media Bias Ratings.”&lt;/span> &lt;em>Allsides. Com&lt;/em>. &lt;a href="https://www.allsides.com/media-bias/ratings">https://www.allsides.com/media-bias/ratings&lt;/a>.
&lt;/div>
&lt;div id="ref-van2021grievance" class="csl-entry">
Van der Vegt, Isabelle, Maximilian Mozes, Bennett Kleinberg, and Paul Gill. 2021. &lt;span>“The Grievance Dictionary: Understanding Threatening Language Use.”&lt;/span> &lt;em>Behavior Research Methods&lt;/em> 53 (5): 2105–19.
&lt;/div>
&lt;div id="ref-wang2021flaml" class="csl-entry">
Wang, Chi, Qingyun Wu, Markus Weimer, and Erkang Zhu. 2021. &lt;span>“FLAML: A Fast and Lightweight Automl Library.”&lt;/span> &lt;em>Proceedings of Machine Learning and Systems&lt;/em> 3: 434–47.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Slipping to the Extreme: A Mixed-Method to Explain How Extreme Opinions Infiltrate Online Discussions</title><link>https://www.behavioral-ds.science/blogpost/icwsm2022/</link><pubDate>Mon, 13 Dec 2021 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.science/blogpost/icwsm2022/</guid><description>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/HwFq3ywanp4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;p>In our &lt;a href="https://arxiv.org/pdf/2109.00302.pdf">recent paper&lt;/a> accepted at &lt;a href="https://www.icwsm.org/2022/index.html/">ICWSM 2022&lt;/a>, we propose a complete solution to accelerate the qualitative analysis of problematic online speech — with a specific focus on opinions emerging from online communities — by leveraging machine learning algorithms.&lt;/p>
&lt;p>&lt;strong>Paper citation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code>Quyu Kong, Emily Booth, Francesco Bailo, Amelia Johns, and Marian-Andrei
Rizoiu. Slipping to the Extreme: A Mixed-Method to Explain How Extreme
Opinions Infiltrate Online Discussions. In: Proceedings of the 16TH AAAI
International Conference on Web and Social Media, 2022.
&lt;/code>&lt;/pre>
&lt;p>(see full paper here: &lt;a href="https://arxiv.org/pdf/2109.00302.pdf">https://arxiv.org/pdf/2109.00302.pdf&lt;/a>)&lt;/p>
&lt;h3 id="problematic-speech-a-modern-plague">Problematic Speech: A Modern Plague&lt;/h3>
&lt;p>Problematic speech is online interactions, speech, and artifacts that are inaccurate, misleading, inappropriately attributed, or altogether fabricated [1].
In 2020, the COVID-19 pandemic alerted the world to complex issues that arise from social media platforms circulating user-generated misinformation, hate speech, and conspiracy theories [2].
There are several primary types of quantitative methods for addressing problematic information, including large-scale monitoring of social media datasets [3], understanding platforms, users, and networks contributing to the &amp;ldquo;infodemic&amp;rdquo; [4], and predicting future pathways of information spreading [5].
These studies provide valuable insights into understanding how problematic information spreads and detecting which sources are reshared frequently and by which accounts.
However, these approaches often have less to say about why certain opinions and views gain traction with vulnerable groups and online communities.&lt;/p>
&lt;p>Qualitative research methods are well placed to address this gap.
They provide rich, contextual insights into online communities&amp;rsquo; social beliefs, values, and practices, which shape how information is shared and how opinions are formed [6].
Nevertheless, a common criticism of qualitative research is that the in-depth knowledge comes at the expense of generating insights of limited representativeness and weak robustness of the findings.
Therefore, there is a gap between the depth of insight gained from ethnographic and qualitative approaches and the breadth of knowledge gained from computational methods from data science.&lt;/p>
&lt;p>Our work fills this gap by proposing a mixed-method approach that combines qualitative insights, large-scale data collection, and human-in-the-loop machine learning approaches.
We apply our method to map in-depth and in-breadth the problematic information around four topics:
&lt;em>2019-20 Australian bushfire season&lt;/em>,
&lt;em>Climate change&lt;/em>,
&lt;em>COVID-19&lt;/em>, and
&lt;em>Vaccination&lt;/em>
on three social media platforms (Facebook, Twitter, and YouTube).&lt;/p>
&lt;h2 id="our-solution-mixing-digital-ethnography-with-advanced-machine-learning">Our Solution: Mixing Digital Ethnography with Advanced Machine Learning&lt;/h2>
&lt;p>&lt;img src="fig1.png" alt="The pipeline of machine learning accelerated qualitative research where the human-in-the-loop machine learning algorithms are employed for dataset augmentation.">&lt;/p>
&lt;p>We present a complete solution that bridges and facilitates qualitative and quantitative analysis to study problematic online speech.
The pipeline consists of three components which are detailed in this section.&lt;/p>
&lt;h3 id="deep-qualitative-study">Deep qualitative study&lt;/h3>
&lt;p>The first component is the qualitative study.
We build a platform based on an open-source tool, &lt;a href="https://wikiba.se/">&lt;em>Wikibase&lt;/em>&lt;/a>, where we conduct qualitative and quantitative analysis.
Through the quantitative study, we build an ontology of problematic online speech.
We label a large number of social media postings using their topics.
Simultaneously, we construct a vocabulary of 71 opinions that we also use to label postings.
Some example opinions include:&lt;/p>
&lt;ul>
&lt;li>Climate change crisis isn't real&lt;/li>
&lt;li>United Nations is corrupt&lt;/li>
&lt;li>Climate change is a UN hoax&lt;/li>
&lt;li>United Nations wants to be the global ruling government&lt;/li>
&lt;li>Vaccines cause Autism&lt;/li>
&lt;li>The World Health Organisation is corrupt&lt;/li>
&lt;/ul>
&lt;p>These opinions contain mistrust in the government and supra-national structures (e.g., UN, WHO) and typical misinformation about vaccines.&lt;/p>
&lt;h3 id="unlabeled-data-collection">Unlabeled data collection&lt;/h3>
&lt;p>Qualitative approaches analyze emerging content and construct the vocabulary simultaneously when labeling the data.
However, they lack representativeness as they tend to be overly concentrated on narrow areas of the narrative landscape.
For this reason, the second step in our methodology involves collecting data at scale.
We collect large-scale raw data using the uncovered vocabulary from the deep qualitative study.
We construct keywords (shown in the table) for each topic to crawl social media data from three platforms.
We obtain a total of &lt;strong>13,321,813&lt;/strong> postings — &lt;strong>11,437,009&lt;/strong> Facebook postings, &lt;strong>1,793,927&lt;/strong> tweets and &lt;strong>90,877&lt;/strong> YouTube comments.&lt;/p>
&lt;p>&lt;img src="table.png" alt="keywords">&lt;/p>
&lt;h3 id="dataset-augmentation">Dataset augmentation&lt;/h3>
&lt;p>The next step is to annotate all the postings in our dataset automatically.
We employ machine learning algorithms to augment the data labeling process with a human-in-the-loop setting.&lt;/p>
&lt;p>By adopting the state-of-the-art text classification algorithm, RoBERTa [7,8], we first train the classifiers to identify problematic speech on postings annotated by the qualitative researchers.
Next, we deploy three strategies to select unlabeled data.
The active learning [9] strategy selects the data for which the classifiers are most uncertain.
The top-confidence strategy selects data that classifiers are most certain about.
The third strategy — the random strategy — randomly samples from unlabeled data.
The qualitative researchers then label the sampled data, introduce the newly labeled data in the ontology, and repeat the procedure iteratively until the predictive performance converges.&lt;/p>
&lt;h2 id="humanintheloop-performance">Human-in-the-loop Performance&lt;/h2>
&lt;p>We discuss here the convergence of prediction performance over labeling iterations.&lt;/p>
&lt;p>The following plot depicts the prediction performance on the test set macro-averaged over topics (accuracy in the left panel and F1 score on the right panel) over iterations.
The solid lines show the performance indicators, together with the cross-validation generalization error.&lt;/p>
&lt;p>The cross-validation performance is stable across iterations.
This is expected as the classifiers learn from the same data on which the generalization is estimated — i.e., the classifiers are representative of the data they were trained on.
However, the difference between the test set performance and cross-validation performance is indicative of the representativity over the entire dataset.
The cross-validation accuracy is consistently lower than the test set accuracy because the test data is more imbalanced than labeled data.
The cross-validation F1 is more optimistic than the test set F1.
Finally, the difference between the two stabilizes for the later iterations, further suggesting the convergence.
&lt;img src="fig3.png" alt="Convergence of topic classifier performances over seven iterations.">&lt;/p>
&lt;h2 id="applying-the-qualitative-mapping">Applying the Qualitative Mapping&lt;/h2>
&lt;p>We employ the obtained augmented labeled set to analyze the dynamics of problematic opinions at scale.
We machine-label the opinions in a large set of postings spanning over a long time, allowing us to apply the qualitative-defined coding schema to a significantly larger sample of postings.
This reduces the unavoidable selection bias of the deep qualitative study.
It also offers a critical tool for analyzing co-occurring opinions, which helps identify central opinions.
It is common for postings to express multiple opinions.&lt;/p>
&lt;p>We explore central opinions by building an opinion co-occurrence network in the online conversation of the topic &lt;strong>2019-20 Australian bushfire season&lt;/strong>.
In the network, nodes are the opinions captured during the bushfire conversation, while edges are present when both opinions are detected in the same postings.
The node degree of a given opinion node represents the number of opinions that co-occurred with it.
The edges are weighted by the number of postings in which their connected node opinions co-occurred.&lt;/p>
&lt;p>The following plot presents each edge's daily proportions of weights among all edges between September 2019 and January 2020.
We show six edges (i.e., opinion pairs) to represent three types of temporal dynamics:&lt;/p>
&lt;ul>
&lt;li>A continuous and relatively strong association between prevalent opinions — &amp;ldquo;Climate change crisis isn't real&amp;rdquo; and &amp;ldquo;Climate change is a UN hoax,&amp;rdquo; which not notably is a conspiracy theory.&lt;/li>
&lt;li>Associations with declining relative frequencies — &amp;ldquo;Greta Thunberg should not have a platform or influence as a climate&amp;hellip;&amp;rdquo; and &amp;ldquo;Women and girls don't deserve a voice in the public sphere&amp;rdquo;.&lt;/li>
&lt;li>Rising associations such as &amp;ldquo;bushfires and climate change not related&amp;rdquo; and &amp;ldquo;bushfires were caused by random arsonists&amp;rdquo;; and also the conspiracy theory associations between &amp;ldquo;United Nations is corrupt&amp;rdquo; and &amp;ldquo;United Nations wants to be the global ruling government&amp;rdquo;.
&lt;img src="fig2.png" alt="Daily proportions of edge weights of six selected co-occurred opinions pairs.">&lt;/li>
&lt;/ul>
&lt;p>In the following plot, we map the co-occurrence network from posts published over 14 days in late September 2019, i.e., the period when the betweenness for conspiracy opinions is at peak.
This figure explains the ambivalent network role that conspiracy opinions can play: we first note a conspiracy opinion with relatively high degree and frequency — &amp;ldquo;Climate change is a UN hoax&amp;rdquo; —, while we also observe the presence of low degree but high betweenness conspiracy opinions at the periphery of the network — &amp;ldquo;Bushfires linked to secret elites&amp;rsquo; secret technology (chemtrails, HAARP, HSRN, geoengineering)&amp;quot;, &amp;ldquo;bushfires deliberately lit to promote a climate change agenda&amp;rdquo; and &amp;ldquo;Australia should not be a member of the United Nations&amp;rdquo;.&lt;/p>
&lt;p>&lt;img src="featured.png" alt="A visualization of the co-occurrence network in late September 2020 — node sizes and colors indicate the degrees and betweenness values">&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] Jack, C. 2017. Lexicon of lies: Terms for problematic information. Data &amp;amp; Society.&lt;br>
[2] Posetti, J.; and Bontcheva, K. 2020. Disinfodemic: deciphering COVID-19 disinformation. Policy brief 1.&lt;br>
[3] Ram, R.; Kong, Q.; and Rizoiu, M.-A. 2021. Birdspotter: A Tool for Analyzing and Labeling Twitter Users. In WSDM. ACM.&lt;br>
[4] Smith, N.; and Graham, T. 2019. Mapping the anti-vaccination movement on Facebook. Information, Communication &amp;amp; Society.&lt;br>
[5] Molina, M. D.; Sundar, S. S.; Le, T.; and Lee, D. 2019. “Fake news” is not simply false information: a concept explication and taxonomy of online content. American behavioral scientist&lt;br>
[6] Glaeser, E. L.; and Sunstein, C. R. 2009. Extremism and social learning. Journal of Legal Analysis.&lt;br>
[7] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all you need. In NeurIPS.&lt;br>
[8] Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Roberta: A robustly optimized BERT pretraining approach. arXiv.&lt;br>
[9] Settles, B. 2012. Active learning. Synthesis lectures on artificial intelligence and machine learning.&lt;/p></description></item><item><title>Slipping to the Extreme: A Mixed-Method to Explain How Extreme Opinions Infiltrate Online Discussions</title><link>https://www.behavioral-ds.science/theme2_content/icwsm2022/</link><pubDate>Mon, 13 Dec 2021 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.science/theme2_content/icwsm2022/</guid><description>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/HwFq3ywanp4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;p>In our &lt;a href="https://arxiv.org/pdf/2109.00302.pdf">recent paper&lt;/a> accepted at &lt;a href="https://www.icwsm.org/2022/index.html/">ICWSM 2022&lt;/a>, we propose a complete solution to accelerate the qualitative analysis of problematic online speech — with a specific focus on opinions emerging from online communities — by leveraging machine learning algorithms.&lt;/p>
&lt;p>&lt;strong>Paper citation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code>Quyu Kong, Emily Booth, Francesco Bailo, Amelia Johns, and Marian-Andrei
Rizoiu. Slipping to the Extreme: A Mixed-Method to Explain How Extreme
Opinions Infiltrate Online Discussions. In: Proceedings of the 16TH AAAI
International Conference on Web and Social Media, 2022.
&lt;/code>&lt;/pre>
&lt;p>(see full paper here: &lt;a href="https://arxiv.org/pdf/2109.00302.pdf">https://arxiv.org/pdf/2109.00302.pdf&lt;/a>)&lt;/p>
&lt;h3 id="problematic-speech-a-modern-plague">Problematic Speech: A Modern Plague&lt;/h3>
&lt;p>Problematic speech is online interactions, speech, and artifacts that are inaccurate, misleading, inappropriately attributed, or altogether fabricated [1].
In 2020, the COVID-19 pandemic alerted the world to complex issues that arise from social media platforms circulating user-generated misinformation, hate speech, and conspiracy theories [2].
There are several primary types of quantitative methods for addressing problematic information, including large-scale monitoring of social media datasets [3], understanding platforms, users, and networks contributing to the &amp;ldquo;infodemic&amp;rdquo; [4], and predicting future pathways of information spreading [5].
These studies provide valuable insights into understanding how problematic information spreads and detecting which sources are reshared frequently and by which accounts.
However, these approaches often have less to say about why certain opinions and views gain traction with vulnerable groups and online communities.&lt;/p>
&lt;p>Qualitative research methods are well placed to address this gap.
They provide rich, contextual insights into online communities&amp;rsquo; social beliefs, values, and practices, which shape how information is shared and how opinions are formed [6].
Nevertheless, a common criticism of qualitative research is that the in-depth knowledge comes at the expense of generating insights of limited representativeness and weak robustness of the findings.
Therefore, there is a gap between the depth of insight gained from ethnographic and qualitative approaches and the breadth of knowledge gained from computational methods from data science.&lt;/p>
&lt;p>Our work fills this gap by proposing a mixed-method approach that combines qualitative insights, large-scale data collection, and human-in-the-loop machine learning approaches.
We apply our method to map in-depth and in-breadth the problematic information around four topics:
&lt;em>2019-20 Australian bushfire season&lt;/em>,
&lt;em>Climate change&lt;/em>,
&lt;em>COVID-19&lt;/em>, and
&lt;em>Vaccination&lt;/em>
on three social media platforms (Facebook, Twitter, and YouTube).&lt;/p>
&lt;h2 id="our-solution-mixing-digital-ethnography-with-advanced-machine-learning">Our Solution: Mixing Digital Ethnography with Advanced Machine Learning&lt;/h2>
&lt;p>&lt;img src="fig1.png" alt="The pipeline of machine learning accelerated qualitative research where the human-in-the-loop machine learning algorithms are employed for dataset augmentation.">&lt;/p>
&lt;p>We present a complete solution that bridges and facilitates qualitative and quantitative analysis to study problematic online speech.
The pipeline consists of three components which are detailed in this section.&lt;/p>
&lt;h3 id="deep-qualitative-study">Deep qualitative study&lt;/h3>
&lt;p>The first component is the qualitative study.
We build a platform based on an open-source tool, &lt;a href="https://wikiba.se/">&lt;em>Wikibase&lt;/em>&lt;/a>, where we conduct qualitative and quantitative analysis.
Through the quantitative study, we build an ontology of problematic online speech.
We label a large number of social media postings using their topics.
Simultaneously, we construct a vocabulary of 71 opinions that we also use to label postings.
Some example opinions include:&lt;/p>
&lt;ul>
&lt;li>Climate change crisis isn't real&lt;/li>
&lt;li>United Nations is corrupt&lt;/li>
&lt;li>Climate change is a UN hoax&lt;/li>
&lt;li>United Nations wants to be the global ruling government&lt;/li>
&lt;li>Vaccines cause Autism&lt;/li>
&lt;li>The World Health Organisation is corrupt&lt;/li>
&lt;/ul>
&lt;p>These opinions contain mistrust in the government and supra-national structures (e.g., UN, WHO) and typical misinformation about vaccines.&lt;/p>
&lt;h3 id="unlabeled-data-collection">Unlabeled data collection&lt;/h3>
&lt;p>Qualitative approaches analyze emerging content and construct the vocabulary simultaneously when labeling the data.
However, they lack representativeness as they tend to be overly concentrated on narrow areas of the narrative landscape.
For this reason, the second step in our methodology involves collecting data at scale.
We collect large-scale raw data using the uncovered vocabulary from the deep qualitative study.
We construct keywords (shown in the table) for each topic to crawl social media data from three platforms.
We obtain a total of &lt;strong>13,321,813&lt;/strong> postings — &lt;strong>11,437,009&lt;/strong> Facebook postings, &lt;strong>1,793,927&lt;/strong> tweets and &lt;strong>90,877&lt;/strong> YouTube comments.&lt;/p>
&lt;p>&lt;img src="table.png" alt="keywords">&lt;/p>
&lt;h3 id="dataset-augmentation">Dataset augmentation&lt;/h3>
&lt;p>The next step is to annotate all the postings in our dataset automatically.
We employ machine learning algorithms to augment the data labeling process with a human-in-the-loop setting.&lt;/p>
&lt;p>By adopting the state-of-the-art text classification algorithm, RoBERTa [7,8], we first train the classifiers to identify problematic speech on postings annotated by the qualitative researchers.
Next, we deploy three strategies to select unlabeled data.
The active learning [9] strategy selects the data for which the classifiers are most uncertain.
The top-confidence strategy selects data that classifiers are most certain about.
The third strategy — the random strategy — randomly samples from unlabeled data.
The qualitative researchers then label the sampled data, introduce the newly labeled data in the ontology, and repeat the procedure iteratively until the predictive performance converges.&lt;/p>
&lt;h2 id="humanintheloop-performance">Human-in-the-loop Performance&lt;/h2>
&lt;p>We discuss here the convergence of prediction performance over labeling iterations.&lt;/p>
&lt;p>The following plot depicts the prediction performance on the test set macro-averaged over topics (accuracy in the left panel and F1 score on the right panel) over iterations.
The solid lines show the performance indicators, together with the cross-validation generalization error.&lt;/p>
&lt;p>The cross-validation performance is stable across iterations.
This is expected as the classifiers learn from the same data on which the generalization is estimated — i.e., the classifiers are representative of the data they were trained on.
However, the difference between the test set performance and cross-validation performance is indicative of the representativity over the entire dataset.
The cross-validation accuracy is consistently lower than the test set accuracy because the test data is more imbalanced than labeled data.
The cross-validation F1 is more optimistic than the test set F1.
Finally, the difference between the two stabilizes for the later iterations, further suggesting the convergence.
&lt;img src="fig3.png" alt="Convergence of topic classifier performances over seven iterations.">&lt;/p>
&lt;h2 id="applying-the-qualitative-mapping">Applying the Qualitative Mapping&lt;/h2>
&lt;p>We employ the obtained augmented labeled set to analyze the dynamics of problematic opinions at scale.
We machine-label the opinions in a large set of postings spanning over a long time, allowing us to apply the qualitative-defined coding schema to a significantly larger sample of postings.
This reduces the unavoidable selection bias of the deep qualitative study.
It also offers a critical tool for analyzing co-occurring opinions, which helps identify central opinions.
It is common for postings to express multiple opinions.&lt;/p>
&lt;p>We explore central opinions by building an opinion co-occurrence network in the online conversation of the topic &lt;strong>2019-20 Australian bushfire season&lt;/strong>.
In the network, nodes are the opinions captured during the bushfire conversation, while edges are present when both opinions are detected in the same postings.
The node degree of a given opinion node represents the number of opinions that co-occurred with it.
The edges are weighted by the number of postings in which their connected node opinions co-occurred.&lt;/p>
&lt;p>The following plot presents each edge's daily proportions of weights among all edges between September 2019 and January 2020.
We show six edges (i.e., opinion pairs) to represent three types of temporal dynamics:&lt;/p>
&lt;ul>
&lt;li>A continuous and relatively strong association between prevalent opinions — &amp;ldquo;Climate change crisis isn't real&amp;rdquo; and &amp;ldquo;Climate change is a UN hoax,&amp;rdquo; which not notably is a conspiracy theory.&lt;/li>
&lt;li>Associations with declining relative frequencies — &amp;ldquo;Greta Thunberg should not have a platform or influence as a climate&amp;hellip;&amp;rdquo; and &amp;ldquo;Women and girls don't deserve a voice in the public sphere&amp;rdquo;.&lt;/li>
&lt;li>Rising associations such as &amp;ldquo;bushfires and climate change not related&amp;rdquo; and &amp;ldquo;bushfires were caused by random arsonists&amp;rdquo;; and also the conspiracy theory associations between &amp;ldquo;United Nations is corrupt&amp;rdquo; and &amp;ldquo;United Nations wants to be the global ruling government&amp;rdquo;.
&lt;img src="fig2.png" alt="Daily proportions of edge weights of six selected co-occurred opinions pairs.">&lt;/li>
&lt;/ul>
&lt;p>In the following plot, we map the co-occurrence network from posts published over 14 days in late September 2019, i.e., the period when the betweenness for conspiracy opinions is at peak.
This figure explains the ambivalent network role that conspiracy opinions can play: we first note a conspiracy opinion with relatively high degree and frequency — &amp;ldquo;Climate change is a UN hoax&amp;rdquo; —, while we also observe the presence of low degree but high betweenness conspiracy opinions at the periphery of the network — &amp;ldquo;Bushfires linked to secret elites&amp;rsquo; secret technology (chemtrails, HAARP, HSRN, geoengineering)&amp;quot;, &amp;ldquo;bushfires deliberately lit to promote a climate change agenda&amp;rdquo; and &amp;ldquo;Australia should not be a member of the United Nations&amp;rdquo;.&lt;/p>
&lt;p>&lt;img src="featured.png" alt="A visualization of the co-occurrence network in late September 2020 — node sizes and colors indicate the degrees and betweenness values">&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] Jack, C. 2017. Lexicon of lies: Terms for problematic information. Data &amp;amp; Society.&lt;br>
[2] Posetti, J.; and Bontcheva, K. 2020. Disinfodemic: deciphering COVID-19 disinformation. Policy brief 1.&lt;br>
[3] Ram, R.; Kong, Q.; and Rizoiu, M.-A. 2021. Birdspotter: A Tool for Analyzing and Labeling Twitter Users. In WSDM. ACM.&lt;br>
[4] Smith, N.; and Graham, T. 2019. Mapping the anti-vaccination movement on Facebook. Information, Communication &amp;amp; Society.&lt;br>
[5] Molina, M. D.; Sundar, S. S.; Le, T.; and Lee, D. 2019. “Fake news” is not simply false information: a concept explication and taxonomy of online content. American behavioral scientist&lt;br>
[6] Glaeser, E. L.; and Sunstein, C. R. 2009. Extremism and social learning. Journal of Legal Analysis.&lt;br>
[7] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all you need. In NeurIPS.&lt;br>
[8] Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Roberta: A robustly optimized BERT pretraining approach. arXiv.&lt;br>
[9] Settles, B. 2012. Active learning. Synthesis lectures on artificial intelligence and machine learning.&lt;/p></description></item></channel></rss>