<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Research blogs from the Behavioral Data Science lab | Behavioral Data Science</title><link>https://www.behavioral-ds.science/blogpost/</link><atom:link href="https://www.behavioral-ds.science/blogpost/index.xml" rel="self" type="application/rss+xml"/><description>Research blogs from the Behavioral Data Science lab</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2025</copyright><lastBuildDate>Fri, 20 Jun 2025 00:00:00 +0000</lastBuildDate><image><url>https://www.behavioral-ds.science/img/logo.png</url><title>Research blogs from the Behavioral Data Science lab</title><link>https://www.behavioral-ds.science/blogpost/</link></image><item><title>Before It's Too Late: A State Space Model for the Early Prediction of Misinformation and Disinformation Engagement</title><link>https://www.behavioral-ds.science/blogpost/ic-mamba/</link><pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.science/blogpost/ic-mamba/</guid><description>&lt;h2 id="introduction-from-4chan-to-capitol--when-online-conspiracies-turn-deadly">Introduction: From 4chan to Capitol - When Online Conspiracies Turn Deadly&lt;/h2>
&lt;p>On October 28, 2017, an anonymous 4chan user made a cryptic post claiming Hillary Clinton would be arrested within days. Four years later, on January 6, 2021, devotees of the QAnon conspiracy theory - which originated from that very post - stormed the U.S. Capitol in an act that left five people dead and over 140 police officers injured.&lt;/p>
&lt;p>This tragic event underscores a critical question:&lt;/p>
&lt;p>&lt;strong>What if we could have seen this coming?&lt;/strong>&lt;/p>
&lt;p>Today, we're introducing IC-Mamba, a state-of-the-art model that can forecast social media engagement patterns and detect viral misinformation before it spirals out of control. By modeling the complex temporal dynamics of how content spreads online, IC-Mamba achieves unprecedented accuracy in predicting which posts will go viral - often within just 15-30 minutes of publication.&lt;/p>
&lt;p>&lt;strong>Paper citation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code>Tian, L., Booth, E., Bailo, F., Droogan, J. and Rizoiu, M.A., 2025, April.
Before It's Too Late: A State Space Model for the Early Prediction of Misinformation and Disinformation Engagement.
In Proceedings of the ACM on Web Conference 2025 (pp. 5244-5254).
&lt;/code>&lt;/pre>
&lt;p>(&lt;em>see full paper here: &lt;a href="https://dl.acm.org/doi/abs/10.1145/3696410.3714527">&lt;a href="https://dl.acm.org/doi/abs/10.1145/3696410.3714527">https://dl.acm.org/doi/abs/10.1145/3696410.3714527&lt;/a>&lt;/a>&lt;/em>)&lt;/p>
&lt;h2 id="the-challenge-understanding-intervalcensored-social-media-data">The Challenge: Understanding Interval-Censored Social Media Data&lt;/h2>
&lt;p>Social media platforms don't provide continuous streams of engagement data. Instead, they offer snapshots at irregular intervals - what we call &amp;ldquo;interval-censored&amp;rdquo; data. As illustrated in Figure 1, while users interact with content continuously, platforms only report cumulative counts at discrete observation points.&lt;/p>
&lt;img src="./featured.png" width="100%">
&lt;p style="text-align: center;">&lt;em>Figure 1. Illustration of interval-censored social media engagement data. &lt;/em>&lt;/p>
&lt;p>This creates unique challenges:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Irregular sampling&lt;/strong>: Observation intervals vary from minutes to hours&lt;/li>
&lt;li>&lt;strong>Cumulative counts&lt;/strong>: We only see totals, not individual actions&lt;/li>
&lt;li>&lt;strong>Multi-scale dynamics&lt;/strong>: Engagement patterns evolve from rapid initial spread to long-term influence&lt;/li>
&lt;/ul>
&lt;h2 id="icmamba-a-novel-architecture-for-early-detection">IC-Mamba: A Novel Architecture for Early Detection&lt;/h2>
&lt;p>IC-Mamba combines three key components to tackle these challenges:&lt;/p>
&lt;h4 id="1-timeaware-positional-embeddings">1. Time-Aware Positional Embeddings&lt;/h4>
&lt;p>We developed a dual encoding strategy that captures both relative and absolute temporal relationships:&lt;/p>
&lt;p>&lt;strong>Relative Temporal Encoding (RTE)&lt;/strong>:&lt;/p>
&lt;p>&lt;code>RTE(t, t_ref) = sin((t - t_ref) / σ)&lt;/code>&lt;/p>
&lt;p>&lt;strong>Absolute Temporal Encoding (ATE)&lt;/strong>:&lt;/p>
&lt;p>&lt;code>ATE(t) = [sin(t/10000^(2i/d)), cos(t/10000^(2i/d))]&lt;/code>&lt;/p>
&lt;p>These embeddings are then modulated by observed engagement levels, allowing the model to learn characteristic temporal patterns associated with different levels of social impact.&lt;/p>
&lt;h4 id="2-intervalcensored-state-space-modeling">2. Interval-Censored State Space Modeling&lt;/h4>
&lt;p>We extend the Mamba architecture to handle varying-length censored intervals through:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Interval-aware vectors&lt;/strong> that encode time gaps and engagement counts&lt;/li>
&lt;li>&lt;strong>Time-dependent state transitions&lt;/strong> using matrix exponentials&lt;/li>
&lt;li>&lt;strong>Selective state processing&lt;/strong> through parallel pathways&lt;/li>
&lt;/ul>
&lt;img src="./architecture.png" width="100%">
&lt;p style="text-align: center;">&lt;em>Figure 2. Overview of the IC-Mamba Architecture. &lt;/em>&lt;/p>
&lt;h4 id="3-twotier-architecture">3. Two-Tier Architecture&lt;/h4>
&lt;p>As shown in Figure 3, IC-Mamba uses a hierarchical approach:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Tier 1&lt;/strong>: Models individual post engagement dynamics&lt;/li>
&lt;li>&lt;strong>Tier 2&lt;/strong>: Captures temporal interactions between posts sharing the same opinion&lt;/li>
&lt;/ul>
&lt;img src="./2-tier.png" width="80%">
&lt;p style="text-align: center;">&lt;em>Figure 3. Overview of the Two-Tier IC-Mamba Architecture. &lt;/em>&lt;/p>
&lt;h2 id="how-icmamba-works">How IC-Mamba Works&lt;/h2>
&lt;h4 id="postlevel-engagement-prediction">Post-Level Engagement Prediction&lt;/h4>
&lt;p>IC-Mamba's post-level prediction capabilities stem from its sophisticated handling of temporal dynamics and multi-modal information integration:&lt;/p>
&lt;p>&lt;strong>1. Unified Sequence Representation&lt;/strong>&lt;br>
The model creates a comprehensive embedding for each post by combining:&lt;/p>
&lt;p>&lt;code>SE(p) = Encoder([CLS] ⊕ [text] ⊕ [SEP] ⊕ [user] ⊕ [SEP] ⊕ [timeline] ⊕ [SEP] ⊕ [engagement])&lt;/code>&lt;/p>
&lt;p>This allows IC-Mamba to jointly learn from textual content, user metadata, and temporal engagement patterns in a single unified framework.&lt;/p>
&lt;p>&lt;strong>2. Interval-Aware State Processing&lt;/strong>&lt;br>
For each observation time, the model constructs an interval-aware vector that captures:&lt;/p>
&lt;ul>
&lt;li>Time since last observation (Δt⁻)&lt;/li>
&lt;li>Current engagement levels (log(1 + e))&lt;/li>
&lt;li>Forward interval length (Δt⁺)&lt;/li>
&lt;li>Predicted next engagement&lt;/li>
&lt;/ul>
&lt;p>This rich representation enables the model to understand not just what happened, but when and how quickly it occurred.&lt;/p>
&lt;p>&lt;strong>3. Adaptive State Transitions&lt;/strong>&lt;br>
The state space model adapts to varying observation intervals through:&lt;/p>
&lt;p>&lt;code>A_t(Δt) = exp(Δt · Ã_t)&lt;/code>&lt;/p>
&lt;p>This matrix exponential formulation allows smooth interpolation across irregular time gaps, maintaining temporal coherence even with sparse observations.&lt;/p>
&lt;h4 id="opinionlevel-classification">Opinion-Level Classification&lt;/h4>
&lt;p>IC-Mamba's exceptional performance in identifying misinformation narratives comes from its two-tier architecture and sophisticated temporal modeling:&lt;/p>
&lt;p>&lt;strong>1. Learning Opinion Signatures&lt;/strong>&lt;br>
The first tier (IC-Mamba₁) processes individual posts to learn characteristic engagement patterns for different opinion types. Posts expressing similar opinions often exhibit similar temporal dynamics - for instance, conspiracy theories might show rapid initial spread followed by sustained engagement from dedicated communities.&lt;/p>
&lt;p>&lt;strong>2. Capturing Narrative Evolution&lt;/strong>&lt;br>
The second tier (IC-Mamba₂) models how opinions spread across multiple posts:&lt;/p>
&lt;p>&lt;code>z_o = IC-Mamba₂((h_i, δt_i))&lt;/code>&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>&lt;code>h_i&lt;/code>: Hidden representations from individual posts&lt;/li>
&lt;li>&lt;code>δt_i&lt;/code>: Time intervals between posts expressing the same opinion&lt;/li>
&lt;/ul>
&lt;p>This allows the model to identify coordinated campaigns where multiple accounts push similar narratives with strategic timing.&lt;/p>
&lt;p>&lt;strong>3. Multi-Scale Temporal Features&lt;/strong>&lt;br>
The model learns to distinguish between:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Organic discussions&lt;/strong>: Natural ebb and flow of engagement, constrained by human attention limits&lt;/li>
&lt;li>&lt;strong>Coordinated campaigns&lt;/strong>: Artificial amplification patterns, strategic timing, and sustained engagement beyond natural thresholds&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>4. Context-Aware Classification&lt;/strong>&lt;br>
Unlike text-only approaches (like BERT), IC-Mamba incorporates:&lt;/p>
&lt;ul>
&lt;li>Engagement velocity (how quickly reactions accumulate)&lt;/li>
&lt;li>User network effects (who's sharing and when)&lt;/li>
&lt;li>Temporal coordination (synchronized posting patterns)&lt;/li>
&lt;li>Cross-platform dynamics (when available)&lt;/li>
&lt;/ul>
&lt;p>This multi-faceted approach explains why IC-Mamba achieves F1 scores of 0.69-0.75 on organic content and maintains 0.51 performance even on sophisticated disinformation campaigns where text-only models fail dramatically.&lt;/p>
&lt;h2 id="understanding-engagement-patterns-misinformation-vs-disinformation">Understanding Engagement Patterns: Misinformation vs. Disinformation&lt;/h2>
&lt;p>Figure 4 reveals differences between misinformation discussions and coordinated campaigns:&lt;/p>
&lt;img src="./figure4.png" width="120%">
&lt;p style="text-align: center;">&lt;em>Figure 4: Engagement distribution patterns. &lt;/em>&lt;/p>
&lt;!-- ![Figure 4: Engagement distribution patterns](./figure4.png) -->
&lt;p>&lt;strong>Misinformation content (Climate Change)&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Follows human-scale constraints (~150 stable connections - Dunbar's number)&lt;/li>
&lt;li>Plateaus around 10³ engagements&lt;/li>
&lt;li>Uniform decay across engagement types&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Coordinated campaigns (DiN)&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Transcends natural limits, reaching 10⁶ engagements&lt;/li>
&lt;li>Marked stratification, especially in likes&lt;/li>
&lt;li>Self-similar temporal behavior across all observation windows&lt;/li>
&lt;/ul>
&lt;h2 id="realworld-impact-tracking-climate-change-is-a-un-hoax">Real-World Impact: Tracking &amp;ldquo;Climate Change is a UN Hoax&amp;rdquo;&lt;/h2>
&lt;img src="./figure6.png" width="120%">
&lt;p style="text-align: center;">&lt;em>Figure 5: Early prediction performance on "Climate Change is a UN Hoax". &lt;/em>&lt;/p>
&lt;!-- ![Figure 6: Early prediction performance on "Climate Change is a UN Hoax".](./figure6.png) -->
&lt;p>Figure 5a and 5b showcase IC-Mamba's ability to forecast opinion-level engagement over 28 days:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>7-day observation window&lt;/strong>: Predictions remain accurate with wider confidence intervals&lt;/li>
&lt;li>&lt;strong>10-day observation window&lt;/strong>: Tighter confidence bands and improved long-term accuracy&lt;/li>
&lt;li>&lt;strong>Multi-engagement tracking&lt;/strong>: Simultaneously forecasts likes, comments, shares, and emoji reactions&lt;/li>
&lt;/ul>
&lt;h2 id="technical-deep-dive-the-mathematics-behind-icmamba">Technical Deep Dive: The Mathematics Behind IC-Mamba&lt;/h2>
&lt;h4 id="state-space-formulation">State Space Formulation&lt;/h4>
&lt;p>The model uses time-dependent state transitions:&lt;/p>
&lt;p>A_t(Δt) = exp(Δt · Ã_t) ∈ ℝ^(D_h × D_h)
h_t = A_t(Δt)h_{t-1} + B_t x_t
y_t = C_t^T h_t&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>&lt;code>h_t&lt;/code>: Hidden state at time t&lt;/li>
&lt;li>&lt;code>Δt&lt;/code>: Interval between observations&lt;/li>
&lt;li>&lt;code>exp(Δt · Ã_t)&lt;/code>: Matrix exponential for smooth interpolation&lt;/li>
&lt;/ul>
&lt;h4 id="loss-functions">Loss Functions&lt;/h4>
&lt;p>We combine two objectives for robust training:&lt;/p>
&lt;p>&lt;strong>Engagement Prediction Loss&lt;/strong>:&lt;/p>
&lt;p>&lt;code>L_pred = (1/|P|) Σ_p Σ_j ||ê_{j+1} - e_{j+1}||²&lt;/code>&lt;/p>
&lt;p>&lt;strong>Temporal Coherence Loss&lt;/strong>:&lt;/p>
&lt;p>&lt;code>L_temp = (1/|P|) Σ_p Σ_j ||h_{j+1} - exp(Δt_j^+ · Ã_t)h_j||²&lt;/code>&lt;/p>
&lt;h2 id="conclusion-a-tool-for-digital-defense">Conclusion: A Tool for Digital Defense&lt;/h2>
&lt;p>IC-Mamba shows our ability to detect and predict the spread of misinformation and disinformation online. By providing early warnings &amp;ndash; often within 15-30 minutes of posting &amp;ndash; it offers platforms and researchers crucial time to:&lt;/p>
&lt;ul>
&lt;li>Identify potentially harmful content before mass exposure&lt;/li>
&lt;li>Track coordinated information operations in real-time&lt;/li>
&lt;li>Design and implement targeted countermeasures&lt;/li>
&lt;li>Protect democratic discourse from manipulation&lt;/li>
&lt;/ul>
&lt;p>As misinformation continues to threaten social cohesion and democratic institutions, tools like IC-Mamba become essential components of our digital defense infrastructure. The model's ability to distinguish between organic discussions and coordinated campaigns, combined with its early detection capabilities, makes it a powerful ally in the fight against online manipulation.&lt;/p>
&lt;p>The code is publicly available at &lt;a href="https://github.com/ltian678/ic-mamba">&lt;a href="https://github.com/ltian678/ic-mamba">https://github.com/ltian678/ic-mamba&lt;/a>&lt;/a>, and an interactive dashboard demonstrating the results can be accessed at &lt;a href="https://ic-mamba.behavioral-ds.science/">https://ic-mamba.behavioral-ds.science/&lt;/a>.&lt;/p>
&lt;hr></description></item><item><title>Some Reddit users just love to disagree, new AI-powered troll-spotting algorithm finds</title><link>https://www.behavioral-ds.science/blogpost/irl_homophily/</link><pubDate>Fri, 09 May 2025 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.science/blogpost/irl_homophily/</guid><description>&lt;p>This is a repost of our article from &lt;a href="https://theconversation.com/some-reddit-users-just-love-to-disagree-new-ai-powered-troll-spotting-algorithm-finds-255879">The Conversation&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Paper citation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code>Lanqin Yuan, Philipp J. Schneider, and Marian-Andrei Rizoiu. 2025. Behavioral Homophily in Social Media via Inverse Reinforcement Learning: A Reddit Case Study. In Proceedings of the ACM on Web Conference 2025 (WWW '25). Association for Computing Machinery, New York, NY, USA, 576–589. https://doi.org/10.1145/3696410.3714618
&lt;/code>&lt;/pre>
&lt;p>(&lt;em>see full paper here: &lt;a href="https://dl.acm.org/doi/pdf/10.1145/3696410.3714618">&lt;a href="https://dl.acm.org/doi/pdf/10.1145/3696410.3714618">https://dl.acm.org/doi/pdf/10.1145/3696410.3714618&lt;/a>&lt;/a>&lt;/em>)&lt;/p>
&lt;p>In today’s fractured online landscape, it is harder than ever to identify harmful actors such as trolls and misinformation spreaders.&lt;/p>
&lt;p>Often, efforts to spot malicious accounts focus on analysing what they say. However, our latest research suggests we should be paying more attention to what they do – and how they do it.&lt;/p>
&lt;p>We have developed a way to identify potentially harmful online actors based solely on their behavioural patterns – the way they interact with others – rather than the content they share. We presented our results at the recent ACM Web Conference, and were awarded Best Paper.
Beyond looking at what people say&lt;/p>
&lt;p>Traditional approaches to spotting problematic online behaviour typically rely on two methods. One is to examine content (what people are saying). The other is to analyse network connections (who follows whom).&lt;/p>
&lt;p>These methods have limitations.&lt;/p>
&lt;p>Users can circumvent content analysis. They may code their language carefully, or share misleading information without using obvious trigger words.&lt;/p>
&lt;h2 id="beyond-looking-at-what-people-say">Beyond looking at what people say&lt;/h2>
&lt;p>Traditional approaches to spotting problematic online behaviour typically rely on two methods. One is to examine content (what people are saying). The other is to analyse network connections (who follows whom).&lt;/p>
&lt;p>These methods have limitations.&lt;/p>
&lt;p>Users can circumvent content analysis. They may code their language carefully, or share misleading information without using obvious trigger words.&lt;/p>
&lt;p>Network analysis falls short on platforms such as Reddit. Here, connections between users aren’t explicit. Communities are organised around topics rather than social relationships.&lt;/p>
&lt;p>We wanted to find a way to identify harmful actors that couldn’t be easily gamed. We realised we could, focusing on behaviour – how people interact, rather than what they say.&lt;/p>
&lt;h2 id="teaching-ai-to-understand-human-behaviour-online">Teaching AI to understand human behaviour online&lt;/h2>
&lt;p>Our approach uses a technique called inverse reinforcement learning. This is a method typically used to understand human decision-making in fields such as autonomous driving or game theory.&lt;/p>
&lt;p>We adapted this technology to analyse how users behave on social media platforms.&lt;/p>
&lt;p>The system works by observing a user’s actions, such as creating new threads, posting comments and replying to others. From those actions it infers the underlying strategy or “policy” that drives their behaviour.&lt;/p>
&lt;p>In our Reddit case study, we analysed 5.9 million interactions over six years. We identified five distinct behavioural personas, including one particularly notable group – “disagreers”.&lt;/p>
&lt;h2 id="meet-the-disagreers">Meet the ‘disagreers’&lt;/h2>
&lt;p>Perhaps our most striking result was finding an entire class of Reddit users whose primary purpose seems to be to disagree with others. These users specifically seek out opportunities to post contradictory comments, especially in response to disagreement, and then move on without waiting for replies.&lt;/p>
&lt;p>The “disagreers” were most common in politically-focused subreddits (forums focused on particular topics) such as r/news, r/worldnews, and r/politics. Interestingly, they were much less common in the now-banned pro-Trump forum r/The_Donald despite its political focus.&lt;/p>
&lt;p>This pattern reveals how behavioural analysis can uncover dynamics that content analysis might miss. In r/The_Donald, users tended to agree with each other while directing hostility toward outside targets. This dynamic may explain why traditional content moderation has struggled to address problems in such communities.&lt;/p>
&lt;h2 id="soccer-fans-and-gamers">Soccer fans and gamers&lt;/h2>
&lt;p>Our research also revealed unexpected connections. Users discussing completely different topics sometimes displayed remarkably similar behavioural patterns.&lt;/p>
&lt;p>We found striking similarities between users discussing soccer (on r/soccer) and e-sports (on r/leagueoflegends).&lt;/p>
&lt;p>This similarity emerges from the fundamental nature of both communities. Soccer and e-sports fans engage in parallel ways: they passionately support specific teams, follow matches with intense interest, participate in heated discussions about strategies and player performances, celebrate victories, and dissect defeats.&lt;/p>
&lt;p>Both communities foster strong tribal identities. Users defend their favoured teams while critiquing rivals.&lt;/p>
&lt;p>Whether debating Premier League tactics or League of Legends champions, the underlying interaction patterns – the timing, sequence and emotional tone of responses – remain consistent across these topically distinct communities.&lt;/p>
&lt;p>This challenges conventional wisdom about online polarisation. While echo chambers are often blamed for increasing division, our research suggests behavioural patterns can transcend topical boundaries. Users may be divided more by how they interact than what they discuss.&lt;/p>
&lt;h2 id="beyond-troll-detection">Beyond troll detection&lt;/h2>
&lt;p>The implications of this research extend well beyond academic interest. Platform moderators could use behavioural patterns to identify potentially problematic users before they’ve posted large volumes of harmful content.&lt;/p>
&lt;p>Unlike content moderation, behavioural analysis does not depend on understanding language. It is hard to evade, since changing one’s behavioural patterns requires more effort than adjusting language.&lt;/p>
&lt;p>The approach could also help design more effective strategies to counter misinformation. Rather than focusing solely on the content, we can design systems that encourage more constructive engagement patterns.&lt;/p>
&lt;p>For social media users, this research offers a reminder that how we engage online – not just what we say – shapes our digital identity and influences others.&lt;/p>
&lt;p>As online spaces continue to grapple with manipulation, harassment and polarisation, approaches that consider behavioural patterns alongside content analysis may offer more effective solutions for fostering healthier online communities.&lt;/p></description></item><item><title>Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions</title><link>https://www.behavioral-ds.science/blogpost/omm/</link><pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.science/blogpost/omm/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In our digital age, the surge of online extremism brings forth urgent concerns such as hate speech normalization, user radicalization, and societal division. Counteracting these issues calls for exploring mitigation strategies, be it through negative interventions, an example of which is the EU’s Digital Services Act seeking to regulate deletion of harmful content, or positive interventions strategically introduced into the opinion landscape to boost certain opinions. In our latest work (to be presented at ICWSM 2024), we introduce the Opinion Market Model (OMM) as a testbed to simulate and measure the impact of positive interventions on online opinion dynamics. Additionally, we show the OMM’s capability to detect possible backfiring of debunking.&lt;/p>
&lt;p>In this blog post, we delve into OMM's structure and demonstrate its ability to uncover cooperation-competition relations across online opinions about (a) Australian bushfires and (b) popular VEVO artists, and show how media coverage modulates the spread of far-right opinion.&lt;/p>
&lt;p>&lt;strong>Paper citation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code>Calderon, Pio, Rohit Ram, and Marian-Andrei Rizoiu. &amp;quot;Opinion Market Model:
Stemming Far-Right Opinion Spread using Positive Interventions.&amp;quot; In
International AAAI Conference on Web and Social Media (ICWSM) 2024.
&lt;/code>&lt;/pre>
&lt;p>(&lt;em>see full paper here: &lt;a href="https://arxiv.org/pdf/2208.06620.pdf">&lt;a href="https://arxiv.org/pdf/2208.06620.pdf">https://arxiv.org/pdf/2208.06620.pdf&lt;/a>&lt;/a>&lt;/em>)&lt;/p>
&lt;h2 id="interventions-in-the-online-opinion-ecosystem">Interventions in the Online Opinion Ecosystem&lt;/h2>
&lt;p>Online social media platforms are breeding grounds for discourse, fostering both cooperation and competition among opinions vying for limited public attention [1]. Two distinct intervention strategies have emerged to counter extremist views in the online landscape. Negative interventions operate by diverting attention away from certain opinions, employing measures such as fact-check alerts [2], shadow-banning [3], and outright banning of extremist content [4]. While these methods have proven effective [5], they are predominantly within the control of social media platforms that use them sparingly [6]. Conversely, positive interventions [7], including debunking misinformation [8] and amplifying media coverage [9], combat extremism by boosting certain opinions, often under the purview of governmental and media entities [10].&lt;/p>
&lt;p>Evaluating the feasibility and effectiveness of positive interventions requires the ability (1) to disentangle inter-opinion interactions from the reaction of opinions to interventions and (2) to test the sensitivity of the opinion ecosystem to interventions. We propose the Opinion Market Model handle these two issues effectively.&lt;/p>
&lt;h2 id="our-solution-the-opinion-market-model">Our Solution: The Opinion Market Model&lt;/h2>
&lt;p>The Opinion Market Model (OMM) models online opinion dynamics resulting from inter-opinion interactions and positive interventions&amp;rsquo; influence. Drawing an analogy from economic markets, the model views opinions as analogous to economic goods. Just as goods can compete or reinforce each other's market share, online opinions can engage in similar dynamics—competing for limited online attention or complementing each other. This analogy allows for applying market share attraction models [11], aiding in understanding opinion interactions and their competition for users&amp;rsquo; attention in the online realm.&lt;/p>
&lt;img src="./featured.png" width="60%">
&lt;p style="text-align: center;">&lt;em>Figure 1. The OMM as a two-tier model of the online opinion ecosystem.&lt;/em>&lt;/p>
&lt;p>OMM models the online opinion ecosystem across two tiers: online attention volumes and opinion market shares. Illustrated in Figure 1 is a simplified scenario with two opinions (labeled 0 and 1) on a social media platform. Each opinion is assumed to have two stances denoting far-right support (+) and moderate views (-).&lt;/p>
&lt;p>In terms of system inputs (top row of Figure 1), the OMM makes a distinction between exogenous signals \(S(t)\) (gray) and interventions \(X(t)\) (yellow) to influence online opinions. Exogenous signals modulate the dynamics of the opinions’ sizes and represent events like natural disasters or speeches. Interventions (such as increased media coverage) are designed to add attention to the opinion ecosystem, increasing the market share of certain opinions while suppressing others.&lt;/p>
&lt;p>The first tier (middle row of Figure 1) uses a discrete-time Hawkes process [12] to estimate opinion attention volumes (i.e. the daily number of postings featuring opinions) driven by both exogenous signals and the self-exciting nature of online popularity.
The second tier (bottom row of Figure 1) uses a market share attraction model to capture both inter-opinion interactions, either cooperative or competitive and driven by limited online user attention, and their response to positive interventions.&lt;/p>
&lt;h3 id="datasets">Datasets&lt;/h3>
&lt;p>We utilize two real-world datasets to showcase the predictive and interpretative capabilities of the OMM. Below, we give a short description of each.&lt;/p>
&lt;p>&lt;strong>Bushfire Opinions dataset.&lt;/strong> The Bushfire Opinions dataset contains 90 days of Twitter and Facebook discussions about bushfires and climate change between 1 November 2019 to 29 January 2020. We filter and label relevant Facebook and Twitter postings using the textual topic and opinion classifiers developed in [13] into six opinions:&lt;/p>
&lt;ol start="0">
&lt;li>Greens policies are the cause of the Australian bushfires.&lt;/li>
&lt;li>Mainstream media cannot be trusted.&lt;/li>
&lt;li>Climate change crisis is not real / is a UN hoax.&lt;/li>
&lt;li>Australian bushfires and climate change are not related.&lt;/li>
&lt;li>Australian bushfires were caused by random arsonists.&lt;/li>
&lt;li>Bushfires are a normal summer occurrence in Australia.&lt;/li>
&lt;/ol>
&lt;p>To distinguish between moderate (-) and far-right (+) opinions, we utilize the far-right stance detector in [14], which employs a measure of textual homophily to gauge the similarity between Twitter users and established far-right figures. Our set of opinions is denoted as \({(i−, i+)|i ∈ {0, &amp;hellip; , 5}}\). In total, we analyze data from two platforms, encompassing 74,461 tweets and 7,974 Facebook posts, all labeled with 12 distinct stanced opinions.&lt;/p>
&lt;p>The exogenous signal \(S(t)\) is defined as the 5-day rolling average of the Google Trends query &amp;ldquo;bushfire+climate change&amp;rdquo; in Australia, functioning as a representation of general interest and offline occurrences [15]. For positive interventions \({X_k(t)}\), we incorporate reputable (R) and controversial (C) news coverage from Australian and international sources. Reputable sources are drawn from the Reputable News Index (RNIX) [16], while controversial international sources are accessed through NELA-GT-2019 [17].&lt;/p>
&lt;p>&lt;strong>VEVO 2017 Top 10 dataset.&lt;/strong> We evaluate the OMM on a second dataset obtained by aligning artist-level time series of YouTube views and Twitter post counts for the top 10 VEVO-affiliated artists over a span of 100 days from 2 January 2017, to 11 April 2017. The YouTube data from the VEVO Music Graph dataset [18] encompasses daily view counts for music videos by verified VEVO artists across six English-speaking countries. Twitter post counts are collected through the Twitter API, using the artist's name as the input query. Unlike the Bushfire Opinions dataset, here we utilize distinct exogenous signals for each artist, referred to as \(S_i(t)\), derived from Google Trends. Our focus for the VEVO 2017 Top 10 dataset is uncovering intrinsic interactions among artists; hence, we leave out interventions \({X_k(t)}\).&lt;/p>
&lt;h3 id="predicting-opinion-volumes-and-market-shares">Predicting Opinion Volumes and Market Shares&lt;/h3>
&lt;p>Since OMM is a two-tier model, we can evaluate its predictive capabilities on two levels: (1) predicting opinion volumes and (2) predicting opinion shares. In the first task, we forecast the total volume of opinionated posts across each considered online platform during the prediction period after training on the training period. In the second task, we predict the market shares for each opinion on each platform during the prediction period.&lt;/p>
&lt;p>&lt;img src="./omm_2.png" alt="">&lt;/p>
&lt;p style="text-align: center;">&lt;em>Figure 2. Fitting and predicting with OMM on the Bushfire Opinions dataset.&lt;/em>&lt;/p>
&lt;p>Fig. 2(a) illustrates the alignment between observed (blue line) and modeled (orange line) opinion volumes in the bushfire dataset. Notably, we fit well within both training and prediction periods. In Fig. 2(b), the distribution of observed (left column) and fitted/predicted (right column) opinion market shares for the bushfire dataset are showcased. Again, OMM adeptly captures trends in opinion shares across both platforms. We also compare (see full text) the OMM to a multivariate linear regression baseline and state-of-the-art models in product share modeling (the Correlated Cascades model [19] and the Competing Products model [20]), and show superior performance.&lt;/p>
&lt;h3 id="uncovering-latent-opinion-interactions">Uncovering Latent Opinion Interactions&lt;/h3>
&lt;p>To understand the interactions of opinions with one another, we borrow the economic concept of elasticity and derive opinion share model elasticities from the OMM. These quantify the instantaneous competition-cooperation interactions across opinions, capturing both the direct effect of an opinion on another and the indirect effect of an opinion across other opinions. Elasticities are signed metrics: if positive we observe a cooperative relation, if negative a competitive relation.&lt;/p>
&lt;p>&lt;img src="./omm_3.png" alt="">&lt;/p>
&lt;p style="text-align: center;">&lt;em>Figure 3. (a) Inter-opinion OMM elasticities in the Bushfire Opinions dataset. (b) Youtube elasticities in the VEVO Top 10 dataset.&lt;/em>&lt;/p>
&lt;p>&lt;strong>Bushfire Opinions.&lt;/strong> Figure 3(a) shows the inter-opinion elasticities for the Bushfire Opinions dataset derived from the fitted OMM model. Within Twitter we observe substantial self-reinforcement among opinions, a sign of the echo chamber effect [21]. In addition, we see significant cross-reinforcement between far-right sympathizers and opponents, suggesting interactions or debates between opposing viewpoints. For Facebook, OMM identifies limited interaction between opinions due to Facebook far-right groups acting as a filter bubble.&lt;/p>
&lt;p>The results suggest that to suppress far-right opinions effectively we should avoid direct confrontation, which might unintentionally attract more attention to them. Instead, a more successful approach is to promote counter-arguments related to the far-right opinions. For example, on Twitter, to diminish the impact of the opinion &amp;ldquo;Australian bushfires were caused by random arsonists&amp;rdquo; (4+), the Opinion Market Model (OMM) suggests boosting opinions like &amp;ldquo;Climate change is real&amp;rdquo; (2-) and &amp;ldquo;Greens are not the cause of the bushfires&amp;rdquo; (0-). However, promoting the opposing view, such as &amp;ldquo;Australian bushfires were not caused by random arsonists&amp;rdquo; (4-), could actually backfire. Interestingly, the opinion &amp;ldquo;Bushfires are a normal summer occurrence in Australia&amp;rdquo; (5+) takes a different approach—it reinforces most moderate opinions and discourages far-right ones. Notably, this opinion (5+) seems to trigger &amp;ldquo;Climate change is real&amp;rdquo; (2-), possibly due to the stark contrast between these viewpoints. This effect is consistent across different platforms.&lt;/p>
&lt;p>&lt;strong>VEVO Artists.&lt;/strong> We show the fitted Youtube elasticities for the VEVO artists in Figure 3(b). We make three observations. Firstly, strong self-reinforcement exists for most artists, reflecting their dedicated fanbases. Secondly, the OMM captures significant artist interactions that align with real-world events and relationships. For example, it detects how Calvin Harris inhibits both Taylor Swift and Katy Perry, possibly linked to their past conflicts, while Harris&amp;rsquo; collaboration with Ariana Grande reinforces their positive relationship. Lastly, the unique relationships between artists (Katy Perry, Taylor Swift, and Ariana Grande) within the same genre (mainstream pop) highlight the intricate nature of fanbase dynamics, showing that similar artists can have diverse pairwise interactions rather than solely cooperating or competing for attention.&lt;/p>
&lt;h3 id="omm-as-a-testbed-for-interventions">OMM as a Testbed for Interventions&lt;/h3>
&lt;p>Positive interventions can have delayed impacts on the opinion ecosystem due to the interconnectedness of opinions. When an intervention boosts a particular opinion, it also indirectly boosts opinions that cooperate with it while inhibiting competitive opinions.&lt;/p>
&lt;p>We can leverage the OMM in a &amp;ldquo;what-if&amp;rdquo; scenario to understand long-term effects of media coverage in the bushfire case study. By synthetically varying intervention sizes, the OMM can serve as a testbed for prioritizing interventions and for designing A/B tests to determine causal impacts. The setup involves changing intervention volumes after a certain time, and then measuring the resulting percentage change in far-right opinions.&lt;/p>
&lt;p>&lt;img src="./omm_4.png" alt="">&lt;/p>
&lt;p style="text-align: center;">&lt;em>Figure 4. OMM-simulated percent changes in the far-right (+) opinion market shares on Facebook (left) and Twitter (right) by modulating the volume of reputable (R) and controversial (C) news for each opinion in the Bushfire Opinions dataset.&lt;/em>&lt;/p>
&lt;p>Figure 4 illustrates the average percentage changes in the market share of far-right opinions when manipulating the interventions \({R_i(t), C_i(t)}\) separately. For Facebook, reputable news generally suppresses far-right opinions and most controversial news reinforces them, except for certain topics like &amp;ldquo;Greens policies are the cause of the Australian bushfires&amp;rdquo; (\(R_0\)) and &amp;ldquo;Australian bushfires were caused by arsonists.&amp;rdquo; (\(R_4\)) On Twitter, both reputable and controversial news suppress far-right opinions, except for reputable news related to certain topics.&lt;/p>
&lt;p>The insights are twofold: firstly, Facebook's effect is moderate compared to Twitter due to Facebook's filter bubble nature. Secondly, indiscriminately increasing reputable news on Twitter can be ineffective and even counterproductive as it might attract more attention to far-right narratives and users (see \(R_3\) and \(R_4\)) [22].&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] Wu, S.; Rizoiu, M.-A.; and Xie, L. 2019. Estimating Attention Flow in Online Video Networks. CSCW, 3: 1–25.&lt;br>
[2] Nekmat, E. 2020. Nudge effect of fact-check alerts: source influence and media skepticism on sharing of news misinformation in social media. Social Media+ Society, 6(1).&lt;br>
[3] Young, G. K. 2022. How much is too much: the difficulties of social media content moderation. Information &amp;amp; Communications Technology Law, 31(1): 1–16.&lt;br>
[4] Jackson, S. 2019. The double-edged sword of banning ex- tremists from social media.&lt;br>
[5] Clayton, K.; Blair, S.; Busam, J. A.; Forstner, S.; Glance, J.; Green, G.; Kawata, A.; Kovvuri, A.; Martin, J.; Morgan, E.; et al. 2020. Real solutions for fake news? Measuring the effectiveness of general warnings and fact-check tags in reducing belief in false stories on social media. Political Behavior, 42(4): 1073–1095.&lt;br>
[6] Porter, E.; and Wood, T. J. 2021. Fact checks actually work, even on Facebook. But not enough people see them. The Washington Post.&lt;br>
[7] GIFCT. 2021. Content-Sharing Algorithms, Processes, and Positive Interventions Working Group.&lt;br>
[8] Haciyakupoglu, G.; Hui, J. Y.; Suguna, V.; Leong, D.; and Rahman, M. F. B. A. 2018. Countering fake news: A survey of recent global initiatives.&lt;br>
[9] Horowitz, M.; Cushion, S.; Dragomir, M.; Gutierrez Manjo ́n, S.; and Pantti, M. 2022. A framework for assessing the role of public service media organizations in countering disinformation. Digital Journalism, 10(5).&lt;br>
[10] Radsch, C. 2016. Media Development and Countering Violent Extremism: An Uneasy Relationship, a Need for Dialogue. Center for International Media Assistance. (2016).&lt;br>
[11] Cooper, L. G. 1993. Chapter 6 Market-share models. In Marketing, volume 5, 259–314. Elsevier.&lt;br>
[12] Browning, R.; Sulem, D.; Mengersen, K.; Rivoirard, V.; and Rousseau, J. 2021. Simple discrete-time self-exciting models can describe complex dynamic processes: A case study of COVID-19. PLoS ONE, 16.&lt;br>
[13] Kong, Q.; Booth, E.; Bailo, F.; Johns, A.; and Rizoiu, M.-A. 2022. Slipping to the Extreme: A Mixed Method to Explain How Extreme Opinions Infiltrate Online Discussions. In AAAI ICWSM, volume 16, 524–535.&lt;br>
[14] Ram, R.; Thomas, E.; Kernot, D.; and Rizoiu, M.-A. 2022. Detecting Extreme Ideologies in Shifting Landscapes: an Automatic &amp;amp; Context-Agnostic Approach. arXiv:2208.04097.&lt;br>
[15] Sheshadri, K.; and Singh, M. P. 2019. The public and legislative impact of hyperconcentrated topic news. Science Advances, 5(8).&lt;br>
[16] Kong, Q.; Rizoiu, M.-A.; and Xie, L. 2020. Describing and predicting online items with reshare cascades via dual mixture self-exciting processes. In 29th ACM CIKM, 645–654.&lt;br>
[17] Gruppi, M.; Horne, B. D.; and Adalı, S. 2020. NELA-GT- 2019: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles. arXiv:2003.08444.&lt;br>
[18] Wu, S.; Rizoiu, M.-A.; and Xie, L. 2019. Estimating Attention Flow in Online Video Networks. CSCW, 3: 1–25.&lt;br>
[19] Zarezade, A.; Khodadadi, A.; Farajtabar, M.; Rabiee, H. R.; and Zha, H. 2017. Correlated cascades: Compete or cooperate. AAAI 2017, 238–244.&lt;br>
[20] Valera, I.; and Gomez-Rodriguez, M. 2015. Modeling Adoption and Usage of Competing Products. In ICDM.&lt;br>
[21] Cinelli, M.; De Francisci Morales, G.; Galeazzi, A.; Quattrociocchi, W.; and Starnini, M. 2021. The echo chamber effect on social media. PNAS, 118(9).&lt;br>
[22] Peucker, M.; Fisher, T. J.; and Davey, J. 2022. Mainstream media use in far-right online ecosystems. Technical report.&lt;/p></description></item></channel></rss>