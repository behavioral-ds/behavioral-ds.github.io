@inproceedings{Kong2021b,
    abstract = {Qualitative research provides methodological guidelines for observing and studying communities and cultures on online social media platforms. However, such methods demand considerable manual effort from researchers and may be overly focused and narrowed to certain online groups. In this work, we propose a complete solution to accelerate qualitative analysis of problematic online speech -- with a specific focus on opinions emerging from online communities -- by leveraging machine learning algorithms. First, we employ qualitative methods of deep observation for understanding problematic online speech. This initial qualitative study constructs an ontology of problematic speech, which contains social media postings annotated with their underlying opinions. The qualitative study also dynamically constructs the set of opinions, simultaneous with labeling the postings. Next, we collect a large dataset from three online social media platforms (Facebook, Twitter and Youtube) using keywords. Finally, we introduce an iterative data exploration procedure to augment the dataset. It alternates between a data sampler, which balances exploration and exploitation of unlabeled data, the automatic labeling of the sampled data, the manual inspection by the qualitative mapping team and, finally, the retraining of the automatic opinion classifier. We present both qualitative and quantitative results. First, we present detailed case studies of the dynamics of problematic speech in a far-right Facebook group, exemplifying its mutation from conservative to extreme. Next, we show that our method successfully learns from the initial qualitatively labeled and narrowly focused dataset, and constructs a larger dataset. Using the latter, we examine the dynamics of opinion emergence and co-occurrence, and we hint at some of the pathways through which extreme opinions creep into the mainstream online discourse.},
    archivePrefix = {arXiv},
    arxivId = {2109.00302},
    author = {Kong, Quyu and Booth, Emily and Bailo, Francesco and Johns, Amelia and Rizoiu, Marian-Andrei},
    booktitle = {AAAI International Conference on Web and Social Media},
    eprint = {2109.00302},
    month = {sep},
    title = {{Slipping to the Extreme: A Mixed Method to Explain How Extreme Opinions Infiltrate Online Discussions}},
    URL_paper = {http://arxiv.org/pdf/2109.00302.pdf},
    URL_video = {https://www.youtube.com/watch?v=HwFq3ywanp4},
    URL_code = {https://github.com/behavioral-ds/problematic-speech},
    year = {2022}
}

@inproceedings{Ahadi2022,
    abstract = {Higher education often aims to create job-ready graduates. Thus, the skills and knowledge taught in professional degrees are expected to align with the needs of the labor market. However, the dynamic nature of the job market makes it challenging to ensure that this alignment occurs. In this study, we show how Skills Analytics can be used to identify critical skills in the workforce, mapping these to the curriculum offerings of a university. This enables us to identify skill gaps between what is taught and what is needed in the job market. Methods are presented that allow universities to test the alignment of their curriculum offerings with the job market. Where gaps are identified, this would enable universities to update their curriculum more rapidly to produce graduates equipped with up-to-date skills required by the local job market. Our contributions include: a new method for ranking skills in curricula based on their relative importance in the job market; and proof of concept methods to find skills gaps between curriculum offerings and an identified job market that can lead to curriculum redesign and enhancements.},
    address = {Durham, United Kingdom},
    author = {Alireza Ahadi and Kirsty Kitto and Marian-Andrei Rizoiu and Katarzyna Musial},
    booktitle = {Proceedings of the 15th International Conference on Educational Data Mining},
    doi = {10.5281/zenodo.6853121},
    editor = {Antonija Mitrovic and Nigel Bosch},
    isbn = {978-1-7336736-3-1},
    month = {July},
    pages = {538--542},
    publisher = {International Educational Data Mining Society},
    title = {{Skills Taught vs Skills Sought: Using Skills Analytics to Identify the Gaps between Curriculum and Job Markets}},
    year = {2022},
    URL = {https://zenodo.org/record/6853121},
    URL_paper = {https://educationaldatamining.org/edm2022/proceedings/2022.EDM-posters.56/2022.EDM-posters.56.pdf}
}

@article{Dawson2021a,
    abstract = {Job security can never be taken for granted, especially in times of rapid, widespread and unexpected social and economic change. These changes can force workers to transition to new jobs. This may be because new technologies emerge or production is moved abroad. Perhaps it is a global crisis, such as COVID-19, which shutters industries and displaces labor en masse . Regardless of the impetus, people are faced with the challenge of moving between jobs to find new work. Successful transitions typically occur when workers leverage their existing skills in the new occupation. Here, we propose a novel method to measure the similarity between occupations using their underlying skills. We then build a recommender system for identifying optimal transition pathways between occupations using job advertisements (ads) data and a longitudinal household survey. Our results show that not only can we accurately predict occupational transitions (Accuracy = 76%), but we account for the asymmetric difficulties of moving between jobs (it is easier to move in one direction than the other). We also build an early warning indicator for new technology adoption (showcasing Artificial Intelligence), a major driver of rising job transitions. By using real-time data, our systems can respond to labor demand shifts as they occur (such as those caused by COVID-19). They can be leveraged by policy-makers, educators, and job seekers who are forced to confront the often distressing challenges of finding new jobs.},
    archivePrefix = {arXiv},
    arxivId = {2011.11801},
    author = {Dawson, Nikolas and Williams, Mary-Anne and Rizoiu, Marian-Andrei},
    doi = {10.1371/journal.pone.0254722},
    editor = {Fu, Shihe},
    eprint = {2011.11801},
    issn = {1932-6203},
    journal = {PLOS ONE},
    month = {aug},
    number = {8},
    pages = {e0254722},
    title = {{Skill-driven recommendations for job transition pathways}},
    url = {https://dx.plos.org/10.1371/journal.pone.0254722},
    URL_paper = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0254722&type=printable},
    volume = {16},
    year = {2021}
}


@article{McCarthy2021a,
    abstract = {Ever since the web began, the number of websites has been growing exponentially. These websites cover an ever-increasing range of online services that fill a variety of social and economic functions across a growing range of industries. Yet the networked nature of the web, combined with the economics of preferential attachment, increasing returns and global trade, suggest that over the long run a small number of competitive giants are likely to dominate each functional market segment, such as search, retail and social media. Here we perform a large scale longitudinal study to quantify the distribution of attention given in the online environment to competing organisations. In two large online social media datasets, containing more than 10 billion posts and spanning more than a decade, we tally the volume of external links posted towards the organisations' main domain name as a proxy for the online attention they receive. We also use the Common Crawl dataset—which contains the linkage patterns between more than a billion different websites—to study the patterns of link concentration over the past three years across the entire web. Lastly, we showcase the linking between economic, financial and market data by exploring the relationships between online attention on social media and the growth in enterprise value in the electric carmaker Tesla. Our analysis shows that despite the fact that we observe consistent growth in all the macro indicators—the total amount of online attention, in the number of organisations with an online presence, and in the functions they perform—we also observe that a smaller number of organisations account for an ever-increasing proportion of total user attention, usually with one large player dominating each function. These results highlight how evolution of the online economy involves innovation, diversity, and then competitive dominance.},
    archivePrefix = {arXiv},
    arxivId = {2003.07049},
    author = {McCarthy, Paul X. and Gong, Xian and Eghbal, Sina and Falster, Daniel S. and Rizoiu, Marian-Andrei},
    doi = {10.1371/journal.pone.0249993},
    editor = {Zollo, Fabiana},
    eprint = {2003.07049},
    issn = {1932-6203},
    journal = {PLOS ONE},
    month = {apr},
    number = {4},
    pages = {e0249993},
    title = {{Evolution of diversity and dominance of companies in online activity}},
    url = {https://dx.plos.org/10.1371/journal.pone.0249993},
%    URL_paper = {http://arxiv.org/abs/2003.07049},
    URL_paper = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0249993&type=printable},
    volume = {16},
    year = {2021}
}

@article{Unwin2021,
    abstract = {Developing new methods for modelling infectious diseases outbreaks is important for monitoring transmission and developing policy. In this paper we propose using semi-mechanistic Hawkes Processes for modelling malaria transmission in near-elimination settings. Hawkes Processes are well founded mathematical methods that enable us to combine the benefits of both statistical and mechanistic models to recreate and forecast disease transmission beyond just malaria outbreak scenarios. These methods have been successfully used in numerous applications such as social media and earthquake modelling, but are not yet widespread in epidemiology. By using domain-specific knowledge, we can both recreate transmission curves for malaria in China and Eswatini and disentangle the proportion of cases which are imported from those that are community based.},
    author = {Unwin, H Juliette T and Routledge, Isobel and Flaxman, Seth and Rizoiu, Marian-Andrei and Lai, Shengjie and Cohen, Justin and Weiss, Daniel J and Mishra, Swapnil and Bhatt, Samir},
    doi = {10.1371/journal.pcbi.1008830},
    editor = {Perkins, Alex},
    issn = {1553-7358},
    journal = {PLOS Computational Biology},
    month = {apr},
    number = {4},
    pages = {e1008830},
    title = {{Using Hawkes Processes to model imported and local malaria cases in near-elimination settings}},
    url = {https://dx.plos.org/10.1371/journal.pcbi.1008830},
    volume = {17},
    URL_paper = {https://www.medrxiv.org/content/10.1101/2020.07.17.20156174v1.full.pdf},
    URL_code = {https://github.com/mrc-ide/epihawkes},
    year = {2021}
}

@article{Dawson2021,
    abstract = {In Australia and beyond, journalism is reportedly an industry in crisis, a crisis exacerbated by COVID-19. However, the evidence revealing the crisis is often anecdotal or limited in scope. In this unprecedented longitudinal research, we draw on data from the Australian journalism jobs market from January 2012 until March 2020. Using Data Science and Machine Learning techniques, we analyse two distinct data sets: job advertisements (ads) data comprising 3698 journalist job ads from a corpus of over 8 million Australian job ads; and official employment data from the Australian Bureau of Statistics. Having matched and analysed both sources, we address both the demand for and supply of journalists in Australia over this critical period. The data show that the crisis is real, but there are also surprises. Counter-intuitively, the number of journalism job ads in Australia rose from 2012 until 2016, before falling into decline. Less surprisingly, for the entire period studied the figures reveal extreme volatility, characterised by large and erratic fluctuations. The data also clearly show that COVID-19 has significantly worsened the crisis. We then tease out more granular findings, including: that there are now more women than men journalists in Australia, but that gender inequity is worsening, with women journalists getting younger and worse-paid just as men journalists are, on average, getting older and better-paid; that, despite the crisis besetting the industry, the demand for journalism skills has increased; and that, perhaps concerningly, the skills sought by journalism job ads increasingly include ‘social media' and ‘generalist communications' skills.},
    archivePrefix = {arXiv},
    arxivId = {2008.12459},
    author = {Dawson, Nikolas and Molitorisz, Sacha and Rizoiu, Marian-Andrei and Fray, Peter},
    doi = {10.1177/1464884921996286},
    eprint = {2008.12459},
    issn = {1464-8849},
    journal = {Journalism},
    month = {feb},
    pages = {146488492199628},
    title = {{Layoffs, inequity and COVID-19: A longitudinal study of the journalism jobs crisis in Australia from 2012 to 2020}},
    url = {http://journals.sagepub.com/doi/10.1177/1464884921996286},
    year = {2021},
    URL_paper = {https://arxiv.org/pdf/2008.12459.pdf}
}

@inproceedings{Largeron2021,
    abstract = {This paper studies the dynamics of opinion formation and polarization in social media. We investigate whether users' stance concerning contentious subjects is influenced by the online discussions they are exposed to and interactions with users supporting different stances. We set up a series of predictive exercises based on machine learning models. Users are described using several posting activities features capturing their overall activity levels, posting success, the reactions their posts attract from users of different stances, and the types of discussions in which they engage. Given the user description at present, the purpose is to predict their stance in the future. Using a dataset of Brexit discussions on the Reddit platform, we show that the activity features regularly outperform the textual baseline, confirming the link between exposure to discussion and opinion. We find that the most informative features relate to the stance composition of the discussion in which users prefer to engage.},
    author = {Largeron, Christine and Mardale, Andrei and Rizoiu, Marian-Andrei},
    booktitle = {Symposium on Intelligent Data Analysis (IDA'21)},
    doi = {10.1007/978-3-030-74251-5_22},
    pages = {275--286},
    title = {{Linking the Dynamics of User Stance to the Structure of Online Discussions}},
    url = {https://link.springer.com/10.1007/978-3-030-74251-5_22},
    year = {2021},
    URL_paper = {https://arxiv.org/pdf/2101.09852.pdf},
    URL_code = {https://github.com/behavioral-ds/online-opinion-dynamics},
    URL_video = {https://www.youtube.com/watch?v=DPy3l0RdgPM}
}

@inproceedings{Ram2021a,
    abstract = {The impact of online social media on societal events and institutions is profound, and with the rapid increases in user uptake, we are just starting to understand its ramifications. Social scientists and practitioners who model online discourse as a proxy for real-world behavior often curate large social media datasets. A lack of available tooling aimed at non-data science experts frequently leaves this data (and the insights it holds) underutilized. Here, we propose birdspotter – a tool to analyze and label Twitter users –, and birdspotter.ml – an exploratory visualizer for the computed metrics. birdspotter provides an end-to-end analysis pipeline, from the processing of pre-collected Twitter data to general-purpose labeling of users and estimating their social influence, within a few lines of code. The package features tutorials and detailed documentation. We also illustrate how to train birdspotter into a fully-fledged bot detector that achieves better than state-of-the-art performances without making Twitter API calls, and we showcase its usage in an exploratory analysis of a topical COVID-19 dataset.},
    address = {New York, NY, USA},
    author = {Ram, Rohit and Kong, Quyu and Rizoiu, Marian-Andrei},
    booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
    doi = {10.1145/3437963.3441695},
    isbn = {9781450382977},
    issn = {23318422},
    month = {mar},
    pages = {918--921},
    publisher = {ACM},
    title = {{Birdspotter: A Tool for Analyzing and Labeling Twitter Users}},
    url = {https://dl.acm.org/doi/10.1145/3437963.3441695},
    year = {2021},
    URL_paper = {https://arxiv.org/pdf/2012.02370.pdf},
    URL_code = {https://github.com/behavioral-ds/BirdSpotter},
    URL_video = {https://youtu.be/52HwHAiK1rs}
}
@inproceedings{Kong2021a,
    abstract = {Modeling online discourse dynamics is a core activity in understanding the spread of information, both offline and online, and emergent online behavior. There is currently a disconnect between the practitioners of online social media analysis - usually social, political and communication scientists - and the accessibility to tools capable of handling large quantities of online data, and examining online users and their behavior. We present two tools,birdspotter and evently, for analyzing online users based on their involvement in retweet cascades. birdspotter provides a toolkit to measure social influence and botnets of Twitter users. While it leverages the multimodal information of tweets, such as text contents, evently augments the user measurement by modeling the temporal dynamics of information diffusions using self-exciting processes. Both tools are designed for users with a wide range of computer expertise and include tutorials and detailed documentation. We illustrate a case study of a topical dataset relating to COVID-19, using both tools for end-to-end analysis of online user behavior.},
    address = {New York, NY, USA},
    archivePrefix = {arXiv},
    arxivId = {2006.06167},
    author = {Kong, Quyu and Ram, Rohit and Rizoiu, Marian-Andrei},
    booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
    doi = {10.1145/3437963.3441708},
    eprint = {2006.06167},
    isbn = {9781450382977},
    month = {mar},
    pages = {1097--1100},
    publisher = {ACM},
    title = {{Evently: Modeling and Analyzing Reshare Cascades with Hawkes Processes}},
    url = {https://dl.acm.org/doi/10.1145/3437963.3441708},
    year = {2021},
    URL_paper = {https://arxiv.org/pdf/2006.06167.pdf},
    URL_code = {https://github.com/behavioral-ds/evently},
    URL_video = {https://youtu.be/zSMHol0qsy4}
}

@phdthesis{Liu2021,
    abstract = {With the prosperity of the online labour market, more and more employers are willing to post recruitment advertisements on the websites. The demand of the labour market changes in a unpredictable speed and many new skills emerge then quickly reflect on the labour market. For the purpose of extracting the existing skills and even find new emerging skills, we leverage the popular natural language processing (NLP) framework, and build reliable model to accomplish this goal. In this research, we use job advertisements from five different English speaking countries including Australia, Canada, UK, Singapore, US and mainly use a natural language processing frame work called spaCy to build named entity recognition (NER) model to identify the skills in these job ads. We propose two kinds of model one is called proprietary model, the other is called joint model to this skills detecting task. We summarise the pros and cons for these two kinds of model respectively and give suggestions about what kind of model should be chosen to solve different problems. Additional to the spacy NER model, we also analyze the factors leading to wrong prediction and compare with the models trained by another NLP framework Flair. The contributions of our research consists of two aspects. One is that we built a reliable NER model whose prediction result is impressive using spaCy and we prove that spaCy is a more suitable choice than Flair when you have rigid time limitations for training, relatively high requirement for accuracy and does not demand powerful device. Furthermore, no prior work provides a NER model to finish this task, our work can play an important role in future related research not necessarily restricted in extracting skills from job ads.},
    author = {Liu, Yaozhong},
    school = {Australian National University},
    title = {{Labour dynamics in the age of automation: detecting emergent skills in labour markets from job ads description}},
    year = {2021},
    URL_paper = {https://www.behavioral-ds.science/authors/yaozhong-liu/thesis.pdf}
}

@phdthesis{Law2021,
    abstract = {Recently, social media has been blamed for the increasingly polarised nature of polit- ical discourse in our society. The ability to measure and combat political polarisation on social media is of significant importance if we wish to prevent polarisation from degrading the functioning of democracy and social cohesion. Stance detection pro- vides a viable solution for addressing this problem, however so far no research has tested this technique on highly structured online discussions such as those found on the Reddit social media platform. In this thesis, we propose a pipeline for annotating Reddit submissions for stance via crowdsourced workers from Amazon Mechanical Turk (MTurk). We conduct ex- periments to determine the optimum approach and parameters for conducting stance labelling with MTurk and produce a dataset of 5895 labelled r/Brexit submissions. We analyse the dynamics around r/Brexit discussions relating user activity to the occurrence of political events. We evaluate various novel strategies for improving BERT model performance on stance detection. Finally, we implement a state of the art stance detection model for Reddit user stance towards Brexit that achieves an F1 Score of 0.5547 compared to 0.3203 obtained by our previous baseline model.},
    author = {Law, Andrew},
    school = {The Australian National University},
    title = {{Exposing the Stance of Reddit Users Towards Brexit}},
    year = {2021},
    URL_paper = {../authors/andrew-law/andrew_law_thesis.pdf}
}

@phdthesis{Khuu2021,
    abstract = {The popularisation of social media has led to widespread occurrences of echo cham- bers, selective exposure and misinformation. This is particularly concerning with regard to contentious topics, where lack of interaction with opposing views can lead to complacence or stubbornness. We build on past work in an attempt to deter- mine how exposure to differing opinions affects an individual's future opinion. We quickly discover that: 1. The problem goes far beyond a simple discrete classification task due to the subtleties of user sentiment and 2. Future stance information being conditional on users choosing to remain active in the discussion network. We address the first issue by proposing a continuous polarity metric to quantify the attitudes of users and find that individuals who choose to remain are polarised users who are stubborn in their beliefs. To resolve the second point we must first determine what makes users choose to leave. We find that future presence correlates with user interaction and social neighbourhood size. Finally, we propose a sequence model that takes into account individual interac- tions to predict future user behaviour.},
    author = {Khuu, Duy},
    title = {{Polarisation and Influence: Investigating Brexit Opinion Dynamics on Reddit}},
    URL_paper = {../authors/duy-khuu/duy-khuu-thesis.pdf},
    year = {2021}
}

@phdthesis{Willingham2021,
    abstract = {The spread of disinformation in the 21st century has become of enormous concern for the integrity of democracy, the way we relate to each other online, and in extreme cases, the health and safety of individuals. This project explores how we can utilise information from disinformation campaigns in the past to predict disinformation as it arises into the future. Building on prior work analysing the structure of a limited number of political scandals, we continue to explore three main streams of work in detecting disinformation: content analysis, hashtag analysis, and network analysis. We trial a number of different content analysis methods, finding that a pre- trained BERT is capable of significantly exceeding a random baseline at detecting disinformation in unseen data from a new political context. We then demonstrate that polarising hashtags can be identified by clustering hashtags based on the users who use them. We finally go on to demonstrate an initial approach to combining information from hashtags or content with the interaction networks that have been shown to be effective in past work. All of these techniques combine to provide a platform for a system that could detect disinformation in real time, as it emerges in political contexts that do not yet exist.},
    author = {Willingham, Thomas},
    doi = {10.13140/RG.2.2.12134.70729},
    school = {Australian National University},
    title = {{Detecting Disinformation and Information Warfare on Social Media}},
    URL_paper = {../authors/thomas-willingham/thesis.pdf},
    year = {2021}
}


@inproceedings{Rizoiu2013b,
    abstract = {The objective of the thesis is to explore how complex data can be treated using unsupervised machine learning techniques, in which additional information is injected to guide the exploratory process. Starting from specific problems, our contributions take into account the different dimensions of the complex data: their nature (image, text), the additional information attached to the data (labels, structure, concept ontologies) and the temporal dimension. A special attention is given to data representation and how additional information can be leveraged to improve this representation.},
    address = {Beijing, China},
    author = {Rizoiu, Marian-Andrei},
    booktitle = {International Joint Conference on Artificial Intelligence IJCAI'13},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_IJCAI-DC-2013.pdf},
    pages = {3239--3240},
    publisher = {AAAI Press},
    series = {IJCAI '13},
    title = {{Semi-Supervised Structuring of Complex Data}},
    year = {2013},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_IJCAI-DC-2013.pdf}
}

@inproceedings{RIZ12,
    abstract = {In this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension. Observations that are close in the description space, but distant in time are considered as dissimilar. We also propose a method to enforce the segmentation contiguity, by introducing, in the objective function, a penalty term inspired from the Normal Distribution Function. We combine the two propositions into a novel time-driven constrained clustering algorithm, called TDCK-Means, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space. This algorithm uses soft semi-supervised constraints, to encourage adjacent observations belonging to the same entity to be assigned to the same cluster. We apply our algorithm to a Political Studies dataset in order to detect typical evolution phases. We adapt the Shannon entropy in order to measure the entity contiguity, and we show that our proposition consistently improves temporal cohesion of clusters, without any significant loss in the multidimensional variance.},
    address = {Athens, Greece},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    booktitle = {2012 IEEE 24th International Conference on Tools with Artificial Intelligence},
    doi = {10.1109/ICTAI.2012.88},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_ICTAI-2012-preprint.pdf},
    isbn = {978-1-4799-0227-9},
    issn = {10823409},
    keywords = {contiguity penalty function,contiguity penalty function.,semi-supervised clustering,temporal clustering,temporal-aware dissimilarity measure},
    mendeley-groups = {Aigaion Import},
    month = {nov},
    pages = {610--617},
    publisher = {IEEE},
    series = {ICTAI '12},
    title = {{Structuring Typical Evolutions Using Temporal-Driven Constrained Clustering}},
    url = {http://ieeexplore.ieee.org/document/6495100/},
    volume = {1},
    year = {2012},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_ICTAI-2012-slides.pdf}
}
@article{Musat2012,
    abstract = {This work outlines a novel system that automatically extracts conceptual labels for statistically obtained topics. By creating a projection of the topic, which is a distribution over all the vocabulary words, over the WordNet ontology we succeed in associating concepts to the said groups of words. The most important contributions of this paper are connected to the validation of the role of these concepts as topical labels and the determination of correlations that emerge between the utility of these labels and the strength of the relation between the concepts and the topics.},
    author = {Muşat, Claudiu Cristian and Trǎuşan-Matu, Ştefan and Velcin, Julien and Rizoiu, Marian-Andrei},
%    URL_paper = {:home/andrei/Mendeley Desktop/Muşat et al/UPB Scientific Bulletin, Series C Electrical Engineering/Muşat et al. - 2012 - Automatic extraction of conceptual labels from topic models.pdf:pdf},
    journal = {UPB Scientific Bulletin, Series C: Electrical Engineering},
    keywords = {Conceptual processing,Labels,Topic models,WordNet},
    number = {2},
    pages = {57--68},
    title = {{Automatic extraction of conceptual labels from topic models}},
    volume = {74},
    year = {2012}
}
@inproceedings{MUS11,
    abstract = {The growing number of statistical topic models led to the need to better evaluate their output. Traditional evaluation means estimate the model's fitness to unseen data. It has recently been proven than the output of human judgment can greatly differ from these measures. Thus the need for methods that better emulate human judgment is stringent. In this paper we present a system that computes the conceptual relevance of individual topics from a given model on the basis of information drawn from a given concept hierarchy, in this case WordNet. The notion of conceptual relevance is regarded as the ability to attribute a concept to each topic and separate words related to the topic from the unrelated ones based on that concept. In multiple experiments we prove the correlation between the automatic evaluation method and the answers received from human evaluators, for various corpora and difficulty levels. By changing the evaluation focus from a statistical one to a conceptual one we were able to detect which topics are conceptually meaningful and rank them accordingly.},
    address = {Barcelona, Catalonia, Spain},
    author = {Muşat, Claudiu Cristian and Velcin, Julien and Trǎuşan-Matu, Ştefan and Rizoiu, Marian-Andrei},
    booktitle = {International Joint Conference on Artificial Intelligence, Proceedings of the Twenty-Second},
    doi = {10.5591/978-1-57735-516-8/IJCAI11-312},
    isbn = {978-1-57735-515-1},
    keywords = {natural language processing},
    pages = {1866--1871},
    publisher = {AAAI Press},
    series = {IJCAI 2011},
    title = {{Improving topic evaluation using conceptual knowledge}},
    url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/viewPDFInterstitial/3010/3754},
    volume = {3},
    year = {2011},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_IJCAI-2011.pdf},
}
@inproceedings{MUS11a,
    abstract = {We propose a system which employs conceptual knowledge to improve topic models by removing unrelated words from the simplified topic description. We use WordNet to detect which topical words are not conceptually similar to the others and then test our assumptions against human judgment. Results obtained on two different corpora in different test conditions show that the words detected as unrelated had a much greater probability than the others to be chosen by human evaluators as not being part of the topic at all. We prove that there is a strong correlation between the said probability and an automatically calculated topical fitness and we discuss the variation of the correlation depending on the method and data used.},
    address = {Warsaw, Poland},
    author = {Muşat, Claudiu Cristian and Velcin, Julien and Rizoiu, Marian-Andrei and Trǎuşan-Matu, Ştefan},
    booktitle = {International Symposium on Methodologies for Intelligent Systems},
    doi = {10.1007/978-3-642-22732-5_12},
    isbn = {978-3-642-22732-5},
    issn = {1860949X},
    keywords = {Evaluation,Improvement,Ontologies,Topic Models},
    pages = {133--142},
    publisher = {Springer},
    series = {ISMIS{\~{}}'11},
    title = {{Concept-Based Topic Model Improvement}},
    url = {http://link.springer.com/10.1007/978-3-642-22732-5{\_}12},
    volume = {369},
    year = {2011},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_ISMIS-2011.pdf},
}
@incollection{Rizoiu2011,
    abstract = {This chapter addresses the issue of topic extraction from text corpora for ontology learning. The first part provides an overview of some of the most significant solutions present today in the literature. These solutions deal mainly with the inferior layers of the Ontology Learning Layer Cake. They are related to the challenges of the Terms and Synonyms layers. The second part shows how these pieces can be bound together into an integrated system for extracting meaningful topics. While the extracted topics are not proper concepts as yet, they constitute a convincing approach towards concept building and therefore ontology learning. This chapter concludes by discussing the research undertaken for filling the gap between topics and concepts as well as perspectives that emerge today in the area of topic extraction.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien},
    booktitle = {Ontology Learning and Knowledge Discovery Using the Web},
    doi = {10.4018/978-1-60960-625-1.ch003},
    editor = {Wong, Wilson and Liu, Wei and Bennamoun, Mohammed},
    URL_paper = {https://eric.univ-lyon2.fr/~jvelcin/public/publis/bookchapter_rizoiu_2011.pdf},
    pages = {38--60},
    publisher = {IGI Global},
    title = {{Topic Extraction for Ontology Learning}},
    url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60960-625-1.ch003},
    year = {2011}
}
@inproceedings{RIZ10,
    abstract = {Organiser les donn{\'{e}}es textuelles et en tirer du sens est un d{\'{e}}fi majeur aujourd'hui. Ainsi, lorsque l'on souhaite analyser un d{\'{e}}bat en ligne ou un forum de discussion, on voudrait pouvoir rapidement voir quels sont les principaux th{\`{e}}mes abord{\'{e}}s et la mani{\`{e}}re dont la discussion se structure autour d'eux. Pour cela, et parce que un m{\^{e}}me texte peut {\^{e}}tre associ{\'{e}} {\`{a}} plusieurs th{\`{e}}mes, nous proposons une m{\'{e}}thode originale pour regrouper les donn{\'{e}}es textuelles en autorisant les chevauchements et pour nommer chaque groupe de mani{\`{e}}re lisible. La contribution principale de cet article est une m{\'{e}}thode globale qui permet de r{\'{e}}aliser toute la cha{\^{i}}ne, partant des donn{\'{e}}es textuelles brutes jusqu'{\`{a}} la caract{\'{e}}risation des groupes {\`{a}} un niveau s{\'{e}}mantique qui d{\'{e}}passe le simple ensemble de mots.},
    address = {Hammamet, Tunisie},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Chauchat, Jean-Hugues},
    booktitle = {Extraction et Gestion des Connaissances, (EGC 10) 10{\`{e}}me Conf{\'{e}}rence},
    keywords = {clustering,overlapping,text mining},
    organization = {C{\'{e}}padu{\`{e}}s},
    pages = {561--572},
    publisher = {Revue des Nouvelles Technologies de l'Information},
    series = {Revue des Nouvelles Technologies de l'Information},
    title = {{Regrouper les donn{\'{e}}es textuelles et nommer les groupes {\`{a}} l'aide des classes recouvrantes}},
    volume = {E-19},
    year = {2010},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_EGC-2010.pdf},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_EGC-2010-slides.pdf}
}

@article{MUS10,
    abstract = {Topic modeling is a growing research field and novel ways of interpreting and evaluating results are necessary. We propose a method for evaluating and improving the performance of topic models generating algorithms relying on WordNet data. We first propose a measure for determining a topic model's fitness factoring in its broadness and redundancy. Then, for each individual topic, the amount of relevant information it provides, along with its most important words and related concepts are determined by defining a cohesion function based on the topic's projection on WordNet concepts. The model as a whole is improved by eliminating each topic's outliers with respect to the ontology projection. We define a inter topic ontology based distance and we further use it to investigate the impact of removing redundant topics from a model with regard to the overlap between topics' ontological projections. Clustering similar topics into conceptually cohesive groups is tried as an alternative to pruning less relevant topics. Results show that evaluating and improving statistical models with WordNet is a promising research track that leads to more coherent topic models.},
    author = {Muşat, Claudiu Cristian and Rizoiu, Marian-Andrei and Trǎuşan-Matu, Ştefan},
    issn = {1843-4460},
    journal = {Romanian Journal of Human-Computer Interaction},
    number = {2},
    pages = {81--96},
    title = {{An Intra and Inter-Topic Evaluation and Cleansing Method}},
    volume = {3},
    year = {2010},
    url = {http://rochi.utcluj.ro/rrioc/en/rrioc-2010-2.html#An_Intra_and_Inter-Topic_Evaluation_and},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_RRIOC-2010.pdf}
}
@article{Rizoiu2013a,
    abstract = {Feature-based format is the main data representation format used by machine learning algorithms. When the features do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly-constructed features are rarely comprehensible. We seek to construct, in an unsupervised way, new features that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new features as conjunctions of the initial primitive features or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like sky ∧ ¬building ∧ panorama would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is used in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    doi = {10.1007/s10844-013-0235-x},
    URL_paper = {http://arxiv.org/pdf/1512.05467.pdf},
    issn = {0925-9902},
    journal = {Journal of Intelligent Information Systems},
    keywords = {Algorithms for data and knowledge management,Clustering,Data mining,Feature evaluation,Heuristic methods,Nonparametric statistics,Pattern analysis,Representations,Unsupervised feature construction,clustering,data mining,feature evaluation,nonparametric statistics,representations,unsupervised feature construction},
    month = {jun},
    number = {3},
    pages = {501--527},
    title = {{Unsupervised feature construction for improving data representation and semantics}},
    url = {http://link.springer.com/10.1007/s10844-013-0235-x},
    volume = {40},
    year = {2013}
}

@article{Rizoiu2014b,
    abstract = {In this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension. Observations that are close in the description space, but distant in time are considered as dissimilar. We also propose a method to enforce the segmentation contiguity, by introducing, in the objective function, a penalty term inspired from the Normal Distribution Function. We combine the two propositions into a novel time-driven constrained clustering algorithm, called TDCK-Means, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space. This algorithm uses soft semi-supervised constraints, to encourage adjacent observations belonging to the same entity to be assigned to the same cluster. We apply our algorithm to a Political Studies dataset in order to detect typical evolution phases. We adapt the Shannon entropy in order to measure the entity contiguity, and we show that our proposition consistently improves temporal cohesion of clusters, without any significant loss in the multidimensional variance.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    doi = {10.1142/S0218213014600136},
    URL_paper = {http://arxiv.org/pdf/1601.02603.pdf},
    issn = {0218-2130},
    journal = {International Journal on Artificial Intelligence Tools},
    month = {aug},
    number = {04},
    pages = {1460013},
    title = {{How to Use Temporal-Driven Constrained Clustering to Detect Typical Evolutions}},
    url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213014600136},
    volume = {23},
    year = {2014}
}
@article{Rizoiu2015a,
    abstract = {One of the prevalent learning tasks involving images is content-based image classification. This is a difficult task especially because the low-level features used to digitally describe images usually capture little information about the semantics of the images. In this paper, we tackle this difficulty by enriching the semantic content of the image representation by using external knowledge. The underlying hypothesis of our work is that creating a more semantically rich representation for images would yield higher machine learning performances, without the need to modify the learning algorithms themselves. The external semantic information is presented under the form of non-positional image labels, therefore positioning our work in a weakly supervised context. Two approaches are proposed: the first one leverages the labels into the visual vocabulary construction algorithm, the result being dedicated visual vocabularies. The second approach adds a filtering phase as a pre-processing of the vocabulary construction. Known positive and known negative sets are constructed and features that are unlikely to be associated with the objects denoted by the labels are filtered. We apply our proposition to the task of content-based image classification and we show that semantically enriching the image representation yields higher classification performances than the baseline representation.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    doi = {10.3233/IDA-140702},
    URL_paper = {http://arxiv.org/pdf/1512.04605.pdf},
    journal = {Intelligent Data Analysis},
    keywords = {bag-of-features representation,image numerical representation,semantic-enriched representation,semisupervised learning,visual vocabulary construction},
    number = {1},
    pages = {161--185},
    title = {{Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised Context}},
    volume = {19},
    year = {2015},
    url = {http://dx.doi.org/10.3233/IDA-140702}
}
@article{Rizoiu2015,
    abstract = {We present CommentWatcher, an open source tool aimed at analyzing discussions on web forums. Constructed as a web platform, CommentWatcher features automatic mass fetching of user posts from forum on multiple sites, extracting topics, visualizing the topics as an expression cloud and exploring their temporal evolution. The underlying social network of users is simultaneously constructed using the citation relations between users and visualized as a graph structure. Our platform addresses the issues of the diversity and dynamics of structures of webpages hosting the forums by implementing a parser architecture that is independent of the HTML structure of webpages. This allows easy on-the-fly adding of new websites. Two types of users are targeted: end users who seek to study the discussed topics and their temporal evolution, and researchers in need of establishing a forum benchmark dataset and comparing the performances of analysis tools.},
    annote = {NULL},
    archivePrefix = {arXiv},
    arxivId = {1504.07459},
    author = {Rizoiu, Marian-Andrei and Guille, Adrien and Velcin, Julien},
    eprint = {1504.07459},
    URL_paper = {http://arxiv.org/pdf/1504.07459.pdf},
    journal = {arXiv preprint},
    month = {apr},
    title = {{CommentWatcher: An Open Source Web-based platform for analyzing discussions on web forums}},
    url = {http://arxiv.org/abs/1504.07459},
    year = {2015},
    URL_code = {https://github.com/behavioral-ds/CommentWatcher}
}
@inproceedings{Kim2015a,
    abstract = {Evolutionary clustering aims at capturing the temporal evolution of clusters. This issue is particularly important in the context of social media data that are naturally temporally driven. In this paper, we propose a new probabilistic model-based evolutionary clustering technique. The Temporal Multinomial Mixture (TMM) is an extension of classical mixture model that optimizes feature co-occurrences in the trade-off with temporal smoothness. Our model is evaluated for two recent case studies on opinion aggregation over time. We compare four different probabilistic clustering models and we show the superiority of our proposal in the task of instance-oriented clustering.},
    address = {Vienna, Austria.},
    archivePrefix = {arXiv},
    arxivId = {1601.02300},
    author = {Kim, Young-Min and Velcin, Julien and Bonnevay, St{\'{e}}phane and Rizoiu, Marian-Andrei},
    booktitle = {European Conference on Information Retrieval, Proceedings of the 37th},
    doi = {10.1007/978-3-319-16354-3_66},
    eprint = {1601.02300},
    URL_paper = {http://arxiv.org/pdf/1601.02300.pdf},
    isbn = {9783319163536},
    issn = {16113349},
    keywords = {evolutionary clustering,mixture model,temporal analysis.},
    pages = {593--604},
    publisher = {Springer International Publishing Switzerland},
    series = {ECIR '15},
    title = {{Temporal Multinomial Mixture for Instance-Oriented Evolutionary Clustering}},
    url = {http://link.springer.com/10.1007/978-3-319-16354-3{\_}66},
    volume = {9022},
    year = {2015}
}
@article{Rizoiu2016a,
    abstract = {We propose ClusPath, a novel algorithm for detecting general evolution tendencies in a population of entities. We show how abstract notions, such as the Swedish socio-economical model (in a political dataset) or the companies fiscal optimization (in an eco- nomical dataset) can be inferred from low-level descriptive features. Such high-level regularities in the evolution of entities are detected by combining spatial and temporal features into a spatio-temporal dissimilarity measure and using semi-supervised clustering techniques. The relations between the evolution phases are modeled using a graph structure, inferred si- multaneously with the partition, by using a “slow changing world” assumption. The idea is to ensure a smooth passage for entities along their evolution paths, which catches the long- term trends in the dataset. Additionally, we also provide a method, based on an evolutionary algorithm, to tune the parameters of ClusPath to new, unseen datasets. This method assesses the fitness of a solution using four opposed quality measures and proposes a balanced com- promise.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Bonnevay, St{\'{e}}phane and Lallich, St{\'{e}}phane},
    doi = {10.1007/s10618-015-0445-7},
    URL_paper = {http://arxiv.org/pdf/1512.03501.pdf},
    issn = {1384-5810},
    journal = {Data Mining and Knowledge Discovery},
    keywords = {Pareto front estimation,detection of long-term trends,evolutionary clustering,semi-supervised clustering,temporal cluster graph,temporal clustering},
    month = {sep},
    number = {5},
    pages = {1324--1349},
    series = {EKML/PKDD{\~{}}'14},
    title = {{ClusPath: a temporal-driven clustering to infer typical evolution paths}},
    url = {http://link.springer.com/10.1007/s10618-015-0445-7},
    volume = {30},
    year = {2016},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_PKDD-2016_slides.pdf},
    URL_code = {https://github.com/behavioral-ds/cluspath-distrib}
}


@article{kong2020cikm,
    author={Kong, Quyu and Rizoiu, Marian-Andrei and Xie, Lexing},
    title={Describing and Predicting Online Items with Reshare Cascades via Dual Mixture Self-exciting Processes},
    booktitle={ACM International Conference on Information and Knowledge Management (CIKM'20)},
    pages={645--654},
    year={2020},
    url_Paper = {https://arxiv.org/pdf/2001.11132.pdf},
    url_Code = {https://github.com/qykong/dual-mixture-hawkes-processes},
    abstract = {It is well-known that online behavior is long-tailed, with most cascaded actions being short and a few being very long. A prominent drawback in generative models for online events is the inability to describe unpopular items well. This work addresses these shortcomings by proposing dual mixture self-exciting processes to jointly learn from groups of cascades. We first start from the observation that maximum likelihood estimates for content virality and influence decay are separable in a Hawkes process. Next, our proposed model, which leverages a Borel mixture model and a kernel mixture model, jointly models the unfolding of a heterogeneous set of cascades. When applied to cascades of the same online items, the model directly characterizes their spread dynamics and supplies interpretable quantities, such as content virality and content influence decay, as well as methods for predicting the final content popularities. On two retweet cascade datasets --- one relating to YouTube videos and the second relating to controversial news articles --- we show that our models capture the differences between online items at the granularity of items, publishers and categories. In particular, we are able to distinguish between far-right, conspiracy, controversial and reputable online news articles based on how they diffuse through social media, achieving an F1 score of 0.945. On holdout datasets, we show that the dual mixture model provides, for reshare diffusion cascades especially unpopular ones, better generalization performance and, for online items, accurate item popularity predictions.},
    url = {https://doi.org/10.1145/3340531.3411861}
}

@inproceedings{kong2020modeling,
    title={Modeling Information Cascades with Self-exciting Processes via Generalized Epidemic Models},
    author={Kong, Quyu and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle={ACM International Conference on Web Search and Data Mining (WSDM'20)},
    year={2020},
    abstract = {Epidemic models and self-exciting processes are two types of models used to describe diffusion phenomena online and offline. These models were originally developed in different scientific communities, and their commonalities are under-explored. This work establishes, for the first time, a general connection between the two model classes via three new mathematical components. The first is a generalized version of stochastic Susceptible-Infected-Recovered (SIR) model with arbitrary recovery time distributions; the second is the relationship between the (latent and arbitrary) recovery time distribution, recovery hazard function, and the infection kernel of self-exciting processes; the third includes methods for simulating, fitting, evaluating and predicting the generalized process. On three large Twitter diffusion datasets, we conduct goodness-of-fit tests and holdout log-likelihood evaluation of self-exciting processes with three infection kernels --- exponential, power-law and Tsallis Q-exponential. We show that the modeling performance of the infection kernels varies with respect to the temporal structures of diffusions, and also with respect to user behavior, such as the likelihood of being bots. We further improve the prediction of popularity by combining two models that are identified as complementary by the goodness-of-fit tests.},
    url_Paper = {https://arxiv.org/abs/1910.05451},
    url_Code = {https://github.com/qykong/generalized-sir-and-hawkes},
    url = {https://doi.org/10.1145/3336191.3371821}
}

@inproceedings{wu2019estimating,
    address = {Austin, TX, USA},
    author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW '19)},
    title = {Estimating Attention Flow in Online Video Networks},
    year = {2019},
    abstract = {Online videos have shown tremendous increase in Internet traffic. Most video hosting sites implement recommender systems, which connect the videos into a directed network and conceptually act as a source of pathways for users to navigate. At present, little is known about how human attention is allocated over such large-scale networks, and about the impacts of the recommender systems. In this paper, we first construct the Vevo network -- a YouTube video network with 60,740 music videos interconnected by the recommendation links, and we collect their associated viewing dynamics. This results in a total of 310 million views every day over a period of 9 weeks. Next, we present large-scale measurements that connect the structure of the recommendation network and the video attention dynamics. We use the bow-tie structure to characterize the Vevo network and we find that its core component (23.1% of the videos), which occupies most of the attention (82.6% of the views), is made out of videos that are mainly recommended among themselves. This is indicative of the links between video recommendation and the inequality of attention allocation. Finally, we address the task of estimating the attention flow in the video recommendation network. We propose a model that accounts for the network effects for predicting video popularity, and we show it consistently outperforms the baselines. This model also identifies a group of artists gaining attention because of the recommendation network. Altogether, our observations and our models provide a new set of tools to better understand the impacts of recommender systems on collective social attention.},
    url = {https://dl.acm.org/doi/10.1145/3359285},
    url_Abstract = {https://arxiv.org/abs/1908.07123},
    url_Paper = {https://arxiv.org/pdf/1908.07123.pdf},
    url_Data = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TORICY},
    url_Code = {https://github.com/avalanchesiqi/networked-popularity},
    url_Slides = {http://users.cecs.anu.edu.au/~siqi.wu/files/cscw2019slides.pdf},
    url_Blog = {https://medium.com/acm-cscw/how-does-the-network-of-youtube-music-videos-drive-attention-42130144b59b}
}

@inproceedings{zhang2019efficient,
    author    = {Rui Zhang and Christian Walder and Marian-Andrei Rizoiu and Lexing Xie},
    title     = {Efﬁcient Non-parametric Bayesian Hawkes Processes},
    booktitle={The 28th International Joint Conference on Artificial Intelligence (IJCAI-19)},
    year={2019},
    address = {Macau, China},
    abstract = {In this paper, we develop an efﬁcient non-parametric Bayesian estimation of the kernel function of Hawkes processes. The non-parametric Bayesian approach is important because it provides ﬂexible Hawkes kernels and quantiﬁes their uncertainty. Our method is based on the cluster representation of Hawkes processes. Utilizing the stationarity of the Hawkes process, we efﬁciently sample random branching structures and thus, we split the Hawkes process into clusters of Poisson processes. We derive two algorithms — a block Gibbs sampler and a maximum a posteriori estimator based on expectation maximization — and we show that our methods have a linear time complexity, both theoretically and empirically. On synthetic data, we show our methods to be able to infer ﬂexible Hawkes triggering kernels. On two large scale Twitter diffusion datasets, we show that our methods outperform the current state-of-the-art in goodness-of-ﬁt and that the time complexity is linear in the size of the dataset. We also observe that on diffusions related to online videos, the learned kernels reﬂect the perceived longevity for different content types such as music or pets videos.},
    url_Paper = {https://arxiv.org/abs/1810.03730},
    url_Code = {https://github.com/RuiZhang2016/Efficient-Nonparametric-Bayesian-Hawkes-Processes}
}

@Article{Kim2019,
    author={Kim, Dongwoo and Graham, Timothy and Wan, Zimin and Rizoiu, Marian-Andrei},
    title={Analysing user identity via time-sensitive semantic edit distance (t-SED): a case study of Russian trolls on Twitter},
    journal={Journal of Computational Social Science},
    year={2019},
    month={Jul},
    day={01},
    volume={2},
    number={2},
    pages={331--351},
    abstract={In the digital era, individuals are increasingly profiled and grouped based on the traces that they leave behind in online social networks such as Twitter and Facebook. In this paper, we develop and evaluate a novel text analysis approach for studying user identity and social roles by redefining identity as a sequence of timestamped items (e.g., tweet texts). We operationalise this idea by developing a novel text distance metric, the time-sensitive semantic edit distance (t-SED), which accounts for the temporal context across multiple traces. To evaluate this method, we undertake a case study of Russian online-troll activity within US political discourse. The novel metric allows us to classify the social roles of trolls based on their traces, in this case tweets, into one of the predefined categories left-leaning, right-leaning, and news feed. We show the effectiveness of the t-SED metric to measure the similarities between tweets while accounting for the temporal context, and we use novel data visualisation techniques and qualitative analysis to uncover new empirical insights into Russian troll activity that have not been identified in the previous work. In addition, we highlight a connection with the field of actor--network theory and the related hypotheses of Gabriel Tarde, and we discuss how social sequence analysis using t-SED may provide new avenues for tackling a longstanding problem in social theory: how to analyse society without separating reality into micro vs. macro-levels.},
    issn={2432-2725},
    doi={10.1007/s42001-019-00051-x},
    url={https://doi.org/10.1007/s42001-019-00051-x},
    url_paper = {https://arxiv.org/pdf/1901.05228.pdf}
}

@inproceedings{kong2019linking,
    address = {Melbourne, VIC, Australia},
    author = {Kong, Quyu},
    booktitle = {ACM International Conference on Web Search and Data Mining (WSDM '19), Doctoral Consortium},
    title = {{Linking Epidemic Models and Hawkes Point Processes for Modeling Information Diffusion}},
    year = {2019},
    abstract = {Epidemic models and Hawkes point process models are two common model classes for information diffusion. Recent work has revealed the equivalence between the two for information diffusion modeling. This allows tools created for one class of models to be applied to another. However, epidemic models and Hawkes point processes can be connected in more ways. This thesis aims to develop a rich set of mathematical equivalences and extensions, and use them to ask and answer questions in social media and beyond. Specifically, we show our plan of generalizing the equivalence of the two model classes by extending it to Hawkes point process models with arbitrary memory kernels. We then outline a rich set of quantities describing diffusion, including diffusion size and extinction probability, introduced in the fields where the models are originally designed. Lastly, we discuss some novel applications of these quantities in a range of problems such as popularity prediction and popularity intervention.},
    url_Paper = {http://cm.cecs.anu.edu.au/documents/kong_wsdm2019_dc.pdf}
}

@inproceedings{Rizoiu2018a,
    abstract = {Serious concerns have been raised about the role of `socialbots' in manipulating public opinion and influencing the outcome of elections by retweeting partisan content to increase its reach. Here we analyze the role and influence of socialbots on Twitter by determining how they contribute to retweet diffusions. We collect a large dataset of tweets during the 1st U.S. presidential debate in 2016 and we analyze its 1.5 million users from three perspectives: user influence, political behavior (partisanship and engagement) and botness. First, we define a measure of user influence based on the user's active contributions to information diffusions, i.e. their tweets and retweets. Given that Twitter does not expose the retweet structure -- it associates all retweets with the original tweet -- we model the latent diffusion structure using only tweet time and user features, and we implement a scalable novel approach to estimate influence over all possible unfoldings. Next, we use partisan hashtag analysis to quantify user political polarization and engagement. Finally, we use the BotOrNot API to measure user botness (the likelihood of being a bot). We build a two-dimensional "polarization map" that allows for a nuanced analysis of the interplay between botness, partisanship and influence. We find that not only are socialbots more active on Twitter -- starting more retweet cascades and retweeting more -- but they are 2.5 times more influential than humans, and more politically engaged. Moreover, pro-Republican bots are both more influential and more politically engaged than their pro-Democrat counterparts. However we caution against blanket statements that software designed to appear human dominates politics-related activity on Twitter. Firstly, it is known that accounts controlled by teams of humans (e.g. organizational accounts) are often identified as bots. Secondly, we find that many highly influential Twitter users are in fact pro-Democrat and that most pro-Republican users are mid-influential and likely to be human (low botness).},
    address = {Stanford, CA, USA},
    author = {Rizoiu, Marian-Andrei and Graham, Timothy and Zhang, Rui and Zhang, Yifei and Ackland, Robert and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '18)},
    title = {{{\#}DebateNight: The Role and Influence of Socialbots on Twitter During the 1st 2016 U.S. Presidential Debate}},
    year = {2018},
    url_Abstract = {https://arxiv.org/abs/1802.09808},
    url_Paper = {https://arxiv.org/pdf/1802.09808.pdf},
    url_Code = {https://github.com/computationalmedia/cascade-influence},
    url_Slides = {http://www.rizoiu.eu/documents/research/presentations/RIZOIU_ICWSM-2018_slides.pdf}
}

@inproceedings{wu2018beyond,
    address = {Stanford, CA, USA},
    author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '18)},
    title = {Beyond Views: Measuring and Predicting Engagement in Online Videos},
    year = {2018},
    abstract = {The share of videos in the internet traffic has been growing, therefore understanding how videos capture attention on a global scale is also of growing importance. Most current research focus on modeling the number of views, but we argue that video engagement, or time spent watching is a more appropriate measure for resource allocation problems in attention, networking, and promotion activities. In this paper, we present a first large-scale measurement of video-level aggregate engagement from publicly available data streams, on a collection of 5.3 million YouTube videos published over two months in 2016. We study a set of metrics including time and the average percentage of a video watched. We define a new metric, relative engagement, that is calibrated against video properties and strongly correlate with recognized notions of quality. Moreover, we find that engagement measures of a video are stable over time, thus separating the concerns for modeling engagement and those for popularity -- the latter is known to be unstable over time and driven by external promotions. We also find engagement metrics predictable from a cold-start setup, having most of its variance explained by video context, topics and channel information -- R2=0.77. Our observations imply several prospective uses of engagement metrics -- choosing engaging topics for video production, or promoting engaging videos in recommender systems.},
    url_Abstract = {https://arxiv.org/abs/1709.02541},
    url_Paper = {https://arxiv.org/pdf/1709.02541.pdf},
    url_Code = {https://github.com/avalanchesiqi/youtube-engagement},
    url_Slides = {http://cm.cecs.anu.edu.au/documents/wu_icwsm2018_slides.pdf}
}

@inproceedings{Mishra2018rnn-mas,
    address = {Stanford, CA, USA},
    author = {Mishra, Swapnil and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '18)},
    pages = {1--10},
    title = {{Modeling Popularity in Asynchronous Social Media Streams with Recurrent Neural Networks}},
    year = {2018},
    abstract = {Understanding and predicting the popularity of online items is an important open problem in social media analysis. Considerable progress has been made recently in data-driven predictions, and in linking popularity to external promotions. However, the existing methods typically focus on a single source of external influence, whereas for many types of online content such as YouTube videos or news articles, attention is driven by multiple heterogeneous sources simultaneously - e.g. microblogs or traditional media coverage. Here, we propose RNN-MAS, a recurrent neural network for modeling asynchronous streams. It is a sequence generator that connects multiple streams of different granularity via joint inference. We show RNN-MAS not only to outperform the current state-of-the-art Youtube popularity prediction system by 17%, but also to capture complex dynamics, such as seasonal trends of unseen influence. We define two new metrics: promotion score quantifies the gain in popularity from one unit of promotion for a Youtube video; the loudness level captures the effects of a particular user tweeting about the video. We use the loudness level to compare the effects of a video being promoted by a single highly-followed user (in the top 1% most followed users) against being promoted by a group of mid-followed users. We find that results depend on the type of content being promoted: superusers are more successful in promoting Howto and Gaming videos, whereas the cohort of regular users are more influential for Activism videos. This work provides more accurate and explainable popularity predictions, as well as computational tools for content producers and marketers to allocate resources for promotion campaigns.},
    url_Abstract = {https://arxiv.org/abs/1804.02101},
    url_Paper = {https://arxiv.org/pdf/1804.02101.pdf},
    url_Code = {https://github.com/computationalmedia/rnn-mas}
}

@incollection{Rizoiu2018HPE,
    author = {Rizoiu, Marian-Andrei and Lee, Young and Mishra, Swapnil and Xie, Lexing},
    title = {{A Tutorial on Hawkes Processes for Events in Social Media}},
    booktitle = {Frontiers of Multimedia Research},
    editor = {Chang, Shih-Fu},
    year = {2018},
    isbn = {978-1-97000-107-5},
    pages = {191--218},
    numpages = {28},
    abstract = {This chapter provides an accessible introduction for point processes, and especially Hawkes processes, for modeling discrete, inter-dependent events over continuous time. We start by reviewing the definitions and the key concepts in point processes. We then introduce the Hawkes process, its event intensity function, as well as schemes for event simulation and parameter estimation. We also describe a practical example drawn from social media data - we show how to model retweet cascades using a Hawkes self-exciting process. We presents a design of the memory kernel, and results on estimating parameters and predicting popularity. The code and sample event data are available as an online appendix.},
    url = {https://doi.org/10.1145/3122865.3122874},
    url_Abstract = {https://arxiv.org/abs/1708.06401},
    url_Paper = {https://arxiv.org/pdf/1708.06401.pdf},
    url_Code= {https://github.com/s-mishra/featuredriven-hawkes},
    doi = {10.1145/3122865.3122874},
    acmid = {3122874},
    publisher = {Association for Computing Machinery and Morgan \& Claypool},
    address = {New York, NY, USA}
}

@inproceedings{rizoiu2018sir,
    title={{SIR-Hawkes}: Linking Epidemic Models and {Hawkes} Processes to Model Diffusions in Finite Populations},
    author={Rizoiu, Marian-Andrei and Mishra, Swapnil and Kong, Quyu and Carman, Mark and Xie, Lexing},
    address = {Lyon, France},
    booktitle = {Proceedings of the 2018 World Wide Web Conference},
    series = {WWW '18},
    year={2018},
    abstract = {Among the statistical tools for online information diffusion modeling, both epidemic models and Hawkes point processes are popular choices. The former originate from epidemiology, and consider information as a viral contagion which spreads into a population of online users. The latter have roots in geophysics and finance, view individual actions as discrete events in continuous time, and modulate the rate of events according to the self-exciting nature of event sequences. Here, we establish a novel connection between these two frameworks. Namely, the rate of events in an extended Hawkes model is identical to the rate of new infections in the Susceptible-Infected-Recovered (SIR) model after marginalizing out recovery events -- which are unobserved in a Hawkes process. This result paves the way to apply tools developed for SIR to Hawkes, and vice versa. It also leads to HawkesN, a generalization of the Hawkes model which accounts for a finite population size. Finally, we derive the distribution of cascade sizes for HawkesN, inspired by methods in stochastic SIR. Such distributions provide nuanced explanations to the general unpredictability of popularity: the distribution for diffusion cascade sizes tends to have two modes, one corresponding to large cascade sizes and another one around zero.},
    doi = {10.1145/3178876.3186108},
    eprint = {1711.01679},
    isbn = {9781450356398},
    pages = {419--428},
    url_Abstract = {https://arxiv.org/abs/1711.01679},
    url_Paper = {https://arxiv.org/pdf/1711.01679.pdf},
    url_Code = {https://github.com/computationalmedia/sir-hawkes},
    url_Slides = {http://www.rizoiu.eu/documents/research/presentations/RIZOIU_WWW-2018_slides.pdf}
}

@inproceedings{kong2018will,
    title={Will This Video Go Viral? Explaining and Predicting the Popularity of Youtube Videos},
    author={Kong, Quyu and Rizoiu, Marian-Andrei and Wu, Siqi and Xie, Lexing},
    address = {Lyon, France},
    booktitle = {Companion Proceedings of the The Web Conference 2018 - Demos},
    series = {WWW '18},
    year={2018},
    abstract = {What makes content go viral? Which videos become popular and why others don't? Such questions have elicited significant attention from both researchers and industry, particularly in the context of online media. A range of models have been recently proposed to explain and predict popularity; however, there is a short supply of practical tools, accessible for regular users, that leverage these theoretical results. Hipie -- an interactive visualization system -- is created to fill this gap, by enabling users to reason about the virality and the popularity of online videos. It retrieves the metadata and the past popularity series of Youtube videos, it employs the Hawkes Intensity Process, a state-of-the-art online popularity model for explaining and predicting video popularity, and it presents videos comparatively in a series of interactive plots. This system will help both content consumers and content producers in a range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and to predict future popularity, to identify viral videos, and to estimate responses to online promotion. },
    doi = {10.1145/3184558.3186972},
    url_Abstract = {https://arxiv.org/abs/1801.04117},
    url_Paper = {https://arxiv.org/pdf/1801.04117.pdf},
    url_Code = {https://github.com/computationalmedia/hipie}
}

@inproceedings{Rizoiu2017HIP,
    address = {Perth, Australia},
    author = {Rizoiu, Marian-Andrei and Xie, Lexing and Sanner, Scott and Cebrian, Manuel and Yu, Honglin and {Van Hentenryck}, Pascal},
    booktitle = {World Wide Web 2017, International Conference on},
    pages = {735--744},
    title = {Expecting to be {HIP}: Hawkes Intensity Processes for Social Media Popularity},
    year = {2017},
    doi = {10.1145/3038912.3052650},
    isbn = {9781450349130},
    abstract = {Modeling and predicting the popularity of online content is a significant problem for the practice of information dissemination, advertising, and consumption. Recent work analyzing massive datasets advances our understanding of popularity, but one major gap remains: To precisely quantify the relationship between the popularity of an online item and the external promotions it receives. This work supplies the missing link between exogenous inputs from public social media platforms, such as Twitter, and endogenous responses within the content platform, such as YouTube. We develop a novel mathematical model, the Hawkes intensity process, which can explain the complex popularity history of each video according to its type of content, network of diffusion, and sensitivity to promotion. Our model supplies a prototypical description of videos, called an endo-exo map. This map explains popularity as the result of an extrinsic factor -- the amount of promotions from the outside world that the video receives, acting upon two intrinsic factors -- sensitivity to promotion, and inherent virality. We use this model to forecast future popularity given promotions on a large 5-months feed of the most-tweeted videos, and found it to lower the average error by 28.6% from approaches based on popularity history. Finally, we can identify videos that have a high potential to become viral, as well as those for which promotions will have hardly any effect.},
    url_Abstract={https://arxiv.org/abs/1602.06033},
    url_Paper = {https://arxiv.org/pdf/1602.06033},
    url_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_WWW-2017_slides.pdf},
    url_code = {https://github.com/andrei-rizoiu/hip-popularity}
}

@inproceedings{Rizoiu2017promo,
    address = {Montreal, Canada},
    author = {Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {The International AAAI Conference on Web and Social Media (ICWSM)},
    pages = {182--191},
    title = {Online Popularity under Promotion: Viral Potential, Forecasting, and the Economics of Time},
    year = {2017},
    abstract = {Modeling the popularity dynamics of an online item is an important open problem in computational social science. This paper presents an in-depth study of popularity dynamics under external promotions, especially in predicting popularity jumps of online videos, and determining effective and efficient schedules to promote online content. The recently-proposed Hawkes Intensity Process (HIP) models popularity as a non-linear interplay between exogenous stimuli and the endogenous reaction. We propose two novel metrics based on HIP: to describe popularity gain per unit of promotion, and to quantify the time it takes for such effects to unfold. We make increasingly accurate forecasts of future popularity by including information about the intrinsic properties of the video, promotions it receives, and the non-linear effects of popularity ranking. We illustrate by simulation the interplay between the unfolding of popularity over time, and the time-sensitive value of resources. Lastly, our model lends a novel explanation of the commonly adopted periodic and constant promotion strategy in advertising, as increasing the perceived viral potential. This study provides quantitative guidelines about setting promotion schedules considering content virality, timing, and economics.},
    url_Abstract={https://arxiv.org/abs/1703.01012},
    url_Paper = {https://arxiv.org/pdf/1703.01012},
    url_code = {https://github.com/andrei-rizoiu/hip-popularity},
    url_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_ICWSM-2017_slides.pdf}
}

@article{Graham2017,
    abstract = {Despite its wide-ranging influence in social science, the field of actor-network theory (ANT) has proven difficult to operationalise quantitatively. Although social network analysis (SNA) and ANT appear to share certain affinities (e.g., the term ‘network'), attempts to develop an ANT approach to SNA (and vice versa) have stumbled upon fundamental problems or ‘discontinuities' between them (Venturini, Munk, and Jacomy, 2016). These problems constitute serious obstacles for progressing ANT research using digital data. In this paper, we propose hypergraphs as one way forward to operationalising ANT. Broadly, we term this method ‘social hypergraph analysis' (SHA). We outline SHA in this paper and apply it to analyse social media data, using a case study of the anti-vaccination debate on Twitter.},
    author = {Graham, Timothy and Ackland, Robert and Rizoiu, Marian-Andrei and Swift, Ben},
    journal = {67th Annual Conference of the International Communication Association},
    title = {{"Social Hypergraph Analysis": Towards an operationalisation of Actor-Network Theory Using hypergraphs}},
    year = {2017},
    url_Paper = {https://eprints.qut.edu.au/127948/1/127948.pdf}
}

@inproceedings{Mishra2016,
    title = {{Feature Driven and Point Process Approaches for Popularity Prediction}},
    author = {Mishra, Swapnil and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {Proceedings of the 25th ACM International Conference on Information and Knowledge Management},
    series = {CIKM '16},
    address = {Indianapolis, IN, USA},
    doi = {10.1145/2983323.2983812},
    keywords = {social media; self-exciting point process; information diffusion; cascade prediction},
    year = {2016},
    abstract = {Predicting popularity, or the total volume of information outbreaks, is an important subproblem for understanding collective behavior in networks. Each of the two main types of recent approaches to the problem, feature-driven and generative models, have desired qualities and clear limitations. This paper bridges the gap between these solutions with a new hybrid approach and a new performance benchmark. We model each social cascade with a marked Hawkes self-exciting point process, and estimate the content virality, memory decay, and user influence. We then learn a predictive layer for popularity prediction using a collection of cascade history. To our surprise, Hawkes process with a predictive overlay outperform recent feature-driven and generative approaches on existing tweet data [44] and a new public benchmark on news tweets. We also found that a basic set of user features and event time summary statistics performs competitively in both classification and regression tasks, and that adding point process information to the feature set further improves predictions. From these observations, we argue that future work on popularity prediction should compare across feature-driven and generative modeling approaches in both classification and regression tasks.},
    url_Abstract={https://arxiv.org/abs/1608.04862},
    url_Paper = {https://arxiv.org/pdf/1608.04862.pdf},
    url_Presentation_Page = {http://cm.cecs.anu.edu.au/post/fdhawkesforpopularity/},
    url_Slides = {http://cm.cecs.anu.edu.au/documents/smishra_cikm16_presentation.pdf},
    url_code = {https://git.io/v6rIN}
}

@inproceedings{Rizoiu2016,
    abstract = {The cumulative effect of collective participation online has an important and adverse impact on individual privacy. As an online system evolves over time, new digital traces of individual behavior may uncover previously hidden statistical links between an individual’s past actions and her private traits. Furthermore, this de-anonymization trend may not be observable when analyzing short or medium time-span snapshots of data. To quantify this effect, we analyze the evolution of individual privacy loss by studying the 13-year long edit history of Wikipedia, including more than 117,523 different users performing 188,805,088 edits. We trace each Wikipedia’s contributor using apparently harmless features, such as the number of edits performed on predefined broad categories in a given time period (e.g. Mathematics, Culture or Nature). We show that even at this unspecific level of identification, it is possible to use off-the-shelf machine learning algorithms to uncover usually undisclosed private traits, such as gender, religion or education. We provide empirical evidence that the prediction accuracy for almost all private traits consistently improves over time. Moreover, we observe that the system also shows improved prediction for users who participated in the system during “safe” periods — periods where a given individual’s private traits could not be — showing that de-anonymization threats are hard to foresee as online systems evolve. Insights from this work should help users, system designers, and policy makers understand and debate the design and long-term effects of online content creation systems.},
    address = {San Francisco, CA, USA},
    author = {Rizoiu, Marian-Andrei and Xie, Lexing and Caetano, Tiberio and Cebrian, Manuel},
    booktitle = {Proceedings of the 9th ACM International Conference on Web Search and Data Mining},
    series = {WSDM '16},
    doi = {10.1145/2835776.2835798},
    keywords = {de-anonymization,online privacy,temporal loss of privacy},
    title = {{Evolution of Privacy Loss on Wikipedia}},
    year = {2016},
    url_Slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_WSDM-2016_slides.pdf}, % use this to distribute the slides of the presentation
    %url_Link = {http://my.repo.com/awesome.paper.pdf} % use this to distribute the link to your paper (e.g. journal entry)
    url_Paper = {http://arxiv.org/pdf/1512.03523.pdf}, % use this to distribute the link ot the paper. It can be either a http:// link or a local link relative to the bib file (somthing like publications/paper.pdf)
    url_Presentation_Page = {http://cm.cecs.anu.edu.au/post/wikiprivacy/}
}

@inproceedings{Mihaita2019,
    abstract = {Congestion prediction represents a major priority for traffic management centres around the world to ensure timely incident response handling. The increasing amounts of generated traffic data have been used to train machine learning predictors for traffic, however this is a challenging task due to inter-dependencies of traffic flow both in time and space. Recently, deep learning techniques have shown significant prediction improvements over traditional models, however open questions remain around their applicability, accuracy and parameter tuning. This paper proposes an advanced deep learning framework for simultaneously predicting the traffic flow on a large number of monitoring stations along a highly circulated motorway in Sydney, Australia, including exit and entry loop count stations, and over varying training and prediction time horizons. The spatial and temporal features extracted from the 36.34 million data points are used in various deep learning architectures that exploit their spatial structure (convolutional neuronal networks), their temporal dynamics (recurrent neuronal networks), or both through a hybrid spatio-temporal modelling (CNN-LSTM). We show that our deep learning models consistently outperform traditional methods, and we conduct a comparative analysis of the optimal time horizon of historical data required to predict traffic flow at different time points in the future.},
    address = {Auckland, New Zealand},
    archivePrefix = {arXiv},
    arxivId = {1907.06356},
    author = {Mihaita, Adriana-Simona and Li, Haowen and He, Zongyang and Rizoiu, Marian-Andrei},
    booktitle = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
    doi = {10.1109/ITSC.2019.8916852},
    eprint = {1907.06356},
    URL_paper = {https://arxiv.org/pdf/1907.06356.pdf},
    isbn = {978-1-5386-7024-8},
    keywords = {,BPNN,CNN,Deep learning,LSTM,Motorway flow predicting,Short- versus long-term prediction},
    month = {oct},
    pages = {1683--1690},
    publisher = {IEEE},
    title = {{Motorway Traffic Flow Prediction using Advanced Deep Learning}},
    url = {https://ieeexplore.ieee.org/document/8916852/},
    year = {2019}
}
@article{Kern2020,
    abstract = {Work is thought to be more enjoyable and beneficial to individuals and society when there is congruence between one's personality and one's occupation. We provide large-scale evidence that occupations have distinctive psychological profiles, which can successfully be predicted from linguistic information unobtrusively collected through social media. Based on 128,279 Twitter users representing 3,513 occupations, we automatically assess user personalities and visually map the personality profiles of different professions. Similar occupations cluster together, pointing to specific sets of jobs that one might be well suited for. Observations that contradict existing classifications may point to emerging occupations relevant to the 21st century workplace. Findings illustrate how social media can be used to match people to their ideal occupation.},
    author = {Kern, Margaret L. and McCarthy, Paul X. and Chakrabarty, Deepanjan and Rizoiu, Marian-Andrei},
    doi = {10.1073/pnas.1917942116},
    URL_paper = {https://www.pnas.org/content/pnas/116/52/26459.full.pdf},
    issn = {0027-8424},
    journal = {Proceedings of the National Academy of Sciences},
    month = {dec},
    number = {52},
    pages = {26459--26464},
    title = {{Social media-predicted personality traits and values can help match people to their ideal jobs}},
    url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1917942116},
    volume = {116},
    year = {2019},
    URL_code = {https://github.com/behavioral-ds/VocationMap}
}
@inproceedings{Mihaita2019a,
    abstract = {Predicting traffic incident duration is a major challenge for many traffic centres around the world. Most research studies focus on predicting the incident duration on motorways rather than arterial roads, due to a high network complexity and lack of data. In this paper we propose a bi-level framework for predicting the accident duration on arterial road networks in Sydney, based on operational requirements of incident clearance target which is less than 45 minutes. Using incident baseline information, we first deploy a classification method using various ensemble tree models in order to predict whether a new incident will be cleared in less than 45min or not. If the incident was classified as short-term, then various regression models are developed for predicting the actual incident duration in minutes by incorporating various traffic flow features. After outlier removal and intensive model hyper-parameter tuning through randomized search and cross-validation, we show that the extreme gradient boost approach outperformed all models, including the gradient-boosted decision-trees by almost 53{\%}. Finally, we perform a feature importance evaluation for incident duration prediction and show that the best prediction results are obtained when leveraging the real-time traffic flow in vicinity road sections to the reported accident location.},
    address = {Singapore},
    archivePrefix = {arXiv},
    arxivId = {1905.12254},
    author = {Mihaita, Adriana-Simona and Liu, Zheyuan and Cai, Chen and Rizoiu, Marian-Andrei},
    booktitle = {Proceedings of the 26th ITS World Congress},
    eprint = {1905.12254},
    URL_paper = {http://arxiv.org/pdf/1905.12254.pdf},
    month = {may},
    pages = {1--12},
    title = {{Arterial incident duration prediction using a bi-level framework of extreme gradient-tree boosting}},
    url = {http://arxiv.org/abs/1905.12254},
    year = {2019}
}
@inproceedings{Ram2019,
    abstract = {Social influence has become an incredibly powerful tool with the advent of so-cial media; as such, measuring and understanding it has become an increasingly importantendeavor.   Understanding  and  measuring  online  social  influence  are  essential  for  sociolo-gists  studying  modern  events,  such  as  understanding  how  ‘influencers’  promote  productseffectively,  how Queenslanders swung the last Australian election or how anti-vaccinationsentiment spreads.  Social influence has been studied thoroughly in both the computationaland  social  science  contexts,  however  there  is  often  a  disconnect  between  the  two  fields.Computational techniques are often not grounded in social theory, nor are they validatedby humans.  Conversely, social scientists rarely exploit the vast amounts of data from socialmedia and they lack the tractable computational models to quantify influence efficiently.},
    address = {Adelaide, Australia},
    author = {Ram, Rohit and Rizoiu, Marian-Andrei},
    booktitle = {Australian Social Network Analysis Conference (ASNAC'19)},
    URL_paper = {https://opus.lib.uts.edu.au/bitstream/10453/137931/1/main.pdf},
    pages = {2},
    title = {{A social science-grounded approach for quantifying online social influence}},
    year = {2019}
}

@phdthesis{Mardale2019,
    abstract = {Internet has met an unprecedented growth of popularity in the last years, thus it is not surprising that many aspects of our lives have been influenced by the improvements of technology. The way information is spreading nowadays is vastly driven by the devel- opment of online social networks. Hundreds of millions of Internet users have access to novel information and various points of view [2], which they can share themselves, leading to emerging online communities. The diffusion of information in these communities has been often the subject of numerous studies as it turned out that the outcome of major events such as the 2008 presidential elections in the United States of America [22] or the decision of leaving the European Union made by English people [21], was influenced by the transfer of information through such kind of networks. It is of particular importance to understand the intrinsic mechanisms of such social net- works, in order to efficiently detect possible changes in the users' attitudes around the different problems they are debating. By aggregating textual information that defines users with the dynamics of the diffusions they are part of, the goal is to predict future stances of the individuals when they interact in the Social Network with their peers. This is of particular usefulness, because it provides a hint of how people change their stances after having interacted with other people, thus leading a to a better understanding of the networks.},
    author = {Mardale, Andrei},
    pages = {34},
    school = {Universit\'e Jean Monnet},
    title = {{Information Diffusion in Online Communities}},
    year = {2019},
    URL_paper = {../authors/andrei-mardale/andrei-mardale-thesis.pdf},
}

@phdthesis{Tripathi2021,
    abstract = {In the era of heavy social media reliance, there is a growing concern about the num- ber of potential vulnerabilities that we are being exposed to but the most detrimental of them all are the ones that operate silently without a hint of detection. One such threat is mis-/dis-information – misinformation being the spread of in-factual in- formation online and disinformation being the deliberate or intentional spread of inaccurate information. Current literature, while vast, does little to offer a concrete approach with which to tackle this problem. Additionally, there is a glaring gap in our current understanding of typical characteristics of a mis-/dis-information cam- paign. In this thesis, we investigate a widely-debated instance of mis-/dis-information cam- paign during the 2019 Australian elections. We use the 2019 #auspol Twitter data and analyse it using our novel four-tiered analytical approach that we introduce in this work. This approach constitutes of looking at the data at four different levels: content-level, user-level, network-level and diffusion-level. We use the results ob- tained from this analysis to report on distinguishing features of our chosen instance of a mis-/dis-information campaign. At the content level, we find that the result of using sentiment analysis on tweets yields a higher objectivity score and higher neutral sentiment scores for the cluster which perpetuates the mis-/dis-information. Using this, we derive that the users that belong to the cluster which perpetuates the mis-/dis-information tend to have deliver their message more objectively, and with little trace of positive or negative emotion, to adopt a false sense of factuality. At the user level, we find that 63% of all inactive or suspended accounts belong to users that spread mis-/dis-information. We use this result to infer that a higher proportion of suspended or inactive accounts is a distinguishing feature of mis-/dis-information cluster. At the network level, we find that cluster of users that spread mis-/dis-information have higher centrality measure. From this, we find strongly connected networks with a high level of inter-connectivity to be a defining characteristic of the mis-/dis- information campaign. Lastly, investigation into retweeting behaviour reveals that misinformation cluster users have much faster as well as slower retweeting times compared to other users. We find that this twofold retweeting behaviour is inorganic in nature and is an indication of an organised effort at spreading disinformation.},
    author = {Tripathi, Kriti},
    pages = {85},
    school = {Australian National University},
    title = {{Profiling Information Warfare on Social Media: A Forensic Analysis of the 2019 Australian Elections}},
    year = {2021},
    URL_paper = {../authors/kriti-tripathi/kriti-tripathi-thesis.pdf},
}


@inproceedings{Dawson2019,
    abstract = {Labour demand and skill shortages have historically been difficult to assess given the high costs of conducting representative surveys and the inherent delays of these indicators. This is particularly consequential for fast developing skills and occupations, such as those relating to Data Science and Analytics (DSA). This paper develops a data-driven solution to detecting skill shortages from online job advertisements (ads) data. We first propose a method to generate sets of highly similar skills based on a set of seed skills from job ads. This provides researchers with a novel method to adaptively select occupations based on granular skills data. Next, we apply this adaptive skills similarity technique to a dataset of over 6.7 million Australian job ads in order to identify occupations with the highest proportions of DSA skills. This uncovers 306,577 DSA job ads across 23 occupational classes from 2012-2019. Finally, we propose five variables for detecting skill shortages from online job ads: (1) posting frequency; (2) salary levels; (3) education requirements; (4) experience demands; and (5) job ad posting predictability. This contributes further evidence to the goal of detecting skills shortages in real-time. In conducting this analysis, we also find strong evidence of skills shortages in Australia for highly technical DSA skills and occupations. These results provide insights to Data Science researchers, educators, and policy-makers from other advanced economies about the types of skills that should be cultivated to meet growing DSA labour demands in the future.},
    address = {Los Angeles, CA, USA},
    archivePrefix = {arXiv},
    arxivId = {1911.02302},
    author = {Dawson, Nikolas and Rizoiu, Marian-Andrei and Johnston, Benjamin and Williams, Mary-Anne},
    booktitle = {2019 IEEE International Conference on Big Data (Big Data)},
    doi = {10.1109/BigData47090.2019.9005967},
    eprint = {1911.02302},
    URL_paper = {http://arxiv.org/pdf/1911.02302.pdf},
    isbn = {978-1-7281-0858-2},
    keywords = {Big Data,Data Science,Labour Demand,Online Job Advertisements,Skill Shortages},
    month = {dec},
    pages = {1637--1643},
    publisher = {IEEE},
    title = {{Adaptively selecting occupations to detect skill shortages from online job ads}},
    url = {https://ieeexplore.ieee.org/document/9005967/},
    year = {2019}
}
@inproceedings{Zhang2020,
    abstract = {The Hawkes process has been widely applied to modeling self-exciting events, including neuron spikes, earthquakes and tweets. To avoid designing parametric kernel functions and to be able to quantify the prediction confidence, non-parametric Bayesian Hawkes processes have been proposed. However the inference of such models suffers from unscalability or slow convergence. In this paper, we first propose a new non-parametric Bayesian Hawkes process whose triggering kernel is modeled as a squared sparse Gaussian process. Second, we present the variational inference scheme for the model optimization, which has the advantage of linear time complexity by leveraging the stationarity of the triggering kernel. Third, we contribute a tighter lower bound than the evidence lower bound of the marginal likelihood for the model selection. Finally, we exploit synthetic data and large-scale social media data to validate the efficiency of our method and the practical utility of our approximate marginal likelihood. We show that our approach outperforms state-of-the-art non-parametric Bayesian and non-Bayesian methods.},
    address = {New York, New York, USA},
    annote = {(CoRE: A*, H5: , a.r.: )},
    archivePrefix = {arXiv},
    arxivId = {1905.10496},
    author = {Zhang, Rui and Walder, Christian and Rizoiu, Marian-Andrei},
    booktitle = {AAAI Conference on Artificial Intelligence (AAAI'20)},
    eprint = {1905.10496},
    URL_paper = {http://arxiv.org/pdf/1905.10496.pdf},
    month = {may},
    title = {{Variational Inference for Sparse Gaussian Process Modulated Hawkes Process}},
    url = {https://doi.org/10.1609/aaai.v34i04.6160},
    year = {2020}
}
@inproceedings{Zhang2020a,
    abstract = {Approximate inference techniques are a cornerstone of the study of Gaussian Processes. Despite this, most work approximately optimises divergence measures (Kullback-Leibler (KL), $\alpha$-divergence, etc.) which lack the basic desiderata for the task at hand, while chiefly offering merely technical convenience. We develop a new approximate inference method for the Gaussian process which overcomes the technical challenges of abandoning these convenient divergences. Our method — dubbed Quantile Propagation (QP) — is similar to expectation propagation (EP) but minimizes the L 2 Wasserstein distance (WD). The WD exhibits all the required properties of a distance metric, while respecting the geometry of the underlying sample space. We show that QP matches quantile functions rather than moments as in EP and has the same mean update but a smaller variance update than EP, thereby alleviating the over-estimation of the posterior variance exhibited by EP. Crucially, despite the significant complexity in even evaluating the WD, our QP has the same favorable locality property as EP, and thereby admits an efficient algorithm. Experiments on classification and Poisson regression tasks demonstrate that QP outperforms both EP and variational Bayes.},
    author = {Zhang, Rui and Walder, Christian and Bonilla, Edwin V. and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {Conference on Neural Information Processing Systems (NeurIPS'20)},
    title = {{Quantile Propagation for Wasserstein-Approximate Gaussian Processes}},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
    pages = {21566--21578},
    publisher = {Curran Associates, Inc.},
    volume = {33},
    year = {2020},
    url_paper = {https://papers.nips.cc/paper/2020/file/f5e62af885293cf4d511ceef31e61c80-Paper.pdf},
    url_code = {https://github.com/RuiZhang2016/Quantile-Propagation-for-Wasserstein-Approximate-Gaussian-Processes}
}
@inproceedings{Mihaita2020,
    abstract = {Traffic flow prediction, particularly in areas that experience highly dynamic flows such as motorways, is a major issue faced in traffic management. Due to increasingly large volumes of data sets being generated every minute, deep learning methods have been used extensively in the latest years for both short and long term prediction. However, such models, despite their efficiency, need large amounts of historical information to be provided, and they take a considerable amount of time and computing resources to train, validate and test. This paper presents two new spatial-temporal approaches for building accurate short-term prediction along a popular motorway in Sydney, by making use of the graph structure of the motorway network (including exits and entries). The methods are built on proximity-based approaches, denoted backtracking and interpolation, which uses the most recent and closest traffic flow information for each of the target counting stations along the motorway. The results indicate that for short-term predictions (less than 10 minutes into the future), the proposed graph-based approaches outperform state-of-the-art deep learning models, such as long-term short memory, convolutional neuronal networks or hybrid models.},
    address = {Rhodes, Greece},
    author = {Mihaita, Adriana-Simona and Papachatgis, Zac and Rizoiu, Marian-Andrei},
    booktitle = {23rd IEEE International Conference on Intelligent Transportation Systems (ITSC'20)},
    URL_paper = {https://arxiv.org/pdf/2006.14824},
    pages = {1--8},
    title = {{Graph modelling approaches for motorway traffic flow prediction}},
    year = {2020}
}

@inproceedings{Wu2020,
    abstract = {A comprehensive understanding of data bias is the cornerstone of mitigating biases in social media research. This paper presents in-depth measurements of the effects of Twitter data sampling across different timescales and different subjects (entities, networks, and cascades). By constructing two complete tweet streams, we show that Twitter rate limit message is an accurate measure for the volume of missing tweets. Despite sampling rates having clear temporal variations, we find that the Bernoulli process with a uniform rate well approximates Twitter data sampling, and it allows to estimate the ground-truth entity frequency and ranking with the observed sample data. In terms of network analysis, we observe significant structure changes in both the user-hashtag bipartite graph and the retweet network. Finally, we measure the retweet cascades. We identify risks for information diffusion models that rely on tweet inter-arrival times and user influence. This work calls attention to the social data bias caused by data collection, and proposes methods to measure the systematic biases introduced by sampling.},
    archivePrefix = {arXiv},
    arxivId = {2003.09557},
    author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '20)},
    eprint = {2003.09557},
    month = {mar},
    pages = {1--10},
    title = {{Variation across Scales: Measurement Fidelity under Twitter Data Sampling}},
    URL_paper = {https://arxiv.org/pdf/2003.09557.pdf},
    URL_code = {https://github.com/avalanchesiqi/twitter-sampling},
    url = {https://ojs.aaai.org/index.php/ICWSM/article/view/7337},
    year = {2020}
}

@inproceedings{Dawson2020,
    abstract = {Skill shortages are a drain on society. They hamper economic opportunities for individuals, slow growth for firms, and impede labor productivity in aggregate. Therefore, the ability to understand and predict skill shortages in advance is critical for policy-makers and educators to help alleviate their adverse effects. This research implements a high-performing Machine Learning approach to predict occupational skill shortages. In addition, we demonstrate methods to analyze the underlying skill demands of occupations in shortage and the most important features for predicting skill shortages. For this work, we compile a unique dataset of both Labor Demand and Labor Supply occupational data in Australia from 2012 to 2018. This includes data from 7.7 million job advertisements (ads) and 20 official labor force measures. We use these data as explanatory variables and leverage the XGBoost classifier to predict yearly skills shortage classifications for 132 standardized occupations. The models we construct achieve macro-F1 average performance scores of up to 83 per cent. Our results show that job ads data and employment statistics were the highest performing feature sets for predicting year-to-year skills shortage changes for occupations. We also find that features such as `Hours Worked', years of `Education', years of `Experience', and median `Salary' are highly important features for predicting occupational skill shortages. This research provides a robust data-driven approach for predicting and analyzing skill shortages, which can assist policy-makers, educators, and businesses to prepare for the future of work.},
    archivePrefix = {arXiv},
    arxivId = {2004.01311},
    author = {Dawson, Nikolas and Rizoiu, Marian-Andrei and Johnston, Benjamin and Williams, Mary-Anne},
    booktitle = {Workshop on Human-in-the-Loop Methods and Future of Work in BigData (HMData'20)},
    eprint = {2004.01311},
    month = {dec},
    pages = {1--10},
    title = {{Predicting Skill Shortages in Labor Markets: A Machine Learning Approach}},
    URL_paper = {https://arxiv.org/pdf/2004.01311.pdf},
    year = {2020}
}

@inproceedings{Xu2022,
    abstract = {The rapid advances in automation technologies, such as artificial intelligence (AI) and robotics, pose an increasing risk of automation for occupations, with a likely significant impact on the labour market. Recent social-economic studies suggest that nearly 50% of occupations are at high risk of being automated in the next decade. However, the lack of granular data and empirically informed models have limited the accuracy of these studies and made it challenging to predict which jobs will be automated. In this paper, we study the automation risk of occupations by performing a classification task between automated and non-automated occupations. The available information is 910 occupations' task statements, skills and interactions categorised by Standard Occupational Classification (SOC). To fully utilize this information, we propose a graph-based semi-supervised classification method named Automated Occupation Classification based on Graph Convolutional Networks (AOC-GCN) to identify the automated risk for occupations. This model integrates a heterogeneous graph to capture occupations' local and global contexts. The results show that our proposed method outperforms the baseline models by considering the information of both internal features of occupations and their external interactions. This study could help policymakers identify potential automated occupations and support individuals' decision-making before entering the job market.},
    author = {Xu, Dawei and Yang, Haoran and Rizoiu, Marian-Andrei and Xu, Guandong},
    booktitle = {International Conference on Advanced Data Mining and Applications (ADMA 2022)},
    pages = {1--16},
    title = {{Being Automated or Not? Risk Identification of Occupations with Graph Neural Networks}},
    URL_paper = {https://arxiv.org/pdf/2209.02182.pdf},
    year = {2022}
}

@inproceedings{Ram2022,
    abstract = {Influence and information campaigns often make use of domestic groups with extreme views. At the state level, influence operations are best understood as a campaign: a suite of information actions deployed to mislead the public and influence decision-makers. This work builds social media data-driven ideology detection based on language usage in social media posts. Our solution is based on homophilic similarity. We assume that social media users who spend significant amounts of time surrounded by specific groups (like the far-right) will adopt their thinking and their narratives. We apply our approach to a dataset of Twitter posts concerning conspiracy theories about climate change, COVID-19 and vaccination. We train our model with 1496 accounts labelled as far-right. We use a cross-validation setup to test the generalisation error and find that we can accurately detect far-right posts. Our method is domain agnostic: we can quickly deploy it to a new discussion domain without requiring further data annotation. This can considerably reduce the time needed to deploy the solution and the required expert annotation time.},
    address = {Sydney, Australia},
    author = {Ram, Rohit and Rizoiu, Marian-Andrei},
    booktitle = {Defence Human Sciences Symposium},
    pages = {1--2},
    title = {{Data-driven ideology detection: a case study of far-right extremist}},
    url = {https://www.dst.defence.gov.au/event/defence-human-sciences-symposium-2022},
    URL_paper = {https://opus.lib.uts.edu.au/bitstream/10453/164081/2/DHSS2022-Abstract-Submission%20%28arXiv%29.pdf},
    URL_video = {https://www.youtube.com/watch?v=csWMgU7R52Q},
    year = {2022}
}

@article{Rizoiu2022,
    abstract = {Interval-censored data solely records the aggregated counts of events during specific time intervals – such as the number of patients admitted to the hospital or the volume of vehicles passing traffic loop detectors – and not the exact occurrence time of the events. It is currently not understood how to fit the Hawkes point processes to this kind of data. Its typical loss function (the point process log-likelihood) cannot be computed without exact event times. Furthermore, it does not have the independent increments property to use the Poisson likelihood. This work builds a novel point process, a set of tools, and approximations for fitting Hawkes processes within interval-censored data scenarios. First, we define the Mean Behavior Poisson process connect the interval-censored loss of MBPP to a broader class of Bregman divergence-based functions. Using the connection, we show that the popularity estimation algorithm Hawkes Intensity Process (HIP) (Rizoiu et al., 2017b) is a particular case of the MBPP. We verify our models through empirical testing on synthetic data and real-world data. We find that our MBPP outperforms HIP on real-world datasets for the task of popularity prediction. This work makes it possible to efficiently fit the Hawkes process to interval- censored data. (MBPP), a novel Poisson process with a direct parameter correspondence to the popular self-exciting Hawkes process. We fit MBPP in the interval-censored setting using an interval-censored Poisson log-likelihood (IC-LL). We use the parameter equivalence to uncover the parameters of the associated Hawkes process. Second, we introduce two novel exogenous functions to distinguish the exogenous from the endogenous events. We propose the multi-impulse exogenous function – for when the exogenous events are observed as event time – and the latent homogeneous Poisson process exogenous function – for when the exogenous events are presented as interval-censored volumes. Third, we provide several approximation methods to estimate the intensity and compensator function of MBPP when no analytical solution exists. Fourth and finally, we},
    archivePrefix = {arXiv},
    arxivId = {2104.07932},
    author = {Rizoiu, Marian-Andrei and Soen, Alexander and Li, Shidi and Calderon, Pio and Dong, Leanne and Menon, Aditya Krishna and Xie, Lexing},
    eprint = {2104.07932},
    journal = {Journal of Machine Learning Research},
    keywords = {Bregman,Hawkes process,Interval-censored,Mean Behavior Poisson process,Poisson process exogenous function,divergence,latent homogeneous,multi-impulse exogenous function,popularity prediction},
    number = {338},
    pages = {1--84},
    title = {{Interval-censored Hawkes processes}},
    url = {https://jmlr.org/papers/v23/21-0917.html},
    URL_paper = {https://jmlr.org/papers/volume23/21-0917/21-0917.pdf},
    volume = {23},
    year = {2022}
}

@phdthesis{CallumPastuszak2022,
    abstract = {Recent years have seen an increase in online disinformation, and the consequences of ineffectively combating it, such as the growth of the Q-Anon movement. Fact checking is a necessary part of combating disinformation, however many aspects of the practice are not well understood, and platforms have struggled to fight the spread of disinformation. This project presents a new methodological approach to investigating the quality and reliability of information of URLs involved in fact checking on Twitter, in order to better understand the role of fact checks in counteracting misinformation on social media. This study details: • New approach for classification based upon the content of the tweet • Creating a method to measure the quality of URLs, specifically addressing the reliability of their information • Identifying co-occurrences of different types of classified tweets, and analysis into their different interactions • Analysing veracity and source alignment},
    author = {{Callum Pastuszak}},
    keywords = {Australian politics,Fact checking,Misinformation,Social media,Twitter},
    pages = {1--54},
    school = {University of Technology Sydney},
    title = {{The role of fact checks in counteracting disinformation on social media}},
    type = {Honours},
    URL_paper = {../authors/callum-pastuszak/callum-pastuszak-thesis.pdf},
    year = {2022}
}

@article{Nurek2023,
    abstract = {Mobile phones contain a wealth of private information, so we try to keep them secure. We provide large-scale evidence that the psychological profiles of individuals and their relations with their peers can be predicted from seemingly anonymous communication traces -- calling and texting logs that service providers routinely collect. Based on two extensive longitudinal studies containing more than 900 college students, we use point process modeling to describe communication patterns. We automatically predict the peer relationship type and temporal dynamics, and assess user personality based on the modeling. For some personality traits, the results are comparable to the gold-standard performances obtained from survey self-report data. Findings illustrate how information usually residing outside the control of individuals can be used to reconstruct sensitive information.},
    archivePrefix = {arXiv},
    arxivId = {2009.02032},
    author = {Nurek, Mateusz and Michalski, Radoslaw and Lizardo, Omar and Rizoiu, Marian-Andrei},
    doi = {10.1109/ACCESS.2023.3238970},
    eprint = {2009.02032},
    issn = {2169-3536},
    journal = {IEEE Access},
    month = {sep},
    pages = {8492--8503},
    title = {{Predicting Relationship Labels and Individual Personality Traits From Telecommunication History in Social Networks Using Hawkes Processes}},
        url = {https://doi.org/10.1109/ACCESS.2023.3238970},
    URL_paper = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10024293},
    volume = {11},
    year = {2023}
}


@inproceedings{Kong2023,
    abstract = {Social media is being increasingly weaponized by state-backed actors to elicit reactions, push narratives and sway public opinion. These are known as Information Operations (IO). The covert nature of IO makes their detection difficult. This is further amplified by missing data due to the user and content removal and privacy requirements. This work advances the hypothesis that the very reactions that Information Operations seek to elicit within the target social systems can be used to detect them. We propose an Interval-censored Transformer Hawkes (IC-TH) architecture and a novel data encoding scheme to account for both observed and missing data. We derive a novel log-likelihood function that we deploy together with a contrastive learning procedure. We showcase the performance of IC-TH on three real-world Twitter datasets and two learning tasks: future popularity prediction and item category prediction. The latter is particularly significant. Using the retweeting timing and patterns solely, we can predict the category of YouTube videos, guess whether news publishers are reputable or controversial and, most importantly, identify state-backed IO agent accounts. Additional qualitative investigations uncover that the automatically discovered clusters of Russian-backed agents appear to coordinate their behavior, activating simultaneously to push specific narratives.},
    archivePrefix = {arXiv},
    arxivId = {2211.14114},
    author = {Kong, Quyu and Calderon, Pio and Ram, Rohit and Boichak, Olga and Rizoiu, Marian-Andrei},
    booktitle = {The Web Conference (WWW'23)},
    eprint = {2211.14114},
    month = {apr},
    pages = {1--9},
    doi = {10.1145/3543507.3583481},
    title = {{Interval-censored Transformer Hawkes: Detecting Information Operations using the Reaction of Social Systems}},
    url = {https://doi.org/10.1145/3543507.3583481},
    URL_paper = {http://arxiv.org/pdf/2211.14114.pdf},
    URL_slides = {../extra/KONG_WWW'23-slides.pdf},
    URL_video = {https://youtu.be/INso-LuqZPg},
    year = {2023}
}

@article{Bailo2023,
    abstract = {This paper focuses on the performance of the far-right community in the Australian Twittersphere during two information crises: the 2019-20 Australian bushfires and the early months of the 2020 COVID-19 pandemic. Using a mixed method approach to analysing the performance of far-right accounts active in both crises, and using an information disorder index to estimate the quality of information being shared on Twitter during the two events, we found that far-right accounts moved from the periphery of these disaster-driven conversations during the Australian bushfires to assume a more central location during the COVID-19 pandemic. We argue that an increase in information disorder and overperformance of far-right accounts during COVID-19 is suggestive of an association between the two, which warrants further investigation.},
    author = {Bailo, Francesco and Johns, Amelia and Rizoiu, Marian-Andrei},
    doi = {10.1080/1369118X.2023.2205479},
    issn = {1369-118X},
    journal = {Information, Communication {\&} Society},
    month = {apr},
    pages = {35},
    title = {{Riding information crises: The performance of far-right Twitter users in Australia during the 2019-20 bushfires and the COVID-19 pandemic}},
    url = {https://www.tandfonline.com/doi/full/10.1080/1369118X.2023.2205479},
    URL_paper = {https://www.tandfonline.com/doi/epdf/10.1080/1369118X.2023.2205479?needAccess=true&role=button},
    year = {2023}
}

@inproceedings{Galat2023,
    abstract = {Biomedical summarization requires large datasets to train for text generation. We show that while transfer learning offers a viable option for addressing this challenge, an in-domain pre-training does not always offer advantages in a BioASQ summarization task. We identify a suitable model architecture and use it to show a benefit of a general-domain pre-training followed by a task-specific fine-tuning in the context of a BioASQ summarization task, leading to a novel three-step fine-tuning approach that works with only a thousand in-domain examples. Our results indicate that a Large Language Model without domain-specific pre-training can have a significant edge in some domain-specific biomedical text generation tasks.},
    archivePrefix = {arXiv},
    arxivId = {2307.04412},
    author = {Galat, Dima and Rizoiu, Marian-Andrei},
    booktitle = {Conference and Labs of the Evaluation Forum (CLEF 2023)},
    eprint = {2307.04412},
    month = {jul},
    title = {{Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training}},
    url = {http://arxiv.org/abs/2307.04412},
    URL_paper = {http://arxiv.org/pdf/2307.04412.pdf},
    year = {2023}
}

@inproceedings{Calderon2024a,
    author = {Calderon, Pio and Ram, Rohit and Rizoiu, Marian-Andrei},
    booktitle = {The International AAAI Conference on Web and Social Media (ICWSM)},
    title = {Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions},
    year = {2024},
    abstract = {Online extremism has severe societal consequences, including normalizing hate speech, user radicalization, and increased social divisions. Various mitigation strategies have been explored to address these consequences. One such strategy uses positive interventions: controlled signals that add attention to the opinion ecosystem to boost certain opinions. To evaluate the effectiveness of positive interventions, we introduce the Opinion Market Model (OMM), a two-tier online opinion ecosystem model that considers both inter-opinion interactions and the role of positive interventions. The size of the opinion attention market is modeled in the first tier using the multivariate discrete-time Hawkes process; in the second tier, opinions cooperate and compete for market share, given limited attention using the market share attraction model. We demonstrate the convergence of our proposed estimation scheme on a synthetic dataset. Next, we test OMM on two learning tasks, applying to two real-world datasets to predict attention market shares and uncover latent relationships between online items. The first dataset comprises Facebook and Twitter discussions containing moderate and far-right opinions about bushfires and climate change. The second dataset captures popular VEVO artists' YouTube and Twitter attention volumes. OMM outperforms the state-of-the-art predictive models on both datasets and captures latent cooperation-competition relations. We uncover (1) self- and cross-reinforcement between far-right and moderate opinions on the bushfires and (2) pairwise artist relations that correlate with real-world interactions such as collaborations and long-lasting feuds. Lastly, we use OMM as a testbed for positive interventions and show how media coverage modulates the spread of far-right opinions.},
    url = {https://arxiv.org/abs/2208.06620},
    url_Paper = {https://arxiv.org/pdf/2208.06620.pdf}
}

@article{Schneider2023,
    author = {Philipp J. Schneider  and Marian-Andrei Rizoiu },
    title = {The effectiveness of moderating harmful online content},
    journal = {Proceedings of the National Academy of Sciences},
    volume = {120},
    number = {34},
    pages = {e2307360120},
    year = {2023},
    doi = {10.1073/pnas.2307360120},
    URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2307360120},
    url_Paper = {https://www.pnas.org/doi/pdf/10.1073/pnas.2307360120},
    url_Slides = {../extra/RIZOIU_PNAS'23-slides.pdf},
    abstract = {In 2022, the European Union introduced the Digital Services Act (DSA), a new legislation to report and moderate harmful content from online social networks. Trusted flaggers are mandated to identify harmful content, which platforms must remove within a set delay (currently 24 h). Here, we analyze the likely effectiveness of EU-mandated mechanisms for regulating highly viral online content with short half-lives. We deploy self-exciting point processes to determine the relationship between the regulated moderation delay and the likely harm reduction achieved. We find that harm reduction is achievable for the most harmful content, even for fast-paced platforms such as Twitter. Our method estimates moderation effectiveness for a given platform and provides a rule of thumb for selecting content for investigation and flagging, managing flaggers' workload.}
}

@article{McCarthy2023,
    abstract = {Startup companies solve many of today's most challenging problems, such as the decarbonisation of the economy or the development of novel life-saving vaccines. Startups are a vital source of innovation, yet the most innovative are also the least likely to survive. The probability of success of startups has been shown to relate to several firm-level factors such as industry, location and the economy of the day. Still, attention has increasingly considered internal factors relating to the firm's founding team, including their previous experiences and failures, their centrality in a global network of other founders and investors, as well as the team's size. The effects of founders' personalities on the success of new ventures are, however, mainly unknown. Here, we show that founder personality traits are a significant feature of a firm's ultimate success. We draw upon detailed data about the success of a large-scale global sample of startups (n = 21,187). We find that the Big Five personality traits of startup founders across 30 dimensions significantly differ from that of the population at large. Key personality facets that distinguish successful entrepreneurs include a preference for variety, novelty and starting new things (openness to adventure), like being the centre of attention (lower levels of modesty) and being exuberant (higher activity levels). We do not find one 'Founder-type' personality; instead, six different personality types appear. Our results also demonstrate the benefits of larger, personality-diverse teams in startups, which show an increased likelihood of success. The findings emphasise the role of the diversity of personality types as a novel dimension of team diversity that influences performance and success.},
    archivePrefix = {arXiv},
    arxivId = {2302.07968},
    author = {McCarthy, Paul X. and Gong, Xian and Braesemann, Fabian and Stephany, Fabian and Rizoiu, Marian-Andrei and Kern, Margaret L.},
    doi = {10.1038/s41598-023-41980-y},
    eprint = {2302.07968},
    issn = {2045-2322},
    journal = {Scientific Reports},
    month = {oct},
    number = {1},
    pages = {17200},
    title = {{The impact of founder personalities on startup success}},
    url = {https://www.nature.com/articles/s41598-023-41980-y},
    url_Paper = {https://www.nature.com/articles/s41598-023-41980-y.pdf},
    volume = {13},
    year = {2023}
}

@article{Yuan2023,
    abstract = {Today, the internet is an integral part of our daily lives, enabling people to be more connected than ever before. However, this greater connectivity and access to information increase exposure to harmful content, such as cyber-bullying and cyber-hatred. Models based on machine learning and natural language offer a way to make online platforms safer by identifying hate speech in web text autonomously. However, the main difficulty is annotating a sufficiently large number of examples to train these models. This paper uses a transfer learning technique to leverage two independent datasets jointly and builds a single representation of hate speech. We build an interpretable two-dimensional visualization tool of the constructed hate speech representation—dubbed the Map of Hate—in which multiple datasets can be projected and comparatively analyzed. The hateful content is annotated differently across the two datasets (racist and sexist in one dataset, hateful and offensive in another). However, the common representation successfully projects the harmless class of both datasets into the same space and can be used to uncover labeling errors (false positives). We also show that the joint representation boosts prediction performances when only a limited amount of supervision is available. These methods and insights hold the potential for safer social media and reduce the need to expose human moderators and annotators to distressing online messaging.},
    archivePrefix = {arXiv},
    arxivId = {2208.10598},
    author = {Yuan, Lanqin and Wang, Tianyu and Ferraro, Gabriela and Suominen, Hanna and Rizoiu, Marian-Andrei},
    doi = {10.1007/s42001-023-00224-9},
    eprint = {2208.10598},
    issn = {2432-2717},
    journal = {Journal of Computational Social Science},
    month = {oct},
    title = {{Transfer learning for hate speech detection in social media}},
    url = {https://link.springer.com/10.1007/s42001-023-00224-9},
    url_Paper = {https://link.springer.com/content/pdf/10.1007/s42001-023-00224-9.pdf},
    year = {2023}
}

@inproceedings{Elia2023,
    abstract = {The fast adoption of new technologies forces companies to continuously adapt their operations making it harder to predict workforce requirements. Several recent studies have attempted to predict the emergence of new roles and skills in the labour market from online job ads. This paper aims to present a novel ontology linking business transformation initiatives to occupations and an approach to automatically populating it by leveraging embeddings extracted from job ads and Wikipedia pages on business transformation and emerging technologies topics. To our knowledge, no previous research explicitly links business transformation initiatives, like the adoption of new technologies or the entry into new markets, to the roles needed. Our approach successfully matches occupations to transformation initiatives under ten different scenarios, five linked to technology adoption and five related to business. This framework presents an innovative approach to guide enterprises and educational institutions on the workforce requirements for specific business transformation initiatives.},
    address = {Wellington, NZ},
    archivePrefix = {arXiv},
    arxivId = {2310.17909},
    author = {Elia, Daniela and Chen, Fang and Zowghi, Didar and Rizoiu, Marian-Andrei},
    booktitle = {Australasian Conference on Information Systems},
    eprint = {2310.17909},
    keywords = {Job Postings,Strategic Workforce Planning,Word Embeddings},
    month = {oct},
    pages = {1--14},
    title = {{The Innovation-to-Occupations Ontology: Linking Business Transformation Initiatives to Occupations and Skills}},
    url = {https://aisel.aisnet.org/acis2023/13/},
    url_Paper = {http://arxiv.org/pdf/2310.17909.pdf},
    year = {2023}
}


@inproceedings{Girard2023,
    abstract = {Explainable AI (XAI) plays a crucial role in enhancing transparency and providing rational explanations to support users of AI systems. Inclusive AI actively seeks to engage and represent individuals with diverse attributes who are affected by and contribute to the AI ecosystem. Both inclusion and XAI advocate for the active involvement of the users and stakeholders during the entire AI system lifecycle. However, the relationship between XAI and Inclusive AI has not been explored. In this paper, we present the results of a systematic literature review with the objective to explore this relationship in the recent AI research literature. We were able to identify 18 research articles on the topic. Our analysis focused on exploring approaches to (1) the human attributes and perspectives, (2) preferred explanation methods, and (3) human-AI interaction. Based on our findings, we identified potential future XAI research directions and proposed strategies for practitioners involved in the design and development of inclusive AI systems.},
    author = {Girard, Amelie and Zowghi, Didar and Bano, Muneera and Rizoiu, Marian-Andrei},
    booktitle = {The Hawaii International Conference on System Sciences (HICSS)},
    keywords = {Explainable AI,Human-centered,Inclusion,Transparency},
    title = {{Inclusive and Explainable AI Systems: A Systematic Literature Review}},
    url = {https://scholarspace.manoa.hawaii.edu/items/6353f320-f22a-468d-9983-f79574795a20},
    url_Paper = {https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/598663fb-b3f1-47f1-aec1-ec3ee939cbed/content},
    year = {2023}
}

@article{Johns2024,
   abstract = {In this paper, we ask how effective Meta's content moderation strategy was on its flagship platform, Facebook, during the COVID-19 pandemic. We analyse the performance of 18 Australian right-wing/anti-vaccination pages, posts and commenting sections collected between January 2019 and July 2021, and use engagement metrics and time series analysis to analyse the data, mapping key policy announcements against page performance. We combine this with content analysis of comments parsed from two public pages that overperformed in the time period. The results show that Meta's content moderation systems were partially effective, with previously high-performing pages showing steady decline. Nonetheless, some pages not only slipped through the net but overperformed, proving this strategy to be piecemeal and inconsistent. The analysis identifies trends that content labelling and ‘shadow banning’ accounts was resisted by these communities, who employed tactics to stay engaged on Facebook, while migrating some conversations to less moderated platforms.},
   author = {Amelia Johns and Francesco Bailo and Emily Booth and Marian-Andrei Rizoiu},
   doi = {10.1177/1329878X241236984},
   issn = {1329-878X},
   journal = {Media International Australia},
   month = {3},
   pages = {36},
   title = {Labelling, shadow bans and community resistance: did meta's strategy to suppress rather than remove COVID misinformation and conspiracy theory on Facebook slow the spread?},
   url = {https://journals.sagepub.com/doi/10.1177/1329878X241236984},
   url_Paper = {https://journals.sagepub.com/doi/pdf/10.1177/1329878X241236984?download=true},
   URL_video = {https://theconversation.com/conspiracy-theorist-tactics-show-its-too-easy-to-get-around-facebooks-content-policies-226118},
   year = {2024},
}

@article{Booth2024,
   abstract = {In response to the rise of various fringe movements in recent years, from anti-vaxxers to QAnon, there has been increased public and scholarly attention to misinformation and conspiracy theories and the online communities that produce them. However, efforts at understanding the radicalisation process largely focus on those who go on to commit violent crimes. This article draws on three waves of research exploring the experiences of individuals currently or formerly involved in fringe communities, including the different stages of investment they progressed through, and ultimately, what made people leave. We propose a pathway model for understanding contemporary online radicalisation, including potential interventions that could be safely made at each stage. Insight into the experience of being immersed in these communities is essential for engaging with these people empathetically, and therefore preventing both the emergence of violent terrorists and protecting vulnerable people from being drawn into these communities.},
   author = {Emily Booth and Jooyoung Lee and Marian-Andrei Rizoiu and Hany Farid},
   doi = {10.1177/14407833241231756},
   issn = {1440-7833},
   journal = {Journal of Sociology},
   month = {2},
   title = {Conspiracy, misinformation, radicalisation: understanding the online pathway to indoctrination and opportunities for intervention},
%   url_Paper = {https://deliverypdf.ssrn.com/delivery.php?ID=180100000127077104086117092064064026032089006029067048123104103072074125022071126075037031036026028039096031110103016103080082126033003036011065125102110080090065027035075008084117002071031096003101031071112104127081069027016117102088102084030120021031&EXT=pdf&INDEX=TRUE},
   url_Paper = {https://farid.berkeley.edu/downloads/publications/jos24.pdf},
   url = {https://doi.org/10.1177/14407833241231756},
   url_Video = {https://theconversation.com/how-people-get-sucked-into-misinformation-rabbit-holes-and-how-to-get-them-out-223717},
   year = {2024},
}

@inproceedings{Gong2024,
    abstract = {In this paper we use for the first time a systematic approach in the study of harmonic centrality at a Web domain level, and gather a number of significant new findings about the Australian web. In particular, we explore the relationship between economic diversity at the firm level and the structure of the Web within the Australian domain space, using harmonic centrality as the main structural feature. The distribution of harmonic centrality values is analyzed over time, and we find that the distributions exhibit a consistent pattern across the different years. The observed distribution is well captured by a partition of the domain space into six clusters; the temporal movement of domain names across these six positions yields insights into the Australian Domain Space and exhibits correlations with other non-structural characteristics. From a more global perspective, we find a signiffcant correlation between the median harmonic centrality of all domains in each OECD country and one measure of global trust, the WJP Rule of Law Index. Further investigation demonstrates that 35 countries in OECD share similar harmonic centrality distributions. The observed homogeneity in distribution presents a compelling avenue for exploration, potentially unveiling critical corporate, regional, or national insights.},
   author = {Xian Gong and Paul X. Mccarthy and Marian-Andrei Rizoiu and Paolo Boldi},
   city = {New York, NY, USA},
   doi = {10.1145/3614419.3643998},
   isbn = {9798400703348},
   journal = {ACM Web Science Conference},
   month = {5},
   pages = {92-102},
   publisher = {ACM},
   title = {Harmony in the Australian Domain Space},
   url = {https://dl.acm.org/doi/10.1145/3614419.3643998},
   url_Paper = {https://arxiv.org/pdf/2404.10006.pdf},
   year = {2024},
}

@inproceedings{Gong2023,
   abstract = {Understanding the relationship between emerging technology and research and development has long been of interest to companies, policy makers and researchers. In this paper new sources of data and tools are combined with a novel technique to construct a model linking a defined set of emerging technologies with the global leading R&D spending companies. The result is a new map of this landscape. This map reveals the proximity of technologies and companies in the knowledge embedded in their corresponding Wikipedia profiles, enabling analysis of the closest associations between the companies and emerging technologies. A significant positive correlation for a related set of patent data validates the approach. Finally, a set of Circular Economy Emerging Technologies are matched to their closest leading R&D spending company, prompting future research ideas in broader or narrower application of the model to specific technology themes, company competitor landscapes and national interest concerns.},
   author = {Xian Gong and Claire McFarland and Paul X. McCarth and Colin Griffith and Marian-Andrei Rizoiu},
   city = {Ljubljana, Slovenia},
   journal = {XXXIV ISPIM Innovation Conference},
   month = {6},
   title = {Informing Innovation Management: Linking Leading R&D Firms and Emerging Technologies},
   url_Paper = {https://arxiv.org/pdf/2305.02476.pdf},
   year = {2023},
}

@inproceedings{Calderon2024b,
   abstract = {The spread of content on social media is shaped by intertwining factors on three levels: the source, the content itself, and the pathways of content spread. At the lowest level, the popularity of the sharing user determines its eventual reach. However, higher-level factors such as the nature of the online item and the credibility of its source also play crucial roles in determining how widely and rapidly the online item spreads. In this work, we propose the Bayesian Mixture Hawkes (BMH) model to jointly learn the influence of source, content and spread. We formulate the BMH model as a hierarchical mixture model of separable Hawkes processes, accommodating different classes of Hawkes dynamics and the influence of feature sets on these classes. We test the BMH model on two learning tasks, cold-start popularity prediction and temporal profile generalization performance, applying to two real-world retweet cascade datasets referencing articles from controversial and traditional media publishers. The BMH model outperforms the state-of-the-art models and predictive baselines on both datasets and utilizes cascade- and item-level information better than the alternatives. Lastly, we perform a counter-factual analysis where we apply the trained publisher-level BMH models to a set of article headlines and show that effectiveness of headline writing style (neutral, clickbait, inflammatory) varies across publishers. The BMH model unveils differences in style effectiveness between controversial and reputable publishers, where we find clickbait to be notably more effective for reputable publishers as opposed to controversial ones, which links to the latter's overuse of clickbait.},
   author = {Pio Calderon and Marian-Andrei Rizoiu},
   city = {Vilnius, Lithuania},
   booktitle = {European Conference on Machine Learning and Data Mining (ECML-PKDD)},
   month = {9},
   title = {What Drives Online Popularity: Author, Content or Sharers? Estimating Spread Dynamics with Bayesian Mixture Hawkes},
   url_Paper = {https://arxiv.org/pdf/2406.03390},
   year = {2024},
}

@article{Yuan2024,
   abstract = {Automatic identification of hateful and abusive content is vital in combating the spread of harmful online content and its damaging effects. Most existing works evaluate models by examining the generalization error on train-test splits on hate speech datasets. These datasets often differ in their definitions and labeling criteria, leading to poor model performance when predicting across new domains and datasets. In this work, we propose a new Multi-task Learning (MTL) pipeline that utilizes MTL to train simultaneously across multiple hate speech datasets to construct a more encompassing classification model. We simulate evaluation on new previously unseen datasets by adopting a leave-one-out scheme in which we omit a target dataset from training and jointly train on the other datasets. Our results consistently outperform a large sample of existing work. We show strong results when examining generalization error in train-test splits and substantial improvements when predicting on previously unseen datasets. Furthermore, we assemble a novel dataset, dubbed PubFigs, focusing on the problematic speech of American Public Political Figures. We automatically detect problematic speech in the $305,235$ tweets in PubFigs, and we uncover insights into the posting behaviors of public figures.},
   author = {Lanqin Yuan and Marian-Andrei Rizoiu},
   doi = {10.1016/j.csl.2024.101690},
   issn = {08852308},
   journal = {Computer Speech \& Language},
   month = {7},
   pages = {101690},
   title = {Generalizing hate speech detection using multi-task learning: A case study of political public figures},
   url = {https://doi.org/10.1016/j.csl.2024.101690},
   url_Paper = {https://arxiv.org/pdf/2208.10598},
   url_Video = {https://www.uts.edu.au/news/tech-design/right-leaning-political-figures-fuel-online-hate},
   year = {2024},
}

@article{Ram2024,
   abstract = {Social inﬂuence pervades our everyday lives and lays the foundation for complex social phenomena, such as the spread of misinformation and the polarization of communities. A disconnect appears between psychology approaches, generally performed and tested in controlled lab experiments, and quantitative methods, which are usually data-driven and rely on network and event analysis. The former are slow, expensive to deploy, and typically do not generalize well to topical issues; the latter often oversimplify the complexities of social inﬂuence and ignore psychosocial literature. This work bridges this gap by introducing a human-in-the-loop active learning method that empirically quantiﬁes social inﬂuence by crowdsourcing pairwise inﬂuence comparisons. We develop simulation and ﬁtting tools, allowing us to estimate the required budget based on the design features and the worker’s decision accuracy. We perform a series of pilot studies to quantify the impact of design features on worker accuracy. We deploy our method to estimate the inﬂuence ranking of 500 X/Twitter users. We validate our measure by showing that the obtained empirical inﬂuence is tightly linked with agency and communion, the Big Two of social cognition, with agency being the most important dimension for inﬂuence formation.},
   author = {Rohit Ram and Marian-Andrei Rizoiu},
   doi = {10.1140/epjds/s13688-024-00492-z},
   issn = {2193-1127},
   issue = {1},
   journal = {EPJ Data Science},
   month = {8},
   pages = {53},
   title = {Empirically measuring online social influence},
   volume = {13},
   url = {https://doi.org/10.1140/epjds/s13688-024-00492-z},
   url_Paper = {https://epjdatascience.springeropen.com/counter/pdf/10.1140/epjds/s13688-024-00492-z.pdf},
   year = {2024},
}

